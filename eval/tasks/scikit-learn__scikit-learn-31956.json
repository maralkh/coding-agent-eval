{
  "id": "scikit-learn__scikit-learn-31956",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "d5715fb24288ca388605b27cc184cd526e9442ac",
  "issue_number": 15931,
  "issue_title": "[WIP] cd_fast speedup",
  "issue_body": "Rewriting implementation to remove redundant calculations.\r\nOld implementation:\r\n1. R = R + w_ii * X[:,ii]\r\n2. tmp = X[:,ii] dot R\r\n3. f(tmp,...)\r\n4. R = R - w[ii] * X[:,ii]\r\n\r\nNew implementation:\r\nsubstitute R from 1 into 2 ->\r\ntmp = X[:,ii] dot (R + w_ii * X[:,ii]) \r\ntmp = X[:,ii] dot R + w_ii * X[:,ii] dot X[:,ii]\r\n2. tmp = X[:,ii] dot R + w_ii * norm_cols_X[ii]\r\nThen to update R:\r\n4. R = R + (w_ii - w[ii]) * X[:,ii]\r\n\r\nThis removes step one, and rewrites step 2 and 4, improving loop speed.\r\nThe method here is probably also extendable to the other 3 functions.\r\n\r\nSadly my python skills are not good enough to build and test this, I did however test this in C++, so everything should work.\r\nHelp with building, testing, and extending is very much appreciated.\r\n\r\nTasklist:\r\n- [ ] Build and validate results.\r\n- [ ] Extend to sparse, gram, and multi_task",
  "pr_number": 31956,
  "pr_title": "ENH speedup coordinate descent by avoiding calls to axpy in innermost loop",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/31880.efficiency.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/31880.efficiency.rst\nindex 9befdee1e144c..195eb42d907eb 100644\n--- a/doc/whats_new/upcoming_changes/sklearn.linear_model/31880.efficiency.rst\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/31880.efficiency.rst\n@@ -1,7 +1,9 @@\n - :class:`linear_model.ElasticNet`, :class:`linear_model.ElasticNetCV`,\n-  :class:`linear_model.Lasso` and :class:`linear_model.LassoCV` with `precompute=True`\n-  (or `precompute=\"auto\"`` and `n_samples > n_features`) are faster to fit by\n-  avoiding a BLAS level 1 (axpy) call in the inner most loop.\n+  :class:`linear_model.Lasso`, :class:`linear_model.LassoCV`,\n+  :class:`linear_model.MultiTaskElasticNet`,\n+  :class:`linear_model.MultiTaskElasticNetCV`,\n+  :class:`linear_model.MultiTaskLasso` and :class:`linear_model.MultiTaskLassoCV`\n+  are faster to fit by avoiding a BLAS level 1 (axpy) call in the innermost loop.\n   Same for functions :func:`linear_model.enet_path` and\n   :func:`linear_model.lasso_path`.\n-  By :user:`Christian Lorentzen <lorentzenchr>`.\n+  By :user:`Christian Lorentzen <lorentzenchr>` :pr:`31956` and\ndiff --git a/sklearn/linear_model/_cd_fast.pyx b/sklearn/linear_model/_cd_fast.pyx\nindex 369ab162d563c..ba8ae2e575576 100644\n--- a/sklearn/linear_model/_cd_fast.pyx\n+++ b/sklearn/linear_model/_cd_fast.pyx\n@@ -329,12 +329,8 @@ def enet_coordinate_descent(\n \n                 w_j = w[j]  # Store previous value\n \n-                if w_j != 0.0:\n-                    # R += w_j * X[:,j]\n-                    _axpy(n_samples, w_j, &X[0, j], 1, &R[0], 1)\n-\n-                # tmp = (X[:,j]*R).sum()\n-                tmp = _dot(n_samples, &X[0, j], 1, &R[0], 1)\n+                # tmp = X[:,j] @ (R + w_j * X[:,j])\n+                tmp = _dot(n_samples, &X[0, j], 1, &R[0], 1) + w_j * norm2_cols_X[j]\n \n                 if positive and tmp < 0:\n                     w[j] = 0.0\n@@ -342,9 +338,9 @@ def enet_coordinate_descent(\n                     w[j] = (fsign(tmp) * fmax(fabs(tmp) - alpha, 0)\n                             / (norm2_cols_X[j] + beta))\n \n-                if w[j] != 0.0:\n-                    # R -=  w[j] * X[:,j] # Update residual\n-                    _axpy(n_samples, -w[j], &X[0, j], 1, &R[0], 1)\n+                if w[j] != w_j:\n+                    # R -= (w[j] - w_j) * X[:,j] # Update residual\n+                    _axpy(n_samples, w_j - w[j], &X[0, j], 1, &R[0], 1)\n \n                 # update the maximum absolute coefficient update\n                 d_w_j = fabs(w[j] - w_j)\n@@ -450,7 +446,7 @@ def sparse_enet_coordinate_descent(\n     # We work with:\n     #     yw = sample_weight * y\n     #     R = sample_weight * residual\n-    #     norm_cols_X = np.sum(sample_weight * (X - X_mean)**2, axis=0)\n+    #     norm2_cols_X = np.sum(sample_weight * (X - X_mean)**2, axis=0)\n \n     if floating is float:\n         dtype = np.float32\n@@ -461,8 +457,8 @@ def sparse_enet_coordinate_descent(\n     cdef unsigned int n_samples = y.shape[0]\n     cdef unsigned int n_features = w.shape[0]\n \n-    # compute norms of the columns of X\n-    cdef floating[::1] norm_cols_X = np.zeros(n_features, dtype=dtype)\n+    # compute squared norms of the columns of X\n+    cdef floating[::1] norm2_cols_X = np.zeros(n_features, dtype=dtype)\n \n     # initial value of the residuals\n     # R = y - Zw, weighted version R = sample_weight * (y - Zw)\n@@ -523,7 +519,7 @@ def sparse_enet_coordinate_descent(\n                 for jj in range(startptr, endptr):\n                     normalize_sum += (X_data[jj] - X_mean_ii) ** 2\n                     R[X_indices[jj]] -= X_data[jj] * w_ii\n-                norm_cols_X[ii] = normalize_sum + \\\n+                norm2_cols_X[ii] = normalize_sum + \\\n                     (n_samples - endptr + startptr) * X_mean_ii ** 2\n                 if center:\n                     for jj in range(n_samples):\n@@ -542,7 +538,7 @@ def sparse_enet_coordinate_descent(\n                         normalize_sum += sample_weight[jj] * X_mean_ii ** 2\n                         R[jj] += sample_weight[jj] * X_mean_ii * w_ii\n                         R_sum += R[jj]\n-                norm_cols_X[ii] = normalize_sum\n+                norm2_cols_X[ii] = normalize_sum\n             startptr = endptr\n \n         # Note: No need to update R_sum from here on because the update terms cancel\n@@ -564,7 +560,7 @@ def sparse_enet_coordinate_descent(\n                 else:\n                     ii = f_iter\n \n-                if norm_cols_X[ii] == 0.0:\n+                if norm2_cols_X[ii] == 0.0:\n                     continue\n \n                 startptr = X_indptr[ii]\n@@ -572,26 +568,11 @@ def sparse_enet_coordinate_descent(\n                 w_ii = w[ii]  # Store previous value\n                 X_mean_ii = X_mean[ii]\n \n-                if w_ii != 0.0:\n-                    # R += w_ii * X[:,ii]\n-                    if no_sample_weights:\n-                        for jj in range(startptr, endptr):\n-                            R[X_indices[jj]] += X_data[jj] * w_ii\n-                        if center:\n-                            for jj in range(n_samples):\n-                                R[jj] -= X_mean_ii * w_ii\n-                    else:\n-                        for jj in range(startptr, endptr):\n-                            tmp = sample_weight[X_indices[jj]]\n-                            R[X_indices[jj]] += tmp * X_data[jj] * w_ii\n-                        if center:\n-                            for jj in range(n_samples):\n-                                R[jj] -= sample_weight[jj] * X_mean_ii * w_ii\n-\n-                # tmp = (X[:,ii] * R).sum()\n+                # tmp = X[:,ii] @ (R + w_ii * X[:,ii])\n                 tmp = 0.0\n                 for jj in range(startptr, endptr):\n                     tmp += R[X_indices[jj]] * X_data[jj]\n+                tmp += w_ii * norm2_cols_X[ii]\n \n                 if center:\n                     tmp -= R_sum * X_mean_ii\n@@ -600,23 +581,23 @@ def sparse_enet_coordinate_descent(\n                     w[ii] = 0.0\n                 else:\n                     w[ii] = fsign(tmp) * fmax(fabs(tmp) - alpha, 0) \\\n-                            / (norm_cols_X[ii] + beta)\n+                            / (norm2_cols_X[ii] + beta)\n \n-                if w[ii] != 0.0:\n-                    # R -=  w[ii] * X[:,ii] # Update residual\n+                if w[ii] != w_ii:\n+                    # R -=  (w[ii] - w_ii) * X[:,ii] # Update residual\n                     if no_sample_weights:\n                         for jj in range(startptr, endptr):\n-                            R[X_indices[jj]] -= X_data[jj] * w[ii]\n+                            R[X_indices[jj]] -= X_data[jj] * (w[ii] - w_ii)\n                         if center:\n                             for jj in range(n_samples):\n-                                R[jj] += X_mean_ii * w[ii]\n+                                R[jj] += X_mean_ii * (w[ii] - w_ii)\n                     else:\n                         for jj in range(startptr, endptr):\n-                            tmp = sample_weight[X_indices[jj]]\n-                            R[X_indices[jj]] -= tmp * X_data[jj] * w[ii]\n+                            kk = X_indices[jj]\n+                            R[kk] -= sample_weight[kk] * X_data[jj] * (w[ii] - w_ii)\n                         if center:\n                             for jj in range(n_samples):\n-                                R[jj] += sample_weight[jj] * X_mean_ii * w[ii]\n+                                R[jj] += sample_weight[jj] * X_mean_ii * (w[ii] - w_ii)\n \n                 # update the maximum absolute coefficient update\n                 d_w_ii = fabs(w[ii] - w_ii)\n@@ -744,10 +725,13 @@ def enet_coordinate_descent_gram(\n     cdef floating w_max\n     cdef floating d_w_ii\n     cdef floating q_dot_w\n-    cdef floating w_norm2\n     cdef floating gap = tol + 1.0\n     cdef floating d_w_tol = tol\n     cdef floating dual_norm_XtA\n+    cdef floating R_norm2\n+    cdef floating w_norm2\n+    cdef floating A_norm2\n+    cdef floating const_\n     cdef unsigned int ii\n     cdef unsigned int n_iter = 0\n     cdef unsigned int f_iter\n@@ -786,7 +770,7 @@ def enet_coordinate_descent_gram(\n                     w[ii] = fsign(tmp) * fmax(fabs(tmp) - alpha, 0) \\\n                         / (Q[ii, ii] + beta)\n \n-                if w[ii] != 0.0 or w_ii != 0.0:\n+                if w[ii] != w_ii:\n                     # Qw +=  (w[ii] - w_ii) * Q[ii]  # Update Qw = Q @ w\n                     _axpy(n_features, w[ii] - w_ii, &Q[ii, 0], 1,\n                           &Qw[0], 1)\n@@ -899,6 +883,12 @@ def enet_coordinate_descent_multi_task(\n     cdef unsigned int n_features = X.shape[1]\n     cdef unsigned int n_tasks = Y.shape[1]\n \n+    # compute squared norms of the columns of X\n+    # same as norm2_cols_X = np.square(X).sum(axis=0)\n+    cdef floating[::1] norm2_cols_X = np.einsum(\n+        \"ij,ij->j\", X, X, dtype=dtype, order=\"C\"\n+    )\n+\n     # to store XtA\n     cdef floating[:, ::1] XtA = np.zeros((n_features, n_tasks), dtype=dtype)\n     cdef floating XtA_axis1norm\n@@ -907,7 +897,6 @@ def enet_coordinate_descent_multi_task(\n     # initial value of the residuals\n     cdef floating[::1, :] R = np.zeros((n_samples, n_tasks), dtype=dtype, order='F')\n \n-    cdef floating[::1] norm_cols_X = np.zeros(n_features, dtype=dtype)\n     cdef floating[::1] tmp = np.zeros(n_tasks, dtype=dtype)\n     cdef floating[::1] w_ii = np.zeros(n_tasks, dtype=dtype)\n     cdef floating d_w_max\n@@ -917,8 +906,8 @@ def enet_coordinate_descent_multi_task(\n     cdef floating W_ii_abs_max\n     cdef floating gap = tol + 1.0\n     cdef floating d_w_tol = tol\n-    cdef floating R_norm\n-    cdef floating w_norm\n+    cdef floating R_norm2\n+    cdef floating w_norm2\n     cdef floating ry_sum\n     cdef floating l21_norm\n     cdef unsigned int ii\n@@ -928,9 +917,6 @@ def enet_coordinate_descent_multi_task(\n     cdef uint32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)\n     cdef uint32_t* rand_r_state = &rand_r_state_seed\n \n-    cdef const floating* X_ptr = &X[0, 0]\n-    cdef const floating* Y_ptr = &Y[0, 0]\n-\n     if l1_reg == 0:\n         warnings.warn(\n             \"Coordinate descent with l1_reg=0 may lead to unexpected\"\n@@ -938,20 +924,16 @@ def enet_coordinate_descent_multi_task(\n         )\n \n     with nogil:\n-        # norm_cols_X = (np.asarray(X) ** 2).sum(axis=0)\n-        for ii in range(n_features):\n-            norm_cols_X[ii] = _nrm2(n_samples, X_ptr + ii * n_samples, 1) ** 2\n-\n         # R = Y - np.dot(X, W.T)\n-        _copy(n_samples * n_tasks, Y_ptr, 1, &R[0, 0], 1)\n+        _copy(n_samples * n_tasks, &Y[0, 0], 1, &R[0, 0], 1)\n         for ii in range(n_features):\n             for jj in range(n_tasks):\n                 if W[jj, ii] != 0:\n-                    _axpy(n_samples, -W[jj, ii], X_ptr + ii * n_samples, 1,\n+                    _axpy(n_samples, -W[jj, ii], &X[0, ii], 1,\n                           &R[0, jj], 1)\n \n         # tol = tol * linalg.norm(Y, ord='fro') ** 2\n-        tol = tol * _nrm2(n_samples * n_tasks, Y_ptr, 1) ** 2\n+        tol = tol * _nrm2(n_samples * n_tasks, &Y[0, 0], 1) ** 2\n \n         for n_iter in range(max_iter):\n             w_max = 0.0\n@@ -962,54 +944,47 @@ def enet_coordinate_descent_multi_task(\n                 else:\n                     ii = f_iter\n \n-                if norm_cols_X[ii] == 0.0:\n+                if norm2_cols_X[ii] == 0.0:\n                     continue\n \n                 # w_ii = W[:, ii] # Store previous value\n                 _copy(n_tasks, &W[0, ii], 1, &w_ii[0], 1)\n \n-                # Using Numpy:\n-                # R += np.dot(X[:, ii][:, None], w_ii[None, :]) # rank 1 update\n-                # Using Blas Level2:\n-                # _ger(RowMajor, n_samples, n_tasks, 1.0,\n-                #      &X[0, ii], 1,\n-                #      &w_ii[0], 1, &R[0, 0], n_tasks)\n-                # Using Blas Level1 and for loop to avoid slower threads\n-                # for such small vectors\n-                for jj in range(n_tasks):\n-                    if w_ii[jj] != 0:\n-                        _axpy(n_samples, w_ii[jj], X_ptr + ii * n_samples, 1,\n-                              &R[0, jj], 1)\n-\n-                # Using numpy:\n-                # tmp = np.dot(X[:, ii][None, :], R).ravel()\n-                # Using BLAS Level 2:\n-                # _gemv(RowMajor, Trans, n_samples, n_tasks, 1.0, &R[0, 0],\n-                #       n_tasks, &X[0, ii], 1, 0.0, &tmp[0], 1)\n+                # tmp = X[:, ii] @ (R + w_ii * X[:,ii][:, None])\n+                # first part: X[:, ii] @ R\n+                #   Using BLAS Level 2:\n+                #   _gemv(RowMajor, Trans, n_samples, n_tasks, 1.0, &R[0, 0],\n+                #         n_tasks, &X[0, ii], 1, 0.0, &tmp[0], 1)\n+                # second part: (X[:, ii] @ X[:,ii]) * w_ii = norm2_cols * w_ii\n+                #   Using BLAS Level 1:\n+                #   _axpy(n_tasks, norm2_cols[ii], &w_ii[0], 1, &tmp[0], 1)\n                 # Using BLAS Level 1 (faster for small vectors like here):\n                 for jj in range(n_tasks):\n-                    tmp[jj] = _dot(n_samples, X_ptr + ii * n_samples, 1,\n-                                   &R[0, jj], 1)\n+                    tmp[jj] = _dot(n_samples, &X[0, ii], 1, &R[0, jj], 1)\n+                    # As we have the loop already, we use it to replace the second BLAS\n+                    # Level 1, i.e., _axpy, too.\n+                    tmp[jj] += w_ii[jj] * norm2_cols_X[ii]\n \n                 # nn = sqrt(np.sum(tmp ** 2))\n                 nn = _nrm2(n_tasks, &tmp[0], 1)\n \n-                # W[:, ii] = tmp * fmax(1. - l1_reg / nn, 0) / (norm_cols_X[ii] + l2_reg)\n+                # W[:, ii] = tmp * fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[ii] + l2_reg)\n                 _copy(n_tasks, &tmp[0], 1, &W[0, ii], 1)\n-                _scal(n_tasks, fmax(1. - l1_reg / nn, 0) / (norm_cols_X[ii] + l2_reg),\n+                _scal(n_tasks, fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[ii] + l2_reg),\n                       &W[0, ii], 1)\n \n+                # Update residual\n                 # Using numpy:\n-                # R -= np.dot(X[:, ii][:, None], W[:, ii][None, :])\n-                # Using BLAS Level 2:\n-                # Update residual : rank 1 update\n-                # _ger(RowMajor, n_samples, n_tasks, -1.0,\n-                #      &X[0, ii], 1, &W[0, ii], 1,\n-                #      &R[0, 0], n_tasks)\n+                #   R -= (W[:, ii] - w_ii) * X[:, ii][:, None]\n+                # Using BLAS Level 1 and 2:\n+                #   _axpy(n_tasks, -1.0, &W[0, ii], 1, &w_ii[0], 1)\n+                #   _ger(RowMajor, n_samples, n_tasks, 1.0,\n+                #        &X[0, ii], 1, &w_ii, 1,\n+                #        &R[0, 0], n_tasks)\n                 # Using BLAS Level 1 (faster for small vectors like here):\n                 for jj in range(n_tasks):\n-                    if W[jj, ii] != 0:\n-                        _axpy(n_samples, -W[jj, ii], X_ptr + ii * n_samples, 1,\n+                    if W[jj, ii] != w_ii[jj]:\n+                        _axpy(n_samples, w_ii[jj] - W[jj, ii], &X[0, ii], 1,\n                               &R[0, jj], 1)\n \n                 # update the maximum absolute coefficient update\n@@ -1031,7 +1006,7 @@ def enet_coordinate_descent_multi_task(\n                 for ii in range(n_features):\n                     for jj in range(n_tasks):\n                         XtA[ii, jj] = _dot(\n-                            n_samples, X_ptr + ii * n_samples, 1, &R[0, jj], 1\n+                            n_samples, &X[0, ii], 1, &R[0, jj], 1\n                             ) - l2_reg * W[jj, ii]\n \n                 # dual_norm_XtA = np.max(np.sqrt(np.sum(XtA ** 2, axis=1)))\n@@ -1042,18 +1017,17 @@ def enet_coordinate_descent_multi_task(\n                     if XtA_axis1norm > dual_norm_XtA:\n                         dual_norm_XtA = XtA_axis1norm\n \n-                # TODO: use squared L2 norm directly\n-                # R_norm = linalg.norm(R, ord='fro')\n-                # w_norm = linalg.norm(W, ord='fro')\n-                R_norm = _nrm2(n_samples * n_tasks, &R[0, 0], 1)\n-                w_norm = _nrm2(n_features * n_tasks, &W[0, 0], 1)\n+                # R_norm2 = linalg.norm(R, ord='fro') ** 2\n+                # w_norm2 = linalg.norm(W, ord='fro') ** 2\n+                R_norm2 = _dot(n_samples * n_tasks, &R[0, 0], 1, &R[0, 0], 1)\n+                w_norm2 = _dot(n_features * n_tasks, &W[0, 0], 1, &W[0, 0], 1)\n                 if (dual_norm_XtA > l1_reg):\n                     const_ = l1_reg / dual_norm_XtA\n-                    A_norm = R_norm * const_\n-                    gap = 0.5 * (R_norm ** 2 + A_norm ** 2)\n+                    A_norm2 = R_norm2 * (const_ ** 2)\n+                    gap = 0.5 * (R_norm2 + A_norm2)\n                 else:\n                     const_ = 1.0\n-                    gap = R_norm ** 2\n+                    gap = R_norm2\n \n                 # ry_sum = np.sum(R * y)\n                 ry_sum = _dot(n_samples * n_tasks, &R[0, 0], 1, &Y[0, 0], 1)\n@@ -1066,7 +1040,7 @@ def enet_coordinate_descent_multi_task(\n                 gap += (\n                     l1_reg * l21_norm\n                     - const_ * ry_sum\n-                    + 0.5 * l2_reg * (1 + const_ ** 2) * (w_norm ** 2)\n+                    + 0.5 * l2_reg * (1 + const_ ** 2) * w_norm2\n                 )\n \n                 if gap <= tol:\ndiff --git a/sklearn/linear_model/tests/test_common.py b/sklearn/linear_model/tests/test_common.py\nindex 348710e70af64..12248a0980db4 100644\n--- a/sklearn/linear_model/tests/test_common.py\n+++ b/sklearn/linear_model/tests/test_common.py\n@@ -278,7 +278,7 @@ def test_model_pipeline_same_dense_and_sparse(LinearModel, params, csr_container\n     model_dense.fit(X, y)\n     model_sparse.fit(X_sparse, y)\n \n-    assert_allclose(model_sparse[1].coef_, model_dense[1].coef_, atol=1e-16)\n+    assert_allclose(model_sparse[1].coef_, model_dense[1].coef_, atol=1e-15)\n     y_pred_dense = model_dense.predict(X)\n     y_pred_sparse = model_sparse.predict(X_sparse)\n     assert_allclose(y_pred_dense, y_pred_sparse)\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/tests/test_common.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-08-16T09:04:21Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31956",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/15931"
}