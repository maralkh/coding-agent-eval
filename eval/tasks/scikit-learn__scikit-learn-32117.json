{
  "id": "scikit-learn__scikit-learn-32117",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "f1261a9356fe4519ac25af3a2e2b7ec31de3be96",
  "issue_number": 23162,
  "issue_title": "MinCovDet estimation of covariance with strong bias?",
  "issue_body": "### Discussed in https://github.com/scikit-learn/scikit-learn/discussions/23161\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **hongfei0224** April 20, 2022</sup>\r\nI was playing with MinCovDet (Minimum Covariance Determinant) in sklearn:\r\n[https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet](url)\r\n\r\nI see the idea of MinCovDet is to make the covariance matrix estimation robust when there is outlier in the data.\r\nTo understand the behavior, I tried the multivariate normal data **without** outliers.\r\n\r\nWhat I did:\r\n- generate multivariate_normal samples with given covariance matrix (the true covariance matrix)\r\n- use `np.cov` to estimate empirical covariance matrix\r\n- use `MinCovDet` to estimate robust covariance matrix\r\n- repeat above with same \"true covariance matrix\"\r\n- check the distribution of empirical covariance matrix and robust covariance matrix\r\n\r\nI was expecting both `np.cov` and `MinCovDet` estimation should be centered around the true covariance matrix, since there is no outlier in data.\r\n\r\nbut seems not the case.\r\n\r\n![image](https://user-images.githubusercontent.com/51205365/164061364-1daa7614-5178-46ad-b3be-0ff2e3c38a42.png)\r\n![image](https://user-images.githubusercontent.com/51205365/164061425-00155926-f523-45a3-b872-ed83660aba5e.png)\r\n\r\nthe code to replicate results\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\r\nimport matplotlib.pyplot as plt\r\nfrom tqdm.autonotebook import tqdm\r\n\r\nres = []\r\nres2= []\r\na = 2\r\nb = 5\r\ncovar = 3 \r\n\r\nfor i in tqdm(range(1000)):\r\n    x=np.random.multivariate_normal(mean=[0,0],cov=[[a,covar],[covar,b]], size=250).T\r\n    res.append(np.cov(x))\r\n    mcd = MinCovDet().fit(x.T)\r\n    res2.append(mcd.covariance_)\r\n\r\npd.Series([xx[0][0] for xx in res]).hist(bins=30, label='empirical',alpha=0.7)\r\npd.Series([xx[0][0] for xx in res2]).hist(bins=30, label='cvd',alpha=0.7)\r\nplt.axvline(a, ls='--', color='black')\r\nplt.title('variance')\r\nplt.legend()\r\nplt.show()\r\n\r\npd.Series([xx[0][1] for xx in res]).hist(bins=30, label='empirical',alpha=0.7)\r\npd.Series([xx[0][1] for xx in res2]).hist(bins=30, label='cvd',alpha=0.7)\r\nplt.axvline(covar, ls='--', color='black')\r\nplt.title('covariance')\r\nplt.legend()\r\nplt.show()\r\n```",
  "pr_number": 32117,
  "pr_title": "FIX: Reduce bias of  `covariance.MinCovDet` with consistency correction",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.covariance/32117.fix.rst b/doc/whats_new/upcoming_changes/sklearn.covariance/32117.fix.rst\nnew file mode 100644\nindex 0000000000000..fb8145e22e5ed\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.covariance/32117.fix.rst\n@@ -0,0 +1,4 @@\n+- Added correction to :class:`covariance.MinCovDet` to adjust for\n+  consistency at the normal distribution. This reduces the bias present\n+  when applying this method to data that is normally distributed.\n+  By :user:`Daniel Herrera-Esposito <dherrera1911>`\ndiff --git a/sklearn/covariance/_elliptic_envelope.py b/sklearn/covariance/_elliptic_envelope.py\nindex c0414991ca7c5..ea4243ef98cc5 100644\n--- a/sklearn/covariance/_elliptic_envelope.py\n+++ b/sklearn/covariance/_elliptic_envelope.py\n@@ -135,10 +135,10 @@ class EllipticEnvelope(OutlierMixin, MinCovDet):\n     ...              [3, 3]])\n     array([ 1, -1])\n     >>> cov.covariance_\n-    array([[0.7411, 0.2535],\n-           [0.2535, 0.3053]])\n+    array([[0.8102, 0.2736],\n+           [0.2736, 0.3330]])\n     >>> cov.location_\n-    array([0.0813 , 0.0427])\n+    array([0.0769 , 0.0397])\n     \"\"\"\n \n     _parameter_constraints: dict = {\ndiff --git a/sklearn/covariance/_robust_covariance.py b/sklearn/covariance/_robust_covariance.py\nindex 8a1946da7daad..515c411573310 100644\n--- a/sklearn/covariance/_robust_covariance.py\n+++ b/sklearn/covariance/_robust_covariance.py\n@@ -213,6 +213,43 @@ def _c_step(\n     return location, covariance, det, support, dist\n \n \n+def _consistency_factor(n_features, alpha):\n+    \"\"\"Multiplicative factor to make covariance estimate consistent\n+    at the normal distribution, as described in [Pison2002]_.\n+\n+    Parameters\n+    ----------\n+    n_features : int\n+        Number of features.\n+\n+    alpha : float\n+        Parameter related to the proportion of discarded points.\n+        This parameter must be in the range (0, 1).\n+\n+    Returns\n+    -------\n+    c_alpha : float\n+        Scaling factor to make covariance matrix consistent.\n+\n+    References\n+    ----------\n+    .. [Butler1993] R. W. Butler. P. L. Davies. M. Jhun. \"Asymptotics for the\n+        Minimum Covariance Determinant Estimator.\" Ann. Statist. 21 (3)\n+        1385 - 1400, September, 1993. https://doi.org/10.1214/aos/1176349264]\n+\n+    .. [Croux1999] Croux, C., Haesbroeck, G. \"Influence Function and\n+        Efficiency of the Minimum Covariance Determinant Scatter Matrix\n+        Estimator\" Journal of Multivariate Analysis 71(2) (1999) 161-190\n+\n+    .. [Pison2002] Pison, G., Van Aelst, S., Willems, G., \"Small sample\n+        corrections for LTS and MCD\" Metrika 55(1) (2002) 111-123\n+    \"\"\"\n+    # Formulas as in Sec 3 of Pison 2002, derived from Eq 4.2 in Croux 1999\n+    q_alpha = chi2.ppf(alpha, df=n_features)\n+    c_alpha = alpha / chi2.cdf(q_alpha, n_features + 2)\n+    return c_alpha\n+\n+\n def select_candidates(\n     X,\n     n_support,\n@@ -704,10 +741,10 @@ class MinCovDet(EmpiricalCovariance):\n     ...                                   size=500)\n     >>> cov = MinCovDet(random_state=0).fit(X)\n     >>> cov.covariance_\n-    array([[0.7411, 0.2535],\n-           [0.2535, 0.3053]])\n+    array([[0.8102, 0.2736],\n+           [0.2736, 0.3330]])\n     >>> cov.location_\n-    array([0.0813 , 0.0427])\n+    array([0.0769 , 0.0397])\n     \"\"\"\n \n     _parameter_constraints: dict = {\n@@ -787,8 +824,7 @@ def fit(self, X, y=None):\n     def correct_covariance(self, data):\n         \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n \n-        Correction using the empirical correction factor suggested\n-        by Rousseeuw and Van Driessen in [RVD]_.\n+        Correction using the asymptotic correction factor derived by [Croux1999]_.\n \n         Parameters\n         ----------\n@@ -804,24 +840,24 @@ def correct_covariance(self, data):\n \n         References\n         ----------\n-\n-        .. [RVD] A Fast Algorithm for the Minimum Covariance\n-            Determinant Estimator, 1999, American Statistical Association\n-            and the American Society for Quality, TECHNOMETRICS\n+        .. [Croux1999] Influence Function and Efficiency of the Minimum\n+            Covariance Determinant Scatter Matrix Estimator, 1999, Journal of\n+            Multivariate Analysis, Volume 71, Issue 2, Pages 161-190\n         \"\"\"\n \n         # Check that the covariance of the support data is not equal to 0.\n         # Otherwise self.dist_ = 0 and thus correction = 0.\n         n_samples = len(self.dist_)\n         n_support = np.sum(self.support_)\n+        n_features = self.raw_covariance_.shape[0]\n         if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n             raise ValueError(\n                 \"The covariance matrix of the support data \"\n                 \"is equal to 0, try to increase support_fraction\"\n             )\n-        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n-        covariance_corrected = self.raw_covariance_ * correction\n-        self.dist_ /= correction\n+        consistency_factor = _consistency_factor(n_features, n_support / n_samples)\n+        covariance_corrected = self.raw_covariance_ * consistency_factor\n+        self.dist_ /= consistency_factor\n         return covariance_corrected\n \n     def reweight_covariance(self, data):\n@@ -832,6 +868,9 @@ def reweight_covariance(self, data):\n         computing location and covariance estimates) described\n         in [RVDriessen]_.\n \n+        Corrects the re-weighted covariance to be consistent at the normal\n+        distribution, following [Croux1999]_.\n+\n         Parameters\n         ----------\n         data : array-like of shape (n_samples, n_features)\n@@ -857,9 +896,14 @@ def reweight_covariance(self, data):\n         .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n             Determinant Estimator, 1999, American Statistical Association\n             and the American Society for Quality, TECHNOMETRICS\n+\n+        .. [Croux1999] Influence Function and Efficiency of the Minimum\n+            Covariance Determinant Scatter Matrix Estimator, 1999, Journal of\n+            Multivariate Analysis, Volume 71, Issue 2, Pages 161-190\n         \"\"\"\n         n_samples, n_features = data.shape\n-        mask = self.dist_ < chi2(n_features).isf(0.025)\n+        quantile_threshold = 0.025\n+        mask = self.dist_ < chi2(n_features).isf(quantile_threshold)\n         if self.assume_centered:\n             location_reweighted = np.zeros(n_features)\n         else:\n@@ -869,7 +913,11 @@ def reweight_covariance(self, data):\n         )\n         support_reweighted = np.zeros(n_samples, dtype=bool)\n         support_reweighted[mask] = True\n-        self._set_covariance(covariance_reweighted)\n+        # Parameter alpha as in [Croux1999] Eq. 4.2\n+        consistency_factor = _consistency_factor(\n+            n_features=n_features, alpha=1 - quantile_threshold\n+        )\n+        self._set_covariance(covariance_reweighted * consistency_factor)\n         self.location_ = location_reweighted\n         self.support_ = support_reweighted\n         X_centered = data - self.location_\ndiff --git a/sklearn/covariance/tests/test_robust_covariance.py b/sklearn/covariance/tests/test_robust_covariance.py\nindex a7bd3996b9e4b..4a7590ef2c18c 100644\n--- a/sklearn/covariance/tests/test_robust_covariance.py\n+++ b/sklearn/covariance/tests/test_robust_covariance.py\n@@ -32,7 +32,7 @@ def test_mcd(global_random_seed):\n     launch_mcd_on_dataset(1700, 5, 800, 0.1, 0.1, 870, global_random_seed)\n \n     # 1D data set\n-    launch_mcd_on_dataset(500, 1, 100, 0.02, 0.02, 350, global_random_seed)\n+    launch_mcd_on_dataset(500, 1, 100, 0.10, 0.10, 350, global_random_seed)\n \n     # n_samples == n_features\n     launch_mcd_on_dataset(20, 20, 0, 0.1, 0.1, 15, global_random_seed)\n@@ -169,3 +169,30 @@ def test_mcd_increasing_det_warning(global_random_seed):\n     warn_msg = \"Determinant has increased\"\n     with pytest.warns(RuntimeWarning, match=warn_msg):\n         mcd.fit(X)\n+\n+\n+@pytest.mark.parametrize(\"n_samples,n_features\", [(2000, 10)])\n+def test_mincovdet_bias_on_normal(n_samples, n_features, global_random_seed):\n+    \"\"\"Check that MinCovDet does not underestimate the empirical\n+    variance on Gaussian data.\n+\n+    A large sample size and n_features makes the test robust.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/23162\n+    \"\"\"\n+    threshold = 0.985  # threshold for variance underesitmation\n+    x = np.random.randn(n_features, n_samples)\n+    # Assume centered data, to reduce test complexity\n+    var_emp = empirical_covariance(x.T, assume_centered=True).diagonal()\n+    cov_mcd = (\n+        MinCovDet(support_fraction=1.0, store_precision=False, assume_centered=True)\n+        .fit(x.T)\n+        .covariance_\n+    )\n+    var_mcd = np.diag(cov_mcd)\n+\n+    # compute mean ratio of variances\n+    mean_var_ratio = np.sum(var_mcd) / np.sum(var_emp)\n+\n+    assert mean_var_ratio > threshold, \"MinCovDet underestimates the Gaussian variance\"\n",
  "fail_to_pass": [
    "test_mincovdet_bias_on_normal"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/covariance/_elliptic_envelope.py",
    "sklearn/covariance/_robust_covariance.py",
    "sklearn/covariance/tests/test_robust_covariance.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-09-05T22:23:50Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32117",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/23162"
}