{
  "id": "scikit-learn__scikit-learn-30562",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "bf606a466502a62e2daaae3287b3133650dd36c3",
  "issue_number": 30440,
  "issue_title": "ENH Array API support for confusion_matrix",
  "issue_body": "#### Reference Issues/PRs\r\ntowards #26024\r\n\r\nEdit: This PR is now superseeded by #30562\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThis PR aims to add Array API support to `confusion_matrix()`. I have run the CUDA tests on Colab and they too, pass.\r\n\r\n@OmarManzoor @ogrisel @lesteve: do you want to have a look?",
  "pr_number": 30562,
  "pr_title": "ENH Array API support for confusion_matrix",
  "gold_patch": "diff --git a/doc/modules/array_api.rst b/doc/modules/array_api.rst\nindex 2f6e16a89a9ea..78eef9b392356 100644\n--- a/doc/modules/array_api.rst\n+++ b/doc/modules/array_api.rst\n@@ -141,6 +141,7 @@ Metrics\n -------\n \n - :func:`sklearn.metrics.accuracy_score`\n+- :func:`sklearn.metrics.confusion_matrix`\n - :func:`sklearn.metrics.d2_tweedie_score`\n - :func:`sklearn.metrics.explained_variance_score`\n - :func:`sklearn.metrics.f1_score`\ndiff --git a/doc/whats_new/upcoming_changes/array-api/30562.feature.rst b/doc/whats_new/upcoming_changes/array-api/30562.feature.rst\nnew file mode 100644\nindex 0000000000000..3c1a58d90bfe5\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/array-api/30562.feature.rst\n@@ -0,0 +1,2 @@\n+- :func:`sklearn.metrics.confusion_matrix` now supports Array API compatible inputs.\n+  By :user:`Stefanie Senger <StefanieSenger>`\ndiff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex 412231af2b8c9..992885a97e46c 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -29,6 +29,7 @@\n from sklearn.utils._array_api import (\n     _average,\n     _bincount,\n+    _convert_to_numpy,\n     _count_nonzero,\n     _find_matching_floating_dtype,\n     _is_numpy_namespace,\n@@ -413,7 +414,7 @@ def confusion_matrix(\n     y_pred : array-like of shape (n_samples,)\n         Estimated targets as returned by a classifier.\n \n-    labels : array-like of shape (n_classes), default=None\n+    labels : array-like of shape (n_classes,), default=None\n         List of labels to index the matrix. This may be used to reorder\n         or select a subset of labels.\n         If ``None`` is given, those that appear at least once\n@@ -475,28 +476,61 @@ def confusion_matrix(\n     >>> (tn, fp, fn, tp)\n     (0, 2, 1, 1)\n     \"\"\"\n-    y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred, sample_weight = _check_targets(\n-        y_true, y_pred, sample_weight\n+    xp, _, device_ = get_namespace_and_device(y_true, y_pred, labels, sample_weight)\n+    y_true = check_array(\n+        y_true,\n+        dtype=None,\n+        ensure_2d=False,\n+        ensure_all_finite=False,\n+        ensure_min_samples=0,\n     )\n+    y_pred = check_array(\n+        y_pred,\n+        dtype=None,\n+        ensure_2d=False,\n+        ensure_all_finite=False,\n+        ensure_min_samples=0,\n+    )\n+    # Convert the input arrays to NumPy (on CPU) irrespective of the original\n+    # namespace and device so as to be able to leverage the the efficient\n+    # counting operations implemented by SciPy in the coo_matrix constructor.\n+    # The final results will be converted back to the input namespace and device\n+    # for the sake of consistency with other metric functions with array API support.\n+    y_true = _convert_to_numpy(y_true, xp)\n+    y_pred = _convert_to_numpy(y_pred, xp)\n+    if sample_weight is None:\n+        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n+    else:\n+        sample_weight = _convert_to_numpy(sample_weight, xp)\n+\n+    if len(sample_weight) > 0:\n+        y_type, y_true, y_pred, sample_weight = _check_targets(\n+            y_true, y_pred, sample_weight\n+        )\n+    else:\n+        # This is needed to handle the special case where y_true, y_pred and\n+        # sample_weight are all empty.\n+        # In this case we don't pass sample_weight to _check_targets that would\n+        # check that sample_weight is not empty and we don't reuse the returned\n+        # sample_weight\n+        y_type, y_true, y_pred, _ = _check_targets(y_true, y_pred)\n+\n+    y_true, y_pred = attach_unique(y_true, y_pred)\n     if y_type not in (\"binary\", \"multiclass\"):\n         raise ValueError(\"%s is not supported\" % y_type)\n \n     if labels is None:\n         labels = unique_labels(y_true, y_pred)\n     else:\n-        labels = np.asarray(labels)\n+        labels = _convert_to_numpy(labels, xp)\n         n_labels = labels.size\n         if n_labels == 0:\n-            raise ValueError(\"'labels' should contains at least one label.\")\n+            raise ValueError(\"'labels' should contain at least one label.\")\n         elif y_true.size == 0:\n             return np.zeros((n_labels, n_labels), dtype=int)\n         elif len(np.intersect1d(y_true, labels)) == 0:\n             raise ValueError(\"At least one label specified must be in y_true\")\n \n-    if sample_weight is None:\n-        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n-\n     n_labels = labels.size\n     # If labels are not consecutive integers starting from zero, then\n     # y_true and y_pred must be converted into index form\n@@ -507,9 +541,9 @@ def confusion_matrix(\n         and y_pred.min() >= 0\n     )\n     if need_index_conversion:\n-        label_to_ind = {y: x for x, y in enumerate(labels)}\n-        y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n-        y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n+        label_to_ind = {label: index for index, label in enumerate(labels)}\n+        y_pred = np.array([label_to_ind.get(label, n_labels + 1) for label in y_pred])\n+        y_true = np.array([label_to_ind.get(label, n_labels + 1) for label in y_true])\n \n     # intersect y_pred, y_true with labels, eliminate items not in labels\n     ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n@@ -550,7 +584,7 @@ def confusion_matrix(\n             UserWarning,\n         )\n \n-    return cm\n+    return xp.asarray(cm, device=device_)\n \n \n @validate_params(\ndiff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\nindex c9fcd959c829c..f58b3b40ae0ed 100644\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -10,6 +10,7 @@\n from scipy.stats import bernoulli\n \n from sklearn import datasets, svm\n+from sklearn.base import config_context\n from sklearn.datasets import make_multilabel_classification\n from sklearn.exceptions import UndefinedMetricWarning\n from sklearn.metrics import (\n@@ -43,8 +44,16 @@\n from sklearn.model_selection import cross_val_score\n from sklearn.preprocessing import LabelBinarizer, label_binarize\n from sklearn.tree import DecisionTreeClassifier\n+from sklearn.utils._array_api import (\n+    device as array_api_device,\n+)\n+from sklearn.utils._array_api import (\n+    get_namespace,\n+    yield_namespace_device_dtype_combinations,\n+)\n from sklearn.utils._mocking import MockDataFrame\n from sklearn.utils._testing import (\n+    _array_api_for_tests,\n     assert_allclose,\n     assert_almost_equal,\n     assert_array_almost_equal,\n@@ -1269,7 +1278,7 @@ def test_confusion_matrix_multiclass_subset_labels():\n @pytest.mark.parametrize(\n     \"labels, err_msg\",\n     [\n-        ([], \"'labels' should contains at least one label.\"),\n+        ([], \"'labels' should contain at least one label.\"),\n         ([3, 4], \"At least one label specified must be in y_true\"),\n     ],\n     ids=[\"empty list\", \"unknown labels\"],\n@@ -1283,10 +1292,14 @@ def test_confusion_matrix_error(labels, err_msg):\n @pytest.mark.parametrize(\n     \"labels\", (None, [0, 1], [0, 1, 2]), ids=[\"None\", \"binary\", \"multiclass\"]\n )\n-def test_confusion_matrix_on_zero_length_input(labels):\n+@pytest.mark.parametrize(\n+    \"sample_weight\",\n+    (None, []),\n+)\n+def test_confusion_matrix_on_zero_length_input(labels, sample_weight):\n     expected_n_classes = len(labels) if labels else 0\n     expected = np.zeros((expected_n_classes, expected_n_classes), dtype=int)\n-    cm = confusion_matrix([], [], labels=labels)\n+    cm = confusion_matrix([], [], sample_weight=sample_weight, labels=labels)\n     assert_array_equal(cm, expected)\n \n \n@@ -3608,3 +3621,21 @@ def test_d2_brier_score_warning_on_less_than_two_samples():\n     warning_message = \"not well-defined with less than two samples\"\n     with pytest.warns(UndefinedMetricWarning, match=warning_message):\n         d2_brier_score(y_true, y_pred)\n+\n+\n+@pytest.mark.parametrize(\n+    \"array_namespace, device, _\", yield_namespace_device_dtype_combinations()\n+)\n+def test_confusion_matrix_array_api(array_namespace, device, _):\n+    \"\"\"Test that `confusion_matrix` works for all array types when `labels` are passed\n+    such that the inner boolean `need_index_conversion` evaluates to `True`.\"\"\"\n+    xp = _array_api_for_tests(array_namespace, device)\n+\n+    y_true = xp.asarray([1, 2, 3], device=device)\n+    y_pred = xp.asarray([4, 5, 6], device=device)\n+    labels = xp.asarray([1, 2, 3], device=device)\n+\n+    with config_context(array_api_dispatch=True):\n+        result = confusion_matrix(y_true, y_pred, labels=labels)\n+        assert get_namespace(result)[0] == get_namespace(y_pred)[0]\n+        assert array_api_device(result) == array_api_device(y_pred)\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex a2476aa2a2667..fe4aee88380a4 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -2225,6 +2225,10 @@ def check_array_api_metric_pairwise(metric, array_namespace, device, dtype_name)\n         check_array_api_multiclass_classification_metric,\n         check_array_api_multilabel_classification_metric,\n     ],\n+    confusion_matrix: [\n+        check_array_api_binary_classification_metric,\n+        check_array_api_multiclass_classification_metric,\n+    ],\n     f1_score: [\n         check_array_api_binary_classification_metric,\n         check_array_api_multiclass_classification_metric,\n",
  "fail_to_pass": [
    "test_confusion_matrix_on_zero_length_input",
    "test_confusion_matrix_array_api"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/tests/test_classification.py",
    "sklearn/metrics/tests/test_common.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-12-30T09:49:34Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30562",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/30440"
}