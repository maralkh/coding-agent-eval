{
  "id": "scikit-learn__scikit-learn-31701",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "a64b6b241d8456c600b64d0a5219da701da4efa3",
  "issue_number": 30886,
  "issue_title": "MNT Make sample_weight checking more consistent in regression metrics",
  "issue_body": "#### Reference Issues/PRs\r\nRef: https://github.com/scikit-learn/scikit-learn/pull/30787#issuecomment-2650501993\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\n`_check_reg_targets` will now perform `check_consistent_length` on `sample_weight` as well as `y_true` and `y_pred` as well as `_check_sample_weight`\r\n   * this means that all array checks are done `_check_reg_targets`, which means all checks are at the start and means we know who is raising errors relating to inputs\r\n   *  only 2 metrics performed `_check_sample_weight` but AFAICT other metrics that accept `sample_weight` would also benefit from this check\r\n\r\n#### Any other comments?\r\n\r\nNot sure of what extra tests to add as `_check_sample_weight` and `check_consistent_length` are both tested separately, and it seems redundant to check those again in the context of `_check_reg_targets`.\r\n\r\nI guess I could try a few different inputs and check that the result is the same as what `_check_sample_weight` gives ?\r\n\r\ncc @ogrisel \r\n\r\n\r\n",
  "pr_number": 31701,
  "pr_title": "MNT Add `_check_sample_weights` to classification metrics",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/31701.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/31701.fix.rst\nnew file mode 100644\nindex 0000000000000..2a790290a7691\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/31701.fix.rst\n@@ -0,0 +1,22 @@\n+\n+- Additional `sample_weight` checking has been added to\n+  :func:`metrics.accuracy_score`,\n+  :func:`metrics.balanced_accuracy_score`,\n+  :func:`metrics.brier_score_loss`,\n+  :func:`metrics.class_likelihood_ratios`,\n+  :func:`metrics.classification_report`,\n+  :func:`metrics.cohen_kappa_score`,\n+  :func:`metrics.confusion_matrix`,\n+  :func:`metrics.f1_score`,\n+  :func:`metrics.fbeta_score`,\n+  :func:`metrics.hamming_loss`,\n+  :func:`metrics.jaccard_score`,\n+  :func:`metrics.matthews_corrcoef`,\n+  :func:`metrics.multilabel_confusion_matrix`,\n+  :func:`metrics.precision_recall_fscore_support`,\n+  :func:`metrics.precision_score`,\n+  :func:`metrics.recall_score` and\n+  :func:`metrics.zero_one_loss`.\n+  `sample_weight` can only be 1D, consistent to `y_true` and `y_pred` in length,and\n+  all values must be finite and not complex.\n+  By :user:`Lucy Liu <lucyleeow>`.\ndiff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex 06503046790be..7a14b8de6bec9 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -66,7 +66,7 @@ def _check_zero_division(zero_division):\n         return np.nan\n \n \n-def _check_targets(y_true, y_pred):\n+def _check_targets(y_true, y_pred, sample_weight=None):\n     \"\"\"Check that y_true and y_pred belong to the same classification task.\n \n     This converts multiclass or binary types to a common shape, and raises a\n@@ -83,6 +83,8 @@ def _check_targets(y_true, y_pred):\n \n     y_pred : array-like\n \n+    sample_weight : array-like, default=None\n+\n     Returns\n     -------\n     type_true : one of {'multilabel-indicator', 'multiclass', 'binary'}\n@@ -92,11 +94,17 @@ def _check_targets(y_true, y_pred):\n     y_true : array or indicator matrix\n \n     y_pred : array or indicator matrix\n+\n+    sample_weight : array or None\n     \"\"\"\n-    xp, _ = get_namespace(y_true, y_pred)\n-    check_consistent_length(y_true, y_pred)\n+    xp, _ = get_namespace(y_true, y_pred, sample_weight)\n+    check_consistent_length(y_true, y_pred, sample_weight)\n     type_true = type_of_target(y_true, input_name=\"y_true\")\n     type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n+    if sample_weight is not None:\n+        sample_weight = _check_sample_weight(\n+            sample_weight, y_true, force_float_dtype=False\n+        )\n \n     y_type = {type_true, type_pred}\n     if y_type == {\"binary\", \"multiclass\"}:\n@@ -148,7 +156,7 @@ def _check_targets(y_true, y_pred):\n             y_pred = csr_matrix(y_pred)\n         y_type = \"multilabel-indicator\"\n \n-    return y_type, y_true, y_pred\n+    return y_type, y_true, y_pred, sample_weight\n \n \n def _validate_multiclass_probabilistic_prediction(\n@@ -200,6 +208,9 @@ def _validate_multiclass_probabilistic_prediction(\n         raise ValueError(f\"y_prob contains values lower than 0: {y_prob.min()}\")\n \n     check_consistent_length(y_prob, y_true, sample_weight)\n+    if sample_weight is not None:\n+        _check_sample_weight(sample_weight, y_true, force_float_dtype=False)\n+\n     lb = LabelBinarizer()\n \n     if labels is not None:\n@@ -356,8 +367,9 @@ def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):\n     xp, _, device = get_namespace_and_device(y_true, y_pred, sample_weight)\n     # Compute accuracy for each possible representation\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n-    check_consistent_length(y_true, y_pred, sample_weight)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n \n     if y_type.startswith(\"multilabel\"):\n         differing_labels = _count_nonzero(y_true - y_pred, xp=xp, device=device, axis=1)\n@@ -464,7 +476,9 @@ def confusion_matrix(\n     (0, 2, 1, 1)\n     \"\"\"\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n     if y_type not in (\"binary\", \"multiclass\"):\n         raise ValueError(\"%s is not supported\" % y_type)\n \n@@ -482,10 +496,6 @@ def confusion_matrix(\n \n     if sample_weight is None:\n         sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n-    else:\n-        sample_weight = np.asarray(sample_weight)\n-\n-    check_consistent_length(y_true, y_pred, sample_weight)\n \n     n_labels = labels.size\n     # If labels are not consecutive integers starting from zero, then\n@@ -654,11 +664,10 @@ def multilabel_confusion_matrix(\n             [1, 2]]])\n     \"\"\"\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    xp, _, device_ = get_namespace_and_device(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n-    if sample_weight is not None:\n-        sample_weight = column_or_1d(sample_weight, device=device_)\n-    check_consistent_length(y_true, y_pred, sample_weight)\n+    xp, _, device_ = get_namespace_and_device(y_true, y_pred, sample_weight)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n \n     if y_type not in (\"binary\", \"multiclass\", \"multilabel-indicator\"):\n         raise ValueError(\"%s is not supported\" % y_type)\n@@ -1171,8 +1180,9 @@ def matthews_corrcoef(y_true, y_pred, *, sample_weight=None):\n     -0.33\n     \"\"\"\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n-    check_consistent_length(y_true, y_pred, sample_weight)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n     if y_type not in {\"binary\", \"multiclass\"}:\n         raise ValueError(\"%s is not supported\" % y_type)\n \n@@ -1759,7 +1769,7 @@ def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):\n         raise ValueError(\"average has to be one of \" + str(average_options))\n \n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n+    y_type, y_true, y_pred, _ = _check_targets(y_true, y_pred)\n     # Convert to Python primitive type to avoid NumPy type / Python str\n     # comparison. See https://github.com/numpy/numpy/issues/6784\n     present_labels = _tolist(unique_labels(y_true, y_pred))\n@@ -2227,7 +2237,9 @@ class are present in `y_true`): both likelihood ratios are undefined.\n     # remove `FutureWarning`, and the Warns section in the docstring should not mention\n     # `raise_warning` anymore.\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n     if y_type != \"binary\":\n         raise ValueError(\n             \"class_likelihood_ratios only supports binary classification \"\n@@ -2945,7 +2957,9 @@ class 2       1.00      0.67      0.80         3\n     \"\"\"\n \n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n \n     if labels is None:\n         labels = unique_labels(y_true, y_pred)\n@@ -3134,15 +3148,15 @@ def hamming_loss(y_true, y_pred, *, sample_weight=None):\n     0.75\n     \"\"\"\n     y_true, y_pred = attach_unique(y_true, y_pred)\n-    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n-    check_consistent_length(y_true, y_pred, sample_weight)\n+    y_type, y_true, y_pred, sample_weight = _check_targets(\n+        y_true, y_pred, sample_weight\n+    )\n \n     xp, _, device = get_namespace_and_device(y_true, y_pred, sample_weight)\n \n     if sample_weight is None:\n         weight_average = 1.0\n     else:\n-        sample_weight = xp.asarray(sample_weight, device=device)\n         weight_average = _average(sample_weight, xp=xp)\n \n     if y_type.startswith(\"multilabel\"):\n@@ -3440,6 +3454,8 @@ def _validate_binary_probabilistic_prediction(y_true, y_prob, sample_weight, pos\n     assert_all_finite(y_prob)\n \n     check_consistent_length(y_prob, y_true, sample_weight)\n+    if sample_weight is not None:\n+        _check_sample_weight(sample_weight, y_true, force_float_dtype=False)\n \n     y_type = type_of_target(y_true, input_name=\"y_true\")\n     if y_type != \"binary\":\ndiff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\nindex b66353e5ecfab..7bec019bdbe43 100644\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -596,7 +596,7 @@ def test_multilabel_confusion_matrix_errors():\n     # Bad sample_weight\n     with pytest.raises(ValueError, match=\"inconsistent numbers of samples\"):\n         multilabel_confusion_matrix(y_true, y_pred, sample_weight=[1, 2])\n-    with pytest.raises(ValueError, match=\"should be a 1d array\"):\n+    with pytest.raises(ValueError, match=\"Sample weights must be 1D array or scalar\"):\n         multilabel_confusion_matrix(\n             y_true, y_pred, sample_weight=[[1, 2, 3], [2, 3, 4], [3, 4, 5]]\n         )\n@@ -2541,7 +2541,7 @@ def test__check_targets():\n                         _check_targets(y1, y2)\n \n         else:\n-            merged_type, y1out, y2out = _check_targets(y1, y2)\n+            merged_type, y1out, y2out, _ = _check_targets(y1, y2)\n             assert merged_type == expected\n             if merged_type.startswith(\"multilabel\"):\n                 assert y1out.format == \"csr\"\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 5cdc2ead54740..a2476aa2a2667 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -881,6 +881,38 @@ def test_format_invariance_with_1d_vectors(name):\n                     metric(y1_row, y2_row)\n \n \n+@pytest.mark.parametrize(\"metric\", CLASSIFICATION_METRICS.values())\n+def test_classification_with_invalid_sample_weight(metric):\n+    # Check invalid `sample_weight` raises correct error\n+    random_state = check_random_state(0)\n+    n_samples = 20\n+    y1 = random_state.randint(0, 2, size=(n_samples,))\n+    y2 = random_state.randint(0, 2, size=(n_samples,))\n+\n+    sample_weight = random_state.random_sample(size=(n_samples - 1,))\n+    with pytest.raises(ValueError, match=\"Found input variables with inconsistent\"):\n+        metric(y1, y2, sample_weight=sample_weight)\n+\n+    sample_weight = random_state.random_sample(size=(n_samples,))\n+    sample_weight[0] = np.inf\n+    with pytest.raises(ValueError, match=\"Input sample_weight contains infinity\"):\n+        metric(y1, y2, sample_weight=sample_weight)\n+\n+    sample_weight[0] = np.nan\n+    with pytest.raises(ValueError, match=\"Input sample_weight contains NaN\"):\n+        metric(y1, y2, sample_weight=sample_weight)\n+\n+    sample_weight = np.array([1 + 2j, 3 + 4j, 5 + 7j])\n+    with pytest.raises(ValueError, match=\"Complex data not supported\"):\n+        metric(y1[:3], y2[:3], sample_weight=sample_weight)\n+\n+    sample_weight = random_state.random_sample(size=(n_samples * 2,)).reshape(\n+        (n_samples, 2)\n+    )\n+    with pytest.raises(ValueError, match=\"Sample weights must be 1D array or scalar\"):\n+        metric(y1, y2, sample_weight=sample_weight)\n+\n+\n @pytest.mark.parametrize(\n     \"name\", sorted(set(CLASSIFICATION_METRICS) - METRIC_UNDEFINED_BINARY_MULTICLASS)\n )\ndiff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py\nindex acaac8c9f6c84..df4913df2135b 100644\n--- a/sklearn/utils/validation.py\n+++ b/sklearn/utils/validation.py\n@@ -2134,7 +2134,13 @@ def _check_psd_eigenvalues(lambdas, enable_warnings=False):\n \n \n def _check_sample_weight(\n-    sample_weight, X, *, dtype=None, ensure_non_negative=False, copy=False\n+    sample_weight,\n+    X,\n+    *,\n+    dtype=None,\n+    force_float_dtype=True,\n+    ensure_non_negative=False,\n+    copy=False,\n ):\n     \"\"\"Validate sample weights.\n \n@@ -2162,6 +2168,10 @@ def _check_sample_weight(\n         If `dtype` is not `{np.float32, np.float64, None}`, then output will\n         be `np.float64`.\n \n+    force_float_dtype : bool, default=True\n+        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n+        dtype or None.\n+\n     ensure_non_negative : bool, default=False,\n         Whether or not the weights are expected to be non-negative.\n \n@@ -2185,7 +2195,7 @@ def _check_sample_weight(\n     float_dtypes = (\n         [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n     )\n-    if dtype is not None and dtype not in float_dtypes:\n+    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n         dtype = max_float_type\n \n     if sample_weight is None:\n@@ -2193,7 +2203,7 @@ def _check_sample_weight(\n     elif isinstance(sample_weight, numbers.Number):\n         sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n     else:\n-        if dtype is None:\n+        if force_float_dtype and dtype is None:\n             dtype = float_dtypes\n         sample_weight = check_array(\n             sample_weight,\n",
  "fail_to_pass": [
    "test_classification_with_invalid_sample_weight"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/tests/test_classification.py",
    "sklearn/metrics/tests/test_common.py",
    "sklearn/utils/validation.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-07-04T12:12:08Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31701",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/30886"
}