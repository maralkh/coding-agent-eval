{
  "id": "scikit-learn__scikit-learn-32549",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "ff9da6428aafc802b6d3afe8df6b5cb75b2d39b0",
  "issue_number": 31187,
  "issue_title": "FIX Raise on empty inputs in accuracy_score",
  "issue_body": "#### Reference Issues/PRs\r\ntowards #29048\r\nsuperseded by #32549\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThis PR adds a `replace_undefined_by` param to `accuracy_score` to deal with empty y_true and y_pred.\r\nAlso adds tests.\r\n\r\n#### Open Question\r\nNote that before this PR `accuracy_score` returned like this:\r\n`accuracy_score(np.array([]), np.array([]))`\r\n> nan\r\n\r\n`accuracy_score(np.array([]), np.array([]), normalize=False)`\r\n> 0.0\r\n\r\nI would like to consider this inconsistency as a bug and fix this with this PR for the next release without deprecation, so it comes faster. Would this be okay? How would you see that, @adrinjalali?",
  "pr_number": 32549,
  "pr_title": "FIX classification metrics raise on empty input",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/32549.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/32549.fix.rst\nnew file mode 100644\nindex 0000000000000..070e3d1e7fefe\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/32549.fix.rst\n@@ -0,0 +1,7 @@\n+- All classification metrics now raise a `ValueError` when required input arrays\n+  (`y_pred`, `y_true`, `y1`, `y2`, `pred_decision`, or `y_proba`) are empty.\n+  Previously, `accuracy_score`, `class_likelihood_ratios`, `classification_report`,\n+  `confusion_matrix`, `hamming_loss`, `jaccard_score`, `matthews_corrcoef`,\n+  `multilabel_confusion_matrix`, and `precision_recall_fscore_support` did not raise\n+  this error consistently.\n+  By :user:`Stefanie Senger <StefanieSenger>`.\ndiff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex 89df0da3ef861..b9bc8129f5a6a 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -107,6 +107,12 @@ def _check_targets(y_true, y_pred, sample_weight=None):\n     check_consistent_length(y_true, y_pred, sample_weight)\n     type_true = type_of_target(y_true, input_name=\"y_true\")\n     type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n+    for array in [y_true, y_pred]:\n+        if _num_samples(array) < 1:\n+            raise ValueError(\n+                \"Found empty input array (e.g., `y_true` or `y_pred`) while a minimum \"\n+                \"of 1 sample is required.\"\n+            )\n     if sample_weight is not None:\n         sample_weight = _check_sample_weight(\n             sample_weight, y_true, force_float_dtype=False\n@@ -379,12 +385,11 @@ def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):\n \n     Returns\n     -------\n-    score : float or int\n-        If ``normalize == True``, return the fraction of correctly\n-        classified samples (float), else returns the number of correctly\n-        classified samples (int).\n+    score : float\n+        If ``normalize == True``, returns the fraction of correctly classified samples,\n+        else returns the number of correctly classified samples.\n \n-        The best performance is 1 with ``normalize == True`` and the number\n+        The best performance is 1.0 with ``normalize == True`` and the number\n         of samples with ``normalize == False``.\n \n     See Also\n@@ -1315,9 +1320,8 @@ def matthews_corrcoef(y_true, y_pred, *, sample_weight=None):\n def zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None):\n     \"\"\"Zero-one classification loss.\n \n-    If normalize is ``True``, return the fraction of misclassifications\n-    (float), else it returns the number of misclassifications (int). The best\n-    performance is 0.\n+    If normalize is ``True``, returns the fraction of misclassifications, else returns\n+    the number of misclassifications. The best performance is 0.\n \n     Read more in the :ref:`User Guide <zero_one_loss>`.\n \n@@ -1340,9 +1344,9 @@ def zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None):\n \n     Returns\n     -------\n-    loss : float or int,\n-        If ``normalize == True``, return the fraction of misclassifications\n-        (float), else it returns the number of misclassifications (int).\n+    loss : float\n+        If ``normalize == True``, returns the fraction of misclassifications, else\n+        returns the number of misclassifications.\n \n     See Also\n     --------\n@@ -2291,7 +2295,7 @@ class after being classified as negative. This is the case when the\n \n     Returns\n     -------\n-    (positive_likelihood_ratio, negative_likelihood_ratio) : tuple\n+    (positive_likelihood_ratio, negative_likelihood_ratio) : tuple of float\n         A tuple of two floats, the first containing the positive likelihood ratio (LR+)\n         and the second the negative likelihood ratio (LR-).\n \n@@ -3219,8 +3223,8 @@ def hamming_loss(y_true, y_pred, *, sample_weight=None):\n \n     Returns\n     -------\n-    loss : float or int\n-        Return the average Hamming loss between element of ``y_true`` and\n+    loss : float\n+        Returns the average Hamming loss between element of ``y_true`` and\n         ``y_pred``.\n \n     See Also\ndiff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\nindex 4bf51b8c6b832..b8dc67b298be7 100644\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -181,36 +181,6 @@ def test_classification_report_dictionary_output():\n     assert isinstance(expected_report[\"macro avg\"][\"support\"], int)\n \n \n-def test_classification_report_output_dict_empty_input():\n-    report = classification_report(y_true=[], y_pred=[], output_dict=True)\n-    expected_report = {\n-        \"accuracy\": 0.0,\n-        \"macro avg\": {\n-            \"f1-score\": np.nan,\n-            \"precision\": np.nan,\n-            \"recall\": np.nan,\n-            \"support\": 0,\n-        },\n-        \"weighted avg\": {\n-            \"f1-score\": np.nan,\n-            \"precision\": np.nan,\n-            \"recall\": np.nan,\n-            \"support\": 0,\n-        },\n-    }\n-    assert isinstance(report, dict)\n-    # assert the 2 dicts are equal.\n-    assert report.keys() == expected_report.keys()\n-    for key in expected_report:\n-        if key == \"accuracy\":\n-            assert isinstance(report[key], float)\n-            assert report[key] == expected_report[key]\n-        else:\n-            assert report[key].keys() == expected_report[key].keys()\n-            for metric in expected_report[key]:\n-                assert_almost_equal(expected_report[key][metric], report[key][metric])\n-\n-\n @pytest.mark.parametrize(\"zero_division\", [\"warn\", 0, 1, np.nan])\n def test_classification_report_zero_division_warning(zero_division):\n     y_true, y_pred = [\"a\", \"b\", \"c\"], [\"a\", \"b\", \"d\"]\n@@ -1293,20 +1263,6 @@ def test_confusion_matrix_error(labels, err_msg):\n         confusion_matrix(y_true, y_pred, labels=labels)\n \n \n-@pytest.mark.parametrize(\n-    \"labels\", (None, [0, 1], [0, 1, 2]), ids=[\"None\", \"binary\", \"multiclass\"]\n-)\n-@pytest.mark.parametrize(\n-    \"sample_weight\",\n-    (None, []),\n-)\n-def test_confusion_matrix_on_zero_length_input(labels, sample_weight):\n-    expected_n_classes = len(labels) if labels else 0\n-    expected = np.zeros((expected_n_classes, expected_n_classes), dtype=int)\n-    cm = confusion_matrix([], [], sample_weight=sample_weight, labels=labels)\n-    assert_array_equal(cm, expected)\n-\n-\n def test_confusion_matrix_dtype():\n     y = [0, 1, 1]\n     weight = np.ones(len(y))\n@@ -2586,6 +2542,12 @@ def test__check_targets():\n         _check_targets(y1, y2)\n \n \n+def test__check_targets_raises_on_empty_inputs():\n+    msg = \"Found empty input array (e.g., `y_true` or `y_pred`) while a minimum of 1\"\n+    with pytest.raises(ValueError, match=re.escape(msg)):\n+        _check_targets(np.array([]), np.array([]))\n+\n+\n def test__check_targets_multiclass_with_both_y_true_and_y_pred_binary():\n     # https://github.com/scikit-learn/scikit-learn/issues/8098\n     y_true = [0, 1]\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 525dcc90cf67a..7eebf5a17ee3f 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -1,4 +1,5 @@\n import math\n+import re\n from functools import partial\n from inspect import signature\n from itertools import chain, permutations, product\n@@ -14,6 +15,7 @@\n     average_precision_score,\n     balanced_accuracy_score,\n     brier_score_loss,\n+    classification_report,\n     cohen_kappa_score,\n     confusion_matrix,\n     coverage_error,\n@@ -892,6 +894,19 @@ def test_format_invariance_with_1d_vectors(name):\n                     metric(y1_row, y2_row)\n \n \n+CLASSIFICATION_METRICS_REPORT = {\n+    **CLASSIFICATION_METRICS,\n+    \"classification_report\": classification_report,\n+}\n+\n+\n+@pytest.mark.parametrize(\"metric\", CLASSIFICATION_METRICS_REPORT.values())\n+def test_classification_metrics_raise_on_empty_input(metric):\n+    msg = \"Found empty input array (e.g., `y_true` or `y_pred`) while a minimum of 1\"\n+    with pytest.raises(ValueError, match=re.escape(msg)):\n+        metric(np.array([]), np.array([]))\n+\n+\n @pytest.mark.parametrize(\"metric\", CLASSIFICATION_METRICS.values())\n def test_classification_with_invalid_sample_weight(metric):\n     # Check invalid `sample_weight` raises correct error\ndiff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py\nindex 02df5b93d6115..a4b6b21470061 100644\n--- a/sklearn/model_selection/tests/test_split.py\n+++ b/sklearn/model_selection/tests/test_split.py\n@@ -1953,7 +1953,7 @@ def test_nested_cv():\n         LeaveOneOut(),\n         GroupKFold(n_splits=3),\n         StratifiedKFold(),\n-        StratifiedGroupKFold(),\n+        StratifiedGroupKFold(n_splits=3),\n         StratifiedShuffleSplit(n_splits=3, random_state=0),\n     ]\n \n",
  "fail_to_pass": [
    "test__check_targets_raises_on_empty_inputs",
    "test_classification_metrics_raise_on_empty_input"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/tests/test_classification.py",
    "sklearn/metrics/tests/test_common.py",
    "sklearn/model_selection/tests/test_split.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-10-21T18:57:02Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32549",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/31187"
}