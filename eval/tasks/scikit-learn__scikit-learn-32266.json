{
  "id": "scikit-learn__scikit-learn-32266",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "289c0e1ba418bd54b3c484c81ee784f1ec9814f2",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 32266,
  "pr_title": "MNT Clean-up deprecations for 1.8: Imputer drops empty feature when keep_empty_features=False even if strategy='constant'",
  "gold_patch": "diff --git a/sklearn/impute/_base.py b/sklearn/impute/_base.py\nindex d8a63330570e2..c1c480de1f387 100644\n--- a/sklearn/impute/_base.py\n+++ b/sklearn/impute/_base.py\n@@ -241,11 +241,6 @@ class SimpleImputer(_BaseImputer):\n \n         .. versionadded:: 1.2\n \n-        .. versionchanged:: 1.6\n-            Currently, when `keep_empty_feature=False` and `strategy=\"constant\"`,\n-            empty features are not dropped. This behaviour will change in version\n-            1.8. Set `keep_empty_feature=True` to preserve this behaviour.\n-\n     Attributes\n     ----------\n     statistics_ : array of shape (n_features,)\n@@ -413,7 +408,7 @@ def _validate_input(self, X, in_fit):\n                     \"Make sure that both dtypes are of the same kind.\"\n                 )\n             elif not in_fit:\n-                fill_value_dtype = self.statistics_.dtype\n+                fill_value_dtype = self._fill_dtype\n                 err_msg = (\n                     f\"The dtype of the filling value (i.e. {fill_value_dtype!r}) \"\n                     f\"cannot be cast to the input data that is {X.dtype!r}. \"\n@@ -461,6 +456,8 @@ def fit(self, X, y=None):\n         else:\n             fill_value = self.fill_value\n \n+        self._fill_dtype = X.dtype\n+\n         if sp.issparse(X):\n             self.statistics_ = self._sparse_fit(\n                 X, self.strategy, self.missing_values, fill_value\n@@ -481,22 +478,15 @@ def _sparse_fit(self, X, strategy, missing_values, fill_value):\n         statistics = np.empty(X.shape[1])\n \n         if strategy == \"constant\":\n-            # TODO(1.8): Remove FutureWarning and add `np.nan` as a statistic\n-            # for empty features to drop them later.\n-            if not self.keep_empty_features and any(\n-                [all(missing_mask[:, i].data) for i in range(missing_mask.shape[1])]\n-            ):\n-                warnings.warn(\n-                    \"Currently, when `keep_empty_feature=False` and \"\n-                    '`strategy=\"constant\"`, empty features are not dropped. '\n-                    \"This behaviour will change in version 1.8. Set \"\n-                    \"`keep_empty_feature=True` to preserve this behaviour.\",\n-                    FutureWarning,\n-                )\n-\n             # for constant strategy, self.statistics_ is used to store\n-            # fill_value in each column\n+            # fill_value in each column, or np.nan for columns to drop\n             statistics.fill(fill_value)\n+\n+            if not self.keep_empty_features:\n+                for i in range(missing_mask.shape[1]):\n+                    if all(missing_mask[:, i].data):\n+                        statistics[i] = np.nan\n+\n         else:\n             for i in range(X.shape[1]):\n                 column = X.data[X.indptr[i] : X.indptr[i + 1]]\n@@ -584,20 +574,16 @@ def _dense_fit(self, X, strategy, missing_values, fill_value):\n \n         # Constant\n         elif strategy == \"constant\":\n-            # TODO(1.8): Remove FutureWarning and add `np.nan` as a statistic\n-            # for empty features to drop them later.\n-            if not self.keep_empty_features and ma.getmask(masked_X).all(axis=0).any():\n-                warnings.warn(\n-                    \"Currently, when `keep_empty_feature=False` and \"\n-                    '`strategy=\"constant\"`, empty features are not dropped. '\n-                    \"This behaviour will change in version 1.8. Set \"\n-                    \"`keep_empty_feature=True` to preserve this behaviour.\",\n-                    FutureWarning,\n-                )\n-\n             # for constant strategy, self.statistcs_ is used to store\n-            # fill_value in each column\n-            return np.full(X.shape[1], fill_value, dtype=X.dtype)\n+            # fill_value in each column, or np.nan for columns to drop\n+            statistics = np.full(X.shape[1], fill_value, dtype=np.object_)\n+\n+            if not self.keep_empty_features:\n+                for i in range(missing_mask.shape[1]):\n+                    if missing_mask[:, i].all():\n+                        statistics[i] = np.nan\n+\n+            return statistics\n \n         # Custom\n         elif isinstance(strategy, Callable):\n@@ -635,14 +621,16 @@ def transform(self, X):\n         missing_mask = _get_mask(X, self.missing_values)\n \n         # Decide whether to keep missing features\n-        if self.strategy == \"constant\" or self.keep_empty_features:\n-            valid_statistics = statistics\n+        if self.keep_empty_features:\n+            valid_statistics = statistics.astype(self._fill_dtype, copy=False)\n             valid_statistics_indexes = None\n         else:\n             # same as np.isnan but also works for object dtypes\n             invalid_mask = _get_mask(statistics, np.nan)\n             valid_mask = np.logical_not(invalid_mask)\n-            valid_statistics = statistics[valid_mask]\n+            valid_statistics = statistics[valid_mask].astype(\n+                self._fill_dtype, copy=False\n+            )\n             valid_statistics_indexes = np.flatnonzero(valid_mask)\n \n             if invalid_mask.any():\n@@ -676,7 +664,7 @@ def transform(self, X):\n                     np.arange(len(X.indptr) - 1, dtype=int), np.diff(X.indptr)\n                 )[mask]\n \n-                X.data[mask] = valid_statistics[indexes].astype(X.dtype, copy=False)\n+                X.data[mask] = valid_statistics[indexes]\n         else:\n             # use mask computed before eliminating invalid mask\n             if valid_statistics_indexes is None:\ndiff --git a/sklearn/impute/_iterative.py b/sklearn/impute/_iterative.py\nindex 4e235755a507c..90b5bda65521a 100644\n--- a/sklearn/impute/_iterative.py\n+++ b/sklearn/impute/_iterative.py\n@@ -637,12 +637,6 @@ def _initial_imputation(self, X, in_fit=False):\n         X_missing_mask = _get_mask(X, self.missing_values)\n         mask_missing_values = X_missing_mask.copy()\n \n-        # TODO (1.8): remove this once the deprecation is removed. In the meantime,\n-        # we need to catch the warning to avoid false positives.\n-        catch_warning = (\n-            self.initial_strategy == \"constant\" and not self.keep_empty_features\n-        )\n-\n         if self.initial_imputer_ is None:\n             self.initial_imputer_ = SimpleImputer(\n                 missing_values=self.missing_values,\n@@ -651,23 +645,10 @@ def _initial_imputation(self, X, in_fit=False):\n                 keep_empty_features=self.keep_empty_features,\n             ).set_output(transform=\"default\")\n \n-            # TODO (1.8): remove this once the deprecation is removed to keep only\n-            # the code in the else case.\n-            if catch_warning:\n-                with warnings.catch_warnings():\n-                    warnings.simplefilter(\"ignore\", FutureWarning)\n-                    X_filled = self.initial_imputer_.fit_transform(X)\n-            else:\n-                X_filled = self.initial_imputer_.fit_transform(X)\n+            X_filled = self.initial_imputer_.fit_transform(X)\n+\n         else:\n-            # TODO (1.8): remove this once the deprecation is removed to keep only\n-            # the code in the else case.\n-            if catch_warning:\n-                with warnings.catch_warnings():\n-                    warnings.simplefilter(\"ignore\", FutureWarning)\n-                    X_filled = self.initial_imputer_.transform(X)\n-            else:\n-                X_filled = self.initial_imputer_.transform(X)\n+            X_filled = self.initial_imputer_.transform(X)\n \n         if in_fit:\n             self._is_empty_feature = np.all(mask_missing_values, axis=0)\n@@ -677,15 +658,6 @@ def _initial_imputation(self, X, in_fit=False):\n             Xt = X[:, ~self._is_empty_feature]\n             mask_missing_values = mask_missing_values[:, ~self._is_empty_feature]\n \n-            if self.initial_imputer_.get_params()[\"strategy\"] == \"constant\":\n-                # The constant strategy has a specific behavior and preserve empty\n-                # features even with ``keep_empty_features=False``. We need to drop\n-                # the column for consistency.\n-                # TODO (1.8): remove this `if` branch once the following issue is\n-                # addressed:\n-                # https://github.com/scikit-learn/scikit-learn/issues/29827\n-                X_filled = X_filled[:, ~self._is_empty_feature]\n-\n         else:\n             # mark empty features as not missing and keep the original\n             # imputation\ndiff --git a/sklearn/impute/tests/test_impute.py b/sklearn/impute/tests/test_impute.py\nindex 4116964c49a7a..013fd7eb8a810 100644\n--- a/sklearn/impute/tests/test_impute.py\n+++ b/sklearn/impute/tests/test_impute.py\n@@ -410,26 +410,29 @@ def test_imputation_constant_error_invalid_type(X_data, missing_value):\n         imputer.fit_transform(X)\n \n \n-# TODO (1.8): check that `keep_empty_features=False` drop the\n-# empty features due to the behaviour change.\n-def test_imputation_constant_integer():\n+@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n+def test_imputation_constant_integer(keep_empty_features):\n     # Test imputation using the constant strategy on integers\n     X = np.array([[-1, 2, 3, -1], [4, -1, 5, -1], [6, 7, -1, -1], [8, 9, 0, -1]])\n \n     X_true = np.array([[0, 2, 3, 0], [4, 0, 5, 0], [6, 7, 0, 0], [8, 9, 0, 0]])\n+    if not keep_empty_features:\n+        X_true = X_true[:, :-1]\n \n     imputer = SimpleImputer(\n-        missing_values=-1, strategy=\"constant\", fill_value=0, keep_empty_features=True\n+        missing_values=-1,\n+        strategy=\"constant\",\n+        fill_value=0,\n+        keep_empty_features=keep_empty_features,\n     )\n     X_trans = imputer.fit_transform(X)\n \n     assert_array_equal(X_trans, X_true)\n \n \n-# TODO (1.8): check that `keep_empty_features=False` drop the\n-# empty features due to the behaviour change.\n @pytest.mark.parametrize(\"array_constructor\", CSR_CONTAINERS + [np.asarray])\n-def test_imputation_constant_float(array_constructor):\n+@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n+def test_imputation_constant_float(array_constructor, keep_empty_features):\n     # Test imputation using the constant strategy on floats\n     X = np.array(\n         [\n@@ -443,23 +446,24 @@ def test_imputation_constant_float(array_constructor):\n     X_true = np.array(\n         [[-1, 1.1, 0, -1], [1.2, -1, 1.3, -1], [0, 0, -1, -1], [1.4, 1.5, 0, -1]]\n     )\n+    if not keep_empty_features:\n+        X_true = X_true[:, :-1]\n \n     X = array_constructor(X)\n \n     X_true = array_constructor(X_true)\n \n     imputer = SimpleImputer(\n-        strategy=\"constant\", fill_value=-1, keep_empty_features=True\n+        strategy=\"constant\", fill_value=-1, keep_empty_features=keep_empty_features\n     )\n     X_trans = imputer.fit_transform(X)\n \n     assert_allclose_dense_sparse(X_trans, X_true)\n \n \n-# TODO (1.8): check that `keep_empty_features=False` drop the\n-# empty features due to the behaviour change.\n @pytest.mark.parametrize(\"marker\", [None, np.nan, \"NAN\", \"\", 0])\n-def test_imputation_constant_object(marker):\n+@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n+def test_imputation_constant_object(marker, keep_empty_features):\n     # Test imputation using the constant strategy on objects\n     X = np.array(\n         [\n@@ -480,22 +484,23 @@ def test_imputation_constant_object(marker):\n         ],\n         dtype=object,\n     )\n+    if not keep_empty_features:\n+        X_true = X_true[:, :-1]\n \n     imputer = SimpleImputer(\n         missing_values=marker,\n         strategy=\"constant\",\n         fill_value=\"missing\",\n-        keep_empty_features=True,\n+        keep_empty_features=keep_empty_features,\n     )\n     X_trans = imputer.fit_transform(X)\n \n     assert_array_equal(X_trans, X_true)\n \n \n-# TODO (1.8): check that `keep_empty_features=False` drop the\n-# empty features due to the behaviour change.\n @pytest.mark.parametrize(\"dtype\", [object, \"category\"])\n-def test_imputation_constant_pandas(dtype):\n+@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n+def test_imputation_constant_pandas(dtype, keep_empty_features):\n     # Test imputation using the constant strategy on pandas df\n     pd = pytest.importorskip(\"pandas\")\n \n@@ -512,8 +517,12 @@ def test_imputation_constant_pandas(dtype):\n         ],\n         dtype=object,\n     )\n+    if not keep_empty_features:\n+        X_true = X_true[:, :-1]\n \n-    imputer = SimpleImputer(strategy=\"constant\", keep_empty_features=True)\n+    imputer = SimpleImputer(\n+        strategy=\"constant\", keep_empty_features=keep_empty_features\n+    )\n     X_trans = imputer.fit_transform(df)\n \n     assert_array_equal(X_trans, X_true)\n@@ -1567,9 +1576,8 @@ def test_iterative_imputer_keep_empty_features(initial_strategy):\n     assert_allclose(X_imputed[:, 1], 0)\n \n \n-# TODO (1.8): check that `keep_empty_features=False` drop the\n-# empty features due to the behaviour change.\n-def test_iterative_imputer_constant_fill_value():\n+@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n+def test_iterative_imputer_constant_fill_value(keep_empty_features):\n     \"\"\"Check that we propagate properly the parameter `fill_value`.\"\"\"\n     X = np.array([[-1, 2, 3, -1], [4, -1, 5, -1], [6, 7, -1, -1], [8, 9, 0, -1]])\n \n@@ -1579,10 +1587,15 @@ def test_iterative_imputer_constant_fill_value():\n         initial_strategy=\"constant\",\n         fill_value=fill_value,\n         max_iter=0,\n-        keep_empty_features=True,\n+        keep_empty_features=keep_empty_features,\n     )\n     imputer.fit_transform(X)\n-    assert_array_equal(imputer.initial_imputer_.statistics_, fill_value)\n+\n+    if keep_empty_features:\n+        assert_array_equal(imputer.initial_imputer_.statistics_, fill_value)\n+    else:\n+        assert_array_equal(imputer.initial_imputer_.statistics_[:-1], fill_value)\n+        assert np.isnan(imputer.initial_imputer_.statistics_[-1])\n \n \n def test_iterative_imputer_min_max_value_remove_empty():\n@@ -1761,37 +1774,6 @@ def test_imputer_transform_preserves_numeric_dtype(dtype_test):\n     assert X_trans.dtype == dtype_test\n \n \n-@pytest.mark.parametrize(\"array_type\", [\"array\", \"sparse\"])\n-@pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n-def test_simple_imputer_constant_keep_empty_features(array_type, keep_empty_features):\n-    \"\"\"Check the behaviour of `keep_empty_features` with `strategy='constant'.\n-    For backward compatibility, a column full of missing values will always be\n-    fill and never dropped.\n-    \"\"\"\n-    X = np.array([[np.nan, 2], [np.nan, 3], [np.nan, 6]])\n-    X = _convert_container(X, array_type)\n-    fill_value = 10\n-    imputer = SimpleImputer(\n-        strategy=\"constant\",\n-        fill_value=fill_value,\n-        keep_empty_features=keep_empty_features,\n-    )\n-\n-    for method in [\"fit_transform\", \"transform\"]:\n-        # TODO(1.8): Remove the condition and still call getattr(imputer, method)(X)\n-        if method.startswith(\"fit\") and not keep_empty_features:\n-            warn_msg = '`strategy=\"constant\"`, empty features are not dropped. '\n-            with pytest.warns(FutureWarning, match=warn_msg):\n-                X_imputed = getattr(imputer, method)(X)\n-        else:\n-            X_imputed = getattr(imputer, method)(X)\n-        assert X_imputed.shape == X.shape\n-        constant_feature = (\n-            X_imputed[:, 0].toarray() if array_type == \"sparse\" else X_imputed[:, 0]\n-        )\n-        assert_array_equal(constant_feature, fill_value)\n-\n-\n @pytest.mark.parametrize(\"array_type\", [\"array\", \"sparse\"])\n @pytest.mark.parametrize(\"strategy\", [\"mean\", \"median\", \"most_frequent\"])\n @pytest.mark.parametrize(\"keep_empty_features\", [True, False])\n@@ -1870,8 +1852,7 @@ def test_simple_imputer_constant_fill_value_casting():\n     X_float64 = np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float64)\n     imputer.fit(X_float64)\n     err_msg = (\n-        f\"The dtype of the filling value (i.e. {imputer.statistics_.dtype!r}) \"\n-        \"cannot be cast\"\n+        f\"The dtype of the filling value (i.e. {imputer._fill_dtype!r}) cannot be cast\"\n     )\n     with pytest.raises(ValueError, match=re.escape(err_msg)):\n         imputer.transform(X_int64)\n",
  "fail_to_pass": [
    "test_imputation_constant_integer",
    "test_imputation_constant_float",
    "test_imputation_constant_object",
    "test_imputation_constant_pandas",
    "test_iterative_imputer_constant_fill_value"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/impute/_base.py",
    "sklearn/impute/_iterative.py",
    "sklearn/impute/tests/test_impute.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-09-24T14:52:57Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32266",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}