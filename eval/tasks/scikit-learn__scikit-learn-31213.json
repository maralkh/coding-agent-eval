{
  "id": "scikit-learn__scikit-learn-31213",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "32aa82d25725d4bc0dfd7707e5c0f8d1387a1ea6",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 31213,
  "pr_title": "TST use global_random_seed in `sklearn/decomposition/tests/test_sparse_pca.py`",
  "gold_patch": "diff --git a/sklearn/decomposition/tests/test_sparse_pca.py b/sklearn/decomposition/tests/test_sparse_pca.py\nindex 4edf7df86a3e2..f8c71a5d0e752 100644\n--- a/sklearn/decomposition/tests/test_sparse_pca.py\n+++ b/sklearn/decomposition/tests/test_sparse_pca.py\n@@ -1,12 +1,12 @@\n # Authors: The scikit-learn developers\n # SPDX-License-Identifier: BSD-3-Clause\n \n-import sys\n \n import numpy as np\n import pytest\n from numpy.testing import assert_array_equal\n \n+from sklearn.datasets import make_low_rank_matrix\n from sklearn.decomposition import PCA, MiniBatchSparsePCA, SparsePCA\n from sklearn.utils import check_random_state\n from sklearn.utils._testing import (\n@@ -57,48 +57,58 @@ def test_correct_shapes():\n     assert U.shape == (12, 13)\n \n \n-def test_fit_transform():\n+def test_fit_transform(global_random_seed):\n     alpha = 1\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n-    spca_lars = SparsePCA(n_components=3, method=\"lars\", alpha=alpha, random_state=0)\n+    spca_lars = SparsePCA(\n+        n_components=3, method=\"lars\", alpha=alpha, random_state=global_random_seed\n+    )\n     spca_lars.fit(Y)\n \n     # Test that CD gives similar results\n-    spca_lasso = SparsePCA(n_components=3, method=\"cd\", random_state=0, alpha=alpha)\n+    spca_lasso = SparsePCA(\n+        n_components=3, method=\"cd\", random_state=global_random_seed, alpha=alpha\n+    )\n     spca_lasso.fit(Y)\n     assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)\n \n \n @if_safe_multiprocessing_with_blas\n-def test_fit_transform_parallel():\n+def test_fit_transform_parallel(global_random_seed):\n     alpha = 1\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n-    spca_lars = SparsePCA(n_components=3, method=\"lars\", alpha=alpha, random_state=0)\n+    spca_lars = SparsePCA(\n+        n_components=3, method=\"lars\", alpha=alpha, random_state=global_random_seed\n+    )\n     spca_lars.fit(Y)\n     U1 = spca_lars.transform(Y)\n     # Test multiple CPUs\n     spca = SparsePCA(\n-        n_components=3, n_jobs=2, method=\"lars\", alpha=alpha, random_state=0\n+        n_components=3,\n+        n_jobs=2,\n+        method=\"lars\",\n+        alpha=alpha,\n+        random_state=global_random_seed,\n     ).fit(Y)\n     U2 = spca.transform(Y)\n     assert not np.all(spca_lars.components_ == 0)\n     assert_array_almost_equal(U1, U2)\n \n \n-def test_transform_nan():\n+def test_transform_nan(global_random_seed):\n     # Test that SparsePCA won't return NaN when there is 0 feature in all\n     # samples.\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n     Y[:, 0] = 0\n-    estimator = SparsePCA(n_components=8)\n+    estimator = SparsePCA(n_components=8, random_state=global_random_seed)\n     assert not np.any(np.isnan(estimator.fit_transform(Y)))\n \n \n-def test_fit_transform_tall():\n-    rng = np.random.RandomState(0)\n+def test_fit_transform_tall(global_random_seed):\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 65, (8, 8), random_state=rng)  # tall array\n     spca_lars = SparsePCA(n_components=3, method=\"lars\", random_state=rng)\n     U1 = spca_lars.fit_transform(Y)\n@@ -107,8 +117,8 @@ def test_fit_transform_tall():\n     assert_array_almost_equal(U1, U2)\n \n \n-def test_initialization():\n-    rng = np.random.RandomState(0)\n+def test_initialization(global_random_seed):\n+    rng = np.random.RandomState(global_random_seed)\n     U_init = rng.randn(5, 3)\n     V_init = rng.randn(3, 4)\n     model = SparsePCA(\n@@ -135,42 +145,9 @@ def test_mini_batch_correct_shapes():\n     assert U.shape == (12, 13)\n \n \n-# XXX: test always skipped\n-@pytest.mark.skipif(True, reason=\"skipping mini_batch_fit_transform.\")\n-def test_mini_batch_fit_transform():\n-    alpha = 1\n-    rng = np.random.RandomState(0)\n-    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n-    spca_lars = MiniBatchSparsePCA(n_components=3, random_state=0, alpha=alpha).fit(Y)\n-    U1 = spca_lars.transform(Y)\n-    # Test multiple CPUs\n-    if sys.platform == \"win32\":  # fake parallelism for win32\n-        import joblib\n-\n-        _mp = joblib.parallel.multiprocessing\n-        joblib.parallel.multiprocessing = None\n-        try:\n-            spca = MiniBatchSparsePCA(\n-                n_components=3, n_jobs=2, alpha=alpha, random_state=0\n-            )\n-            U2 = spca.fit(Y).transform(Y)\n-        finally:\n-            joblib.parallel.multiprocessing = _mp\n-    else:  # we can efficiently use parallelism\n-        spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha, random_state=0)\n-        U2 = spca.fit(Y).transform(Y)\n-    assert not np.all(spca_lars.components_ == 0)\n-    assert_array_almost_equal(U1, U2)\n-    # Test that CD gives similar results\n-    spca_lasso = MiniBatchSparsePCA(\n-        n_components=3, method=\"cd\", alpha=alpha, random_state=0\n-    ).fit(Y)\n-    assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)\n-\n-\n-def test_scaling_fit_transform():\n+def test_scaling_fit_transform(global_random_seed):\n     alpha = 1\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)\n     spca_lars = SparsePCA(n_components=3, method=\"lars\", alpha=alpha, random_state=rng)\n     results_train = spca_lars.fit_transform(Y)\n@@ -178,22 +155,22 @@ def test_scaling_fit_transform():\n     assert_allclose(results_train[0], results_test[0])\n \n \n-def test_pca_vs_spca():\n-    rng = np.random.RandomState(0)\n+def test_pca_vs_spca(global_random_seed):\n+    rng = np.random.RandomState(global_random_seed)\n     Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)\n     Z, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n-    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2)\n-    pca = PCA(n_components=2)\n+    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2, random_state=rng)\n+    pca = PCA(n_components=2, random_state=rng)\n     pca.fit(Y)\n     spca.fit(Y)\n     results_test_pca = pca.transform(Z)\n     results_test_spca = spca.transform(Z)\n     assert_allclose(\n-        np.abs(spca.components_.dot(pca.components_.T)), np.eye(2), atol=1e-5\n+        np.abs(spca.components_.dot(pca.components_.T)), np.eye(2), atol=1e-4\n     )\n     results_test_pca *= np.sign(results_test_pca[0, :])\n     results_test_spca *= np.sign(results_test_spca[0, :])\n-    assert_allclose(results_test_pca, results_test_spca)\n+    assert_allclose(results_test_pca, results_test_spca, atol=1e-4)\n \n \n @pytest.mark.parametrize(\"SPCA\", [SparsePCA, MiniBatchSparsePCA])\n@@ -236,26 +213,31 @@ def test_sparse_pca_dtype_match(SPCA, method, data_type, expected_type):\n \n @pytest.mark.parametrize(\"SPCA\", (SparsePCA, MiniBatchSparsePCA))\n @pytest.mark.parametrize(\"method\", (\"lars\", \"cd\"))\n-def test_sparse_pca_numerical_consistency(SPCA, method):\n+def test_sparse_pca_numerical_consistency(SPCA, method, global_random_seed):\n     # Verify numericall consistentency among np.float32 and np.float64\n-    rtol = 1e-3\n-    alpha = 2\n-    n_samples, n_features, n_components = 12, 10, 3\n-    rng = np.random.RandomState(0)\n-    input_array = rng.randn(n_samples, n_features)\n+    n_samples, n_features, n_components = 20, 20, 5\n+    input_array = make_low_rank_matrix(\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        effective_rank=n_components,\n+        random_state=global_random_seed,\n+    )\n \n     model_32 = SPCA(\n-        n_components=n_components, alpha=alpha, method=method, random_state=0\n+        n_components=n_components,\n+        method=method,\n+        random_state=global_random_seed,\n     )\n     transformed_32 = model_32.fit_transform(input_array.astype(np.float32))\n \n     model_64 = SPCA(\n-        n_components=n_components, alpha=alpha, method=method, random_state=0\n+        n_components=n_components,\n+        method=method,\n+        random_state=global_random_seed,\n     )\n     transformed_64 = model_64.fit_transform(input_array.astype(np.float64))\n-\n-    assert_allclose(transformed_64, transformed_32, rtol=rtol)\n-    assert_allclose(model_64.components_, model_32.components_, rtol=rtol)\n+    assert_allclose(transformed_64, transformed_32)\n+    assert_allclose(model_64.components_, model_32.components_)\n \n \n @pytest.mark.parametrize(\"SPCA\", [SparsePCA, MiniBatchSparsePCA])\n@@ -324,17 +306,20 @@ def test_equivalence_components_pca_spca(global_random_seed):\n     assert_allclose(pca.components_, spca.components_)\n \n \n-def test_sparse_pca_inverse_transform():\n+def test_sparse_pca_inverse_transform(global_random_seed):\n     \"\"\"Check that `inverse_transform` in `SparsePCA` and `PCA` are similar.\"\"\"\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     n_samples, n_features = 10, 5\n     X = rng.randn(n_samples, n_features)\n \n     n_components = 2\n     spca = SparsePCA(\n-        n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0\n+        n_components=n_components,\n+        alpha=1e-12,\n+        ridge_alpha=1e-12,\n+        random_state=global_random_seed,\n     )\n-    pca = PCA(n_components=n_components, random_state=0)\n+    pca = PCA(n_components=n_components, random_state=global_random_seed)\n     X_trans_spca = spca.fit_transform(X)\n     X_trans_pca = pca.fit_transform(X)\n     assert_allclose(\n@@ -343,17 +328,20 @@ def test_sparse_pca_inverse_transform():\n \n \n @pytest.mark.parametrize(\"SPCA\", [SparsePCA, MiniBatchSparsePCA])\n-def test_transform_inverse_transform_round_trip(SPCA):\n+def test_transform_inverse_transform_round_trip(SPCA, global_random_seed):\n     \"\"\"Check the `transform` and `inverse_transform` round trip with no loss of\n     information.\n     \"\"\"\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     n_samples, n_features = 10, 5\n     X = rng.randn(n_samples, n_features)\n \n     n_components = n_features\n     spca = SPCA(\n-        n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0\n+        n_components=n_components,\n+        alpha=1e-12,\n+        ridge_alpha=1e-12,\n+        random_state=global_random_seed,\n     )\n     X_trans_spca = spca.fit_transform(X)\n     assert_allclose(spca.inverse_transform(X_trans_spca), X)\n",
  "fail_to_pass": [
    "test_fit_transform",
    "test_fit_transform_parallel",
    "test_transform_nan",
    "test_fit_transform_tall",
    "test_initialization",
    "test_scaling_fit_transform",
    "test_pca_vs_spca",
    "test_sparse_pca_numerical_consistency",
    "test_sparse_pca_inverse_transform",
    "test_transform_inverse_transform_round_trip"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/decomposition/tests/test_sparse_pca.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-04-16T10:07:07Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31213",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}