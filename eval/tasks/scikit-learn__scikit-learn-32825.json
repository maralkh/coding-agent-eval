{
  "id": "scikit-learn__scikit-learn-32825",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "3300836d26c8e49ce0c2092878c53ad105aa8b53",
  "issue_number": 32805,
  "issue_title": "RFC: `Bagging` estimators: avoid changing `max_samples` default behavior in 1.8",
  "issue_body": "As stated in the change log of PR #31414\n\n> `max_samples` is now interpreted as a fraction of `sample_weight.sum()` instead of `X.shape[0]` when passed as a float.\n\nBecause `max_samples` default to `1.0`, this is a change of the default behavior. I expressed concerns about this change in this thread https://github.com/scikit-learn/scikit-learn/pull/31529#discussion_r2541938811 for a different PR but the same kind of change (on random forests).\n\nIt seems @antoinebaker, the author of both PRs, agrees with me that having `max_samples=None` as default (and `None` is then re-interpreted as `X.shape[0]`) sounds safer/less surprising for users. (I'll let you confirm or not @antoinebaker ^^).\n\nSo **I propose to not merge the PR/commit #31414 in v1.8** (is this even possible? \ud83d\ude05 @lesteve)\n\nOr, merge a PR to change the default before v1.8? (not sure this is possible either).\n\nWhy I think this is \"urgent\"? Because we don't want to change default twice in a row (those are backward incompatible changes: same code produces different results).\n\nWhat are your thoughts? @antoinebaker @ogrisel \n\n\n",
  "pr_number": 32825,
  "pr_title": "Add new default max_samples=None in Bagging estimators",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst b/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst\ndeleted file mode 100644\nindex 17c2f765d4b7c..0000000000000\n--- a/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-- :class:`ensemble.BaggingClassifier`, :class:`ensemble.BaggingRegressor`\n-  and :class:`ensemble.IsolationForest` now use `sample_weight` to draw\n-  the samples instead of forwarding them multiplied by a uniformly sampled\n-  mask to the underlying estimators. Furthermore, `max_samples` is now\n-  interpreted as a fraction of `sample_weight.sum()` instead of `X.shape[0]`\n-  when passed as a float.\n-  By :user:`Antoine Baker <antoinebaker>`.\ndiff --git a/doc/whats_new/upcoming_changes/sklearn.ensemble/32825.fix.rst b/doc/whats_new/upcoming_changes/sklearn.ensemble/32825.fix.rst\nnew file mode 100644\nindex 0000000000000..604ec9421a424\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.ensemble/32825.fix.rst\n@@ -0,0 +1,8 @@\n+- :class:`ensemble.BaggingClassifier`, :class:`ensemble.BaggingRegressor` and\n+  :class:`ensemble.IsolationForest` now use `sample_weight` to draw the samples\n+  instead of forwarding them multiplied by a uniformly sampled mask to the\n+  underlying estimators. Furthermore, when `max_samples` is a float, it is now\n+  interpreted as a fraction of `sample_weight.sum()` instead of `X.shape[0]`.\n+  The new default `max_samples=None` draws `X.shape[0]` samples, irrespective\n+  of `sample_weight`.\n+  By :user:`Antoine Baker <antoinebaker>`. :pr:`31414` and\ndiff --git a/sklearn/ensemble/_bagging.py b/sklearn/ensemble/_bagging.py\nindex 067bdb9e7db0e..a3d0b2bc931c7 100644\n--- a/sklearn/ensemble/_bagging.py\n+++ b/sklearn/ensemble/_bagging.py\n@@ -46,6 +46,63 @@\n MAX_INT = np.iinfo(np.int32).max\n \n \n+def _get_n_samples_bootstrap(n_samples, max_samples, sample_weight):\n+    \"\"\"\n+    Get the number of samples in a bootstrap sample.\n+\n+    Parameters\n+    ----------\n+    n_samples : int\n+        Number of samples in the dataset.\n+\n+    max_samples : None, int or float\n+        The maximum number of samples to draw.\n+\n+        - If None, then draw `n_samples` samples.\n+        - If int, then draw `max_samples` samples.\n+        - If float, then draw `max_samples * n_samples` unweighted samples or\n+          `max_samples * sample_weight.sum()` weighted samples.\n+\n+    sample_weight : array of shape (n_samples,) or None\n+        Sample weights with frequency semantics when `max_samples` is explicitly\n+        set to a float or integer value. When keeping the `max_samples=None` default\n+        value, the equivalence between fitting with integer weighted data points or\n+        integer repeated data points is no longer guaranteed because the effective\n+        bootstrap size is no longer guaranteed to be equivalent.\n+\n+    Returns\n+    -------\n+    n_samples_bootstrap : int\n+        The total number of samples to draw for the bootstrap sample.\n+    \"\"\"\n+    if max_samples is None:\n+        return n_samples\n+    elif isinstance(max_samples, Integral):\n+        return max_samples\n+\n+    if sample_weight is None:\n+        weighted_n_samples = n_samples\n+        weighted_n_samples_msg = f\"the number of samples is {weighted_n_samples} \"\n+    else:\n+        weighted_n_samples = sample_weight.sum()\n+        weighted_n_samples_msg = (\n+            f\"the total sum of sample weights is {weighted_n_samples} \"\n+        )\n+\n+    # max_samples Real fractional value relative to weighted_n_samples\n+    n_samples_bootstrap = max(int(max_samples * weighted_n_samples), 1)\n+    # Warn when number of bootstrap samples is suspiciously small\n+    # This heuristic for \"suspiciously small\" might be adapted if found\n+    # unsuitable in practice\n+    if n_samples_bootstrap < max(10, n_samples ** (1 / 3)):\n+        warn(\n+            f\"Using the fractional value {max_samples=} when {weighted_n_samples_msg}\"\n+            f\"results in a low number ({n_samples_bootstrap}) of bootstrap samples. \"\n+            \"We recommend passing `max_samples` as an integer instead.\"\n+        )\n+    return n_samples_bootstrap\n+\n+\n def _generate_indices(random_state, bootstrap, n_population, n_samples):\n     \"\"\"Draw randomly sampled indices.\"\"\"\n     # Draw sample indices\n@@ -273,6 +330,7 @@ class BaseBagging(BaseEnsemble, metaclass=ABCMeta):\n         \"estimator\": [HasMethods([\"fit\", \"predict\"]), None],\n         \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n         \"max_samples\": [\n+            None,\n             Interval(Integral, 1, None, closed=\"left\"),\n             Interval(RealNotInt, 0, 1, closed=\"right\"),\n         ],\n@@ -295,7 +353,7 @@ def __init__(\n         estimator=None,\n         n_estimators=10,\n         *,\n-        max_samples=1.0,\n+        max_samples=None,\n         max_features=1.0,\n         bootstrap=True,\n         bootstrap_features=False,\n@@ -340,7 +398,9 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n             Sample weights. If None, then samples are equally weighted. Used as\n             probabilities to sample the training set. Note that the expected\n             frequency semantics for the `sample_weight` parameter are only\n-            fulfilled when sampling with replacement `bootstrap=True`.\n+            fulfilled when sampling with replacement `bootstrap=True` and using\n+            a float or integer `max_samples` (instead of the default\n+            `max_samples=None`).\n \n         **fit_params : dict\n             Parameters to pass to the underlying estimators.\n@@ -462,20 +522,7 @@ def _fit(\n         if max_samples is None:\n             max_samples = self.max_samples\n \n-        if not isinstance(max_samples, numbers.Integral):\n-            if sample_weight is None:\n-                max_samples = max(int(max_samples * X.shape[0]), 1)\n-            else:\n-                sw_sum = np.sum(sample_weight)\n-                if sw_sum <= 1:\n-                    raise ValueError(\n-                        f\"The total sum of sample weights is {sw_sum}, which prevents \"\n-                        \"resampling with a fractional value for max_samples=\"\n-                        f\"{max_samples}. Either pass max_samples as an integer or \"\n-                        \"use a larger sample_weight.\"\n-                    )\n-                max_samples = max(int(max_samples * sw_sum), 1)\n-\n+        max_samples = _get_n_samples_bootstrap(X.shape[0], max_samples, sample_weight)\n         if not self.bootstrap and max_samples > X.shape[0]:\n             raise ValueError(\n                 f\"Effective max_samples={max_samples} must be <= n_samples=\"\n@@ -728,13 +775,14 @@ class BaggingClassifier(ClassifierMixin, BaseBagging):\n     n_estimators : int, default=10\n         The number of base estimators in the ensemble.\n \n-    max_samples : int or float, default=1.0\n+    max_samples : int or float, default=None\n         The number of samples to draw from X to train each base estimator (with\n         replacement by default, see `bootstrap` for more details).\n \n+        - If None, then draw `X.shape[0]` samples irrespective of `sample_weight`.\n         - If int, then draw `max_samples` samples.\n-        - If float, then draw `max_samples * X.shape[0]` unweighted samples\n-          or `max_samples * sample_weight.sum()` weighted samples.\n+        - If float, then draw `max_samples * X.shape[0]` unweighted samples or\n+          `max_samples * sample_weight.sum()` weighted samples.\n \n     max_features : int or float, default=1.0\n         The number of features to draw from X to train each base estimator (\n@@ -867,7 +915,7 @@ def __init__(\n         estimator=None,\n         n_estimators=10,\n         *,\n-        max_samples=1.0,\n+        max_samples=None,\n         max_features=1.0,\n         bootstrap=True,\n         bootstrap_features=False,\n@@ -1239,12 +1287,14 @@ class BaggingRegressor(RegressorMixin, BaseBagging):\n     n_estimators : int, default=10\n         The number of base estimators in the ensemble.\n \n-    max_samples : int or float, default=1.0\n+    max_samples : int or float, default=None\n         The number of samples to draw from X to train each base estimator (with\n         replacement by default, see `bootstrap` for more details).\n \n+        - If None, then draw `X.shape[0]` samples irrespective of `sample_weight`.\n         - If int, then draw `max_samples` samples.\n-        - If float, then draw `max_samples * X.shape[0]` samples.\n+        - If float, then draw `max_samples * X.shape[0]` unweighted samples or\n+          `max_samples * sample_weight.sum()` weighted samples.\n \n     max_features : int or float, default=1.0\n         The number of features to draw from X to train each base estimator (\n@@ -1368,7 +1418,7 @@ def __init__(\n         estimator=None,\n         n_estimators=10,\n         *,\n-        max_samples=1.0,\n+        max_samples=None,\n         max_features=1.0,\n         bootstrap=True,\n         bootstrap_features=False,\ndiff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py\nindex 611ea271b3f91..0b73499467da6 100644\n--- a/sklearn/ensemble/tests/test_bagging.py\n+++ b/sklearn/ensemble/tests/test_bagging.py\n@@ -6,6 +6,7 @@\n # SPDX-License-Identifier: BSD-3-Clause\n \n import re\n+import warnings\n from itertools import cycle, product\n \n import joblib\n@@ -26,6 +27,7 @@\n     RandomForestClassifier,\n     RandomForestRegressor,\n )\n+from sklearn.ensemble._bagging import _get_n_samples_bootstrap\n from sklearn.feature_selection import SelectKBest\n from sklearn.linear_model import LogisticRegression, Perceptron\n from sklearn.model_selection import GridSearchCV, ParameterGrid, train_test_split\n@@ -706,16 +708,17 @@ def test_warning_bootstrap_sample_weight():\n def test_invalid_sample_weight_max_samples_bootstrap_combinations():\n     X, y = iris.data, iris.target\n \n-    # Case 1: small weights and fractional max_samples would lead to sampling\n-    # less than 1 sample, which is not allowed.\n+    # Case 1: small weights and fractional max_samples lead to a small\n+    # number of bootstrap samples, which raises a UserWarning.\n     clf = BaggingClassifier(max_samples=1.0)\n     sample_weight = np.ones_like(y) / (2 * len(y))\n     expected_msg = (\n-        r\"The total sum of sample weights is 0.5(\\d*), which prevents resampling with \"\n-        r\"a fractional value for max_samples=1\\.0\\. Either pass max_samples as an \"\n-        r\"integer or use a larger sample_weight\\.\"\n+        \"Using the fractional value max_samples=1.0 when \"\n+        r\"the total sum of sample weights is 0.5(\\d*) \"\n+        r\"results in a low number \\(1\\) of bootstrap samples. \"\n+        \"We recommend passing `max_samples` as an integer.\"\n     )\n-    with pytest.raises(ValueError, match=expected_msg):\n+    with pytest.warns(UserWarning, match=expected_msg):\n         clf.fit(X, y, sample_weight=sample_weight)\n \n     # Case 2: large weights and bootstrap=False would lead to sampling without\n@@ -813,6 +816,55 @@ def test_draw_indices_using_sample_weight(\n                 assert_allclose(estimator.y_, y[samples])\n \n \n+def test_get_n_samples_bootstrap():\n+    n_samples, max_samples, sample_weight = 10, None, \"not_used\"\n+    assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == n_samples\n+\n+    n_samples, max_samples, sample_weight = 10, 5, \"not_used\"\n+    assert (\n+        _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == max_samples\n+    )\n+\n+    n_samples, max_samples, sample_weight = 10, 1e-5, None\n+    assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == 1\n+\n+    n_samples, max_samples, sample_weight = 10, 0.66, None\n+    warning_msg = \".+the number of samples.+low number.+max_samples.+as an integer\"\n+    with pytest.warns(UserWarning, match=warning_msg):\n+        assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == int(\n+            max_samples * n_samples\n+        )\n+\n+    n_samples, max_samples, sample_weight = 10, 1e-5, None\n+    with pytest.warns(UserWarning, match=warning_msg):\n+        assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == 1\n+\n+    warning_msg_with_weights = (\n+        \".+the total sum of sample weights.+low number.+max_samples.+as an integer\"\n+    )\n+    rng = np.random.default_rng(0)\n+    n_samples, max_samples, sample_weight = 1_000_000, 1e-5, rng.uniform(size=1_000_000)\n+    with pytest.warns(UserWarning, match=warning_msg_with_weights):\n+        assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == int(\n+            max_samples * sample_weight.sum()\n+        )\n+\n+    sample_weight = np.ones(3)\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"error\")\n+\n+        n_samples, max_samples, sample_weight = 100, 30, None\n+        assert (\n+            _get_n_samples_bootstrap(n_samples, max_samples, sample_weight)\n+            == max_samples\n+        )\n+\n+        n_samples, max_samples, sample_weight = 100, 0.5, rng.uniform(size=100)\n+        assert _get_n_samples_bootstrap(n_samples, max_samples, sample_weight) == int(\n+            max_samples * sample_weight.sum()\n+        )\n+\n+\n def test_oob_score_removed_on_warm_start():\n     X, y = make_hastie_10_2(n_samples=100, random_state=1)\n \n",
  "fail_to_pass": [
    "test_get_n_samples_bootstrap"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/ensemble/_bagging.py",
    "sklearn/ensemble/tests/test_bagging.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-12-01T16:15:28Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32825",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/32805"
}