{
  "id": "scikit-learn__scikit-learn-31775",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "f72958d81b897ec1c9d5ddd62a99c00850832200",
  "issue_number": 30945,
  "issue_title": "Refactor weighted percentile functions to avoid redundant sorting",
  "issue_body": "# REF: Integrate symmetrization in _weighted_percentile to avoid double sorting\r\n\r\n## Description\r\n\r\nThis pull request refactors the computation of weighted percentiles by integrating symmetrization directly into the `_weighted_percentile` function. With this change, we avoid sorting the input array twice when computing the averaged weighted percentile. The following changes have been made:\r\n\r\n- Added a `symmetrize` parameter to `_weighted_percentile` that, when enabled, computes the averaged weighted percentile using both positive and negative arrays.\r\n- Updated `_averaged_weighted_percentile` to leverage the new symmetrization functionality.\r\n- Preserved the original functionality and all existing comments.\r\n- Ensured that the code complies with the scikit-learn contributing guidelines and passes all relevant tests.\r\n\r\nThis refactor improves efficiency without altering the external API or behavior.\r\n\r\nPlease review and let me know if any adjustments are required.\r\n",
  "pr_number": 31775,
  "pr_title": "MNT Refactor `_average_weighted_percentile` to avoid double sort",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/many-modules/31775.efficiency.rst b/doc/whats_new/upcoming_changes/many-modules/31775.efficiency.rst\nnew file mode 100644\nindex 0000000000000..5aa067aeeb7cf\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/many-modules/31775.efficiency.rst\n@@ -0,0 +1,4 @@\n+- Improved CPU and memory usage in estimators and metric functions that rely on\n+  weighted percentiles and better match NumPy and Scipy (un-weighted) implementations\n+  of percentiles.\n+  By :user:`Lucy Liu <lucyleeow>`\ndiff --git a/sklearn/metrics/_regression.py b/sklearn/metrics/_regression.py\nindex 361405e188c9d..6d5b3599bff71 100644\n--- a/sklearn/metrics/_regression.py\n+++ b/sklearn/metrics/_regression.py\n@@ -26,7 +26,7 @@\n )\n from sklearn.utils._array_api import _xlogy as xlogy\n from sklearn.utils._param_validation import Interval, StrOptions, validate_params\n-from sklearn.utils.stats import _averaged_weighted_percentile, _weighted_percentile\n+from sklearn.utils.stats import _weighted_percentile\n from sklearn.utils.validation import (\n     _check_sample_weight,\n     _num_samples,\n@@ -921,8 +921,8 @@ def median_absolute_error(\n     if sample_weight is None:\n         output_errors = _median(xp.abs(y_pred - y_true), axis=0)\n     else:\n-        output_errors = _averaged_weighted_percentile(\n-            xp.abs(y_pred - y_true), sample_weight=sample_weight\n+        output_errors = _weighted_percentile(\n+            xp.abs(y_pred - y_true), sample_weight=sample_weight, average=True\n         )\n     if isinstance(multioutput, str):\n         if multioutput == \"raw_values\":\ndiff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py\nindex 0cfe554c94ada..5ab6fdd4b6576 100644\n--- a/sklearn/preprocessing/_discretization.py\n+++ b/sklearn/preprocessing/_discretization.py\n@@ -11,7 +11,7 @@\n from sklearn.preprocessing._encoders import OneHotEncoder\n from sklearn.utils import resample\n from sklearn.utils._param_validation import Interval, Options, StrOptions\n-from sklearn.utils.stats import _averaged_weighted_percentile, _weighted_percentile\n+from sklearn.utils.stats import _weighted_percentile\n from sklearn.utils.validation import (\n     _check_feature_names_in,\n     _check_sample_weight,\n@@ -365,17 +365,20 @@ def fit(self, X, y=None, sample_weight=None):\n                         dtype=np.float64,\n                     )\n                 else:\n-                    # TODO: make _weighted_percentile and\n-                    # _averaged_weighted_percentile accept an array of\n+                    # TODO: make _weighted_percentile accept an array of\n                     # quantiles instead of calling it multiple times and\n                     # sorting the column multiple times as a result.\n-                    percentile_func = {\n-                        \"inverted_cdf\": _weighted_percentile,\n-                        \"averaged_inverted_cdf\": _averaged_weighted_percentile,\n-                    }[quantile_method]\n+                    average = (\n+                        True if quantile_method == \"averaged_inverted_cdf\" else False\n+                    )\n                     bin_edges[jj] = np.asarray(\n                         [\n-                            percentile_func(column, sample_weight, percentile_rank=p)\n+                            _weighted_percentile(\n+                                column,\n+                                sample_weight,\n+                                percentile_rank=p,\n+                                average=average,\n+                            )\n                             for p in percentile_levels\n                         ],\n                         dtype=np.float64,\ndiff --git a/sklearn/utils/stats.py b/sklearn/utils/stats.py\nindex c0a83bb820673..35cadf0ca7372 100644\n--- a/sklearn/utils/stats.py\n+++ b/sklearn/utils/stats.py\n@@ -7,11 +7,35 @@\n )\n \n \n-def _weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n-    \"\"\"Compute the weighted percentile with method 'inverted_cdf'.\n-\n-    When the percentile lies between two data points of `array`, the function returns\n-    the lower value.\n+def _weighted_percentile(\n+    array, sample_weight, percentile_rank=50, average=False, xp=None\n+):\n+    \"\"\"Compute the weighted percentile.\n+\n+    Implement an array API compatible (weighted version) of NumPy's 'inverted_cdf'\n+    method when `average=False` (default) and 'averaged_inverted_cdf' when\n+    `average=True`.\n+\n+    For an array ordered by increasing values, when the percentile lies exactly on a\n+    data point:\n+\n+    * 'inverted_cdf' takes the exact data point.\n+    * 'averaged_inverted_cdf' takes the average of the exact data point and the one\n+      above it (this means it gives the same result as `median` for unit weights).\n+\n+    E.g., for the array [1, 2, 3, 4] the percentile rank at each data point would\n+    be [25, 50, 75, 100]. Percentile rank 50 lies on '2'. 'average_inverted_cdf'\n+    computes the average of '2' and '3', making it 'symmetrical' because if you\n+    reverse the array, rank 50 would fall on '3'. It also matches 'median'.\n+    On the other hand, 'inverted_cdf', which does not satisfy the symmetry property,\n+    would give '2'.\n+\n+    When the requested percentile lies between two data points, both methods return\n+    the higher data point.\n+    E.g., for the array [1, 2, 3, 4, 5] the percentile rank at each data point would\n+    be [20, 40, 60, 80, 100]. Percentile rank 50, lies between '2' and '3'. Taking the\n+    higher data point is symmetrical because if you reverse the array, 50 would lie\n+    between '4' and '3'. Both methods match median in this case.\n \n     If `array` is a 2D array, the `values` are selected along axis 0.\n \n@@ -25,6 +49,10 @@ def _weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n         .. versionchanged:: 1.7\n             Supports handling of `NaN` values.\n \n+        .. versionchanged:: 1.8\n+            Supports `average`, which calculates percentile using the\n+            \"averaged_inverted_cdf\" method.\n+\n     Parameters\n     ----------\n     array : 1D or 2D array\n@@ -38,6 +66,14 @@ def _weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n         The probability level of the percentile to compute, in percent. Must be between\n         0 and 100.\n \n+    average : bool, default=False\n+        If `True`, uses the \"averaged_inverted_cdf\" quantile method, otherwise\n+        defaults to \"inverted_cdf\". \"averaged_inverted_cdf\" is symmetrical with\n+        unit `sample_weight`, such that the total of `sample_weight` below or equal to\n+        `_weighted_percentile(percentile_rank)` is the same as the total of\n+        `sample_weight` above or equal to `_weighted_percentile(100-percentile_rank)`.\n+        This symmetry is not guaranteed with non-unit weights.\n+\n     xp : array_namespace, default=None\n         The standard-compatible namespace for `array`. Default: infer.\n \n@@ -93,6 +129,8 @@ def _weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n     # For each feature with index j, find sample index i of the scalar value\n     # `adjusted_percentile_rank[j]` in 1D array `weight_cdf[j]`, such that:\n     # weight_cdf[j, i-1] < adjusted_percentile_rank[j] <= weight_cdf[j, i].\n+    # Note `searchsorted` defaults to equality on the right, whereas Hyndman and Fan\n+    # reference equation has equality on the left.\n     percentile_indices = xp.stack(\n         [\n             xp.searchsorted(\n@@ -101,22 +139,52 @@ def _weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n             for feature_idx in range(weight_cdf.shape[0])\n         ],\n     )\n-    # In rare cases, `percentile_indices` equals to `sorted_idx.shape[0]`\n+    # `percentile_indices` may be equal to `sorted_idx.shape[0]` due to floating\n+    # point error (see #11813)\n     max_idx = sorted_idx.shape[0] - 1\n     percentile_indices = xp.clip(percentile_indices, 0, max_idx)\n \n     col_indices = xp.arange(array.shape[1], device=device)\n     percentile_in_sorted = sorted_idx[percentile_indices, col_indices]\n \n-    result = array[percentile_in_sorted, col_indices]\n+    if average:\n+        # From Hyndman and Fan (1996), `fraction_above` is `g`\n+        fraction_above = (\n+            weight_cdf[col_indices, percentile_indices] - adjusted_percentile_rank\n+        )\n+        is_fraction_above = fraction_above > xp.finfo(floating_dtype).eps\n+        percentile_plus_one_indices = xp.clip(percentile_indices + 1, 0, max_idx)\n+        percentile_plus_one_in_sorted = sorted_idx[\n+            percentile_plus_one_indices, col_indices\n+        ]\n+        # Handle case when next index ('plus one') has sample weight of 0\n+        zero_weight_cols = col_indices[\n+            sample_weight[percentile_plus_one_in_sorted, col_indices] == 0\n+        ]\n+        for col_idx in zero_weight_cols:\n+            cdf_val = weight_cdf[col_idx, percentile_indices[col_idx]]\n+            # Search for next index where `weighted_cdf` is greater\n+            next_index = xp.searchsorted(\n+                weight_cdf[col_idx, ...], cdf_val, side=\"right\"\n+            )\n+            # Handle case where there are trailing 0 sample weight samples\n+            # and `percentile_indices` is already max index\n+            if next_index >= max_idx:\n+                # use original `percentile_indices` again\n+                next_index = percentile_indices[col_idx]\n+\n+            percentile_plus_one_in_sorted[col_idx] = sorted_idx[next_index, col_idx]\n+\n+        result = xp.where(\n+            is_fraction_above,\n+            array[percentile_in_sorted, col_indices],\n+            (\n+                array[percentile_in_sorted, col_indices]\n+                + array[percentile_plus_one_in_sorted, col_indices]\n+            )\n+            / 2,\n+        )\n+    else:\n+        result = array[percentile_in_sorted, col_indices]\n \n     return result[0] if n_dim == 1 else result\n-\n-\n-# TODO: refactor to do the symmetrisation inside _weighted_percentile to avoid\n-# sorting the input array twice.\n-def _averaged_weighted_percentile(array, sample_weight, percentile_rank=50, xp=None):\n-    return (\n-        _weighted_percentile(array, sample_weight, percentile_rank, xp=xp)\n-        - _weighted_percentile(-array, sample_weight, 100 - percentile_rank, xp=xp)\n-    ) / 2\ndiff --git a/sklearn/utils/tests/test_stats.py b/sklearn/utils/tests/test_stats.py\nindex 1c979425f12f8..6cc6505fecbf6 100644\n--- a/sklearn/utils/tests/test_stats.py\n+++ b/sklearn/utils/tests/test_stats.py\n@@ -12,121 +12,175 @@\n from sklearn.utils._array_api import device as array_device\n from sklearn.utils.estimator_checks import _array_api_for_tests\n from sklearn.utils.fixes import np_version, parse_version\n-from sklearn.utils.stats import _averaged_weighted_percentile, _weighted_percentile\n+from sklearn.utils.stats import _weighted_percentile\n \n \n-def test_averaged_weighted_median():\n-    y = np.array([0, 1, 2, 3, 4, 5])\n-    sw = np.array([1, 1, 1, 1, 1, 1])\n+@pytest.mark.parametrize(\"average\", [True, False])\n+@pytest.mark.parametrize(\"size\", [10, 15])\n+def test_weighted_percentile_matches_median(size, average):\n+    \"\"\"Ensure `_weighted_percentile` matches `median` when expected.\n \n-    score = _averaged_weighted_percentile(y, sw, 50)\n+    With unit `sample_weight`, `_weighted_percentile` should match the median except\n+    when `average=False` and the number of samples is even.\n+    For an even array and `average=False`, `percentile_rank=50` gives the lower\n+    of the two 'middle' values, that are averaged when calculating the `median`.\n+    \"\"\"\n+    y = np.arange(size)\n+    sample_weight = np.ones_like(y)\n \n-    assert score == np.median(y)\n+    score = _weighted_percentile(y, sample_weight, 50, average=average)\n \n+    # `_weighted_percentile(average=False)` does not match `median` when n is even\n+    if size % 2 == 0 and average is False:\n+        assert score != np.median(y)\n+    else:\n+        assert approx(score) == np.median(y)\n \n-def test_averaged_weighted_percentile(global_random_seed):\n-    rng = np.random.RandomState(global_random_seed)\n-    y = rng.randint(20, size=10)\n \n-    sw = np.ones(10)\n+@pytest.mark.parametrize(\"average\", [True, False])\n+@pytest.mark.parametrize(\"percentile_rank\", [20, 35, 61])\n+@pytest.mark.parametrize(\"size\", [10, 15])\n+def test_weighted_percentile_matches_numpy(\n+    global_random_seed, size, percentile_rank, average\n+):\n+    \"\"\"Check `_weighted_percentile` with unit weights is correct.\n \n-    score = _averaged_weighted_percentile(y, sw, 20)\n+    `average=True` results should be the same as `np.percentile`'s\n+    'averaged_inverted_cdf'.\n+    `average=False` results should be the same as `np.percentile`'s\n+    'inverted_cdf'.\n+    Note `np.percentile` is the same as `np.quantile` except `q` is in range [0, 100].\n \n-    assert score == np.percentile(y, 20, method=\"averaged_inverted_cdf\")\n+    We parametrize through different `percentile_rank` and `size` to\n+    ensure we get cases where `g=0` and `g>0` (see Hyndman and Fan 1996 for details).\n+    \"\"\"\n+    rng = np.random.RandomState(global_random_seed)\n+    y = rng.randint(20, size=size)\n+    sw = np.ones_like(y)\n \n+    score = _weighted_percentile(y, sw, percentile_rank, average=average)\n \n-def test_averaged_and_weighted_percentile():\n-    y = np.array([0, 1, 2])\n-    sw = np.array([5, 1, 5])\n-    q = 50\n+    if average:\n+        method = \"averaged_inverted_cdf\"\n+    else:\n+        method = \"inverted_cdf\"\n \n-    score_averaged = _averaged_weighted_percentile(y, sw, q)\n-    score = _weighted_percentile(y, sw, q)\n+    assert approx(score) == np.percentile(y, percentile_rank, method=method)\n \n-    assert score_averaged == score\n \n+@pytest.mark.parametrize(\"percentile_rank\", [50, 100])\n+def test_weighted_percentile_plus_one_clip_max(percentile_rank):\n+    \"\"\"Check `j+1` index is clipped to max, when `average=True`.\n \n-def test_weighted_percentile():\n-    \"\"\"Check `weighted_percentile` on artificial data with obvious median.\"\"\"\n-    y = np.empty(102, dtype=np.float64)\n-    y[:50] = 0\n-    y[-51:] = 2\n-    y[-1] = 100000\n-    y[50] = 1\n-    sw = np.ones(102, dtype=np.float64)\n-    sw[-1] = 0.0\n-    value = _weighted_percentile(y, sw, 50)\n-    assert approx(value) == 1\n+    `percentile_plus_one_indices` can exceed max index when `percentile_indices`\n+    is already at max index.\n+    Note that when `g` (Hyndman and Fan) / `fraction_above` is greater than 0,\n+    `j+1` (Hyndman and Fan) / `percentile_plus_one_indices` is calculated but\n+    never used, so it does not matter what this value is.\n+    When percentile of percentile rank 100 falls exactly on the last value in the\n+    `weighted_cdf`, `g=0` and `percentile_indices` is at max index. In this case\n+    we set `percentile_plus_one_indices` to be max index as well, so the result is\n+    the average of 2x the max index (i.e. last value of `weighted_cdf`).\n+    \"\"\"\n+    # Note for both `percentile_rank`s 50 and 100,`percentile_indices` is already at\n+    # max index\n+    y = np.array([[0, 0], [1, 1]])\n+    sw = np.array([[0.1, 0.2], [2, 3]])\n+    score = _weighted_percentile(y, sw, percentile_rank, average=True)\n+    for idx in range(2):\n+        assert score[idx] == approx(1.0)\n \n \n def test_weighted_percentile_equal():\n-    \"\"\"Check `weighted_percentile` with all weights equal to 1.\"\"\"\n-    y = np.empty(102, dtype=np.float64)\n-    y.fill(0.0)\n+    \"\"\"Check `weighted_percentile` with unit weights and all 0 values in `array`.\"\"\"\n+    y = np.zeros(102, dtype=np.float64)\n     sw = np.ones(102, dtype=np.float64)\n     score = _weighted_percentile(y, sw, 50)\n     assert approx(score) == 0\n \n \n-def test_weighted_percentile_zero_weight():\n-    \"\"\"Check `weighted_percentile` with all weights equal to 0.\"\"\"\n-    y = np.empty(102, dtype=np.float64)\n-    y.fill(1.0)\n-    sw = np.ones(102, dtype=np.float64)\n-    sw.fill(0.0)\n+# XXX: is this really what we want? Shouldn't we raise instead?\n+# https://github.com/scikit-learn/scikit-learn/issues/31032\n+def test_weighted_percentile_all_zero_weights():\n+    \"\"\"Check `weighted_percentile` with all weights equal to 0 returns last index.\"\"\"\n+    y = np.arange(10)\n+    sw = np.zeros(10)\n     value = _weighted_percentile(y, sw, 50)\n-    assert approx(value) == 1.0\n+    assert approx(value) == 9.0\n \n \n-def test_weighted_percentile_zero_weight_zero_percentile():\n-    \"\"\"Check `weighted_percentile(percentile_rank=0)` behaves correctly.\n+@pytest.mark.parametrize(\"average\", [True, False])\n+@pytest.mark.parametrize(\"percentile_rank, expected_value\", [(0, 2), (50, 3), (100, 5)])\n+def test_weighted_percentile_ignores_zero_weight(\n+    average, percentile_rank, expected_value\n+):\n+    \"\"\"Check leading, trailing and middle 0 weights behave correctly.\n \n-    Ensures that (leading)zero-weight observations ignored when `percentile_rank=0`.\n+    Check that leading zero-weight observations are ignored when `percentile_rank=0`.\n     See #20528 for details.\n+    Check that when `average=True` and the `j+1` ('plus one') index has sample weight\n+    of 0, it is ignored. Also check that trailing zero weight observations are ignored\n+    (e.g., when `percentile_rank=100`).\n     \"\"\"\n-    y = np.array([0, 1, 2, 3, 4, 5])\n-    sw = np.array([0, 0, 1, 1, 1, 0])\n-    value = _weighted_percentile(y, sw, 0)\n-    assert approx(value) == 2\n+    y = np.array([0, 1, 2, 3, 4, 5, 6])\n+    sw = np.array([0, 0, 1, 1, 0, 1, 0])\n \n-    value = _weighted_percentile(y, sw, 50)\n-    assert approx(value) == 3\n+    value = _weighted_percentile(\n+        np.vstack((y, y)).T, np.vstack((sw, sw)).T, percentile_rank, average=average\n+    )\n+    for idx in range(2):\n+        assert approx(value[idx]) == expected_value\n \n-    value = _weighted_percentile(y, sw, 100)\n-    assert approx(value) == 4\n \n+@pytest.mark.parametrize(\"average\", [True, False])\n+@pytest.mark.parametrize(\"percentile_rank\", [20, 35, 50, 61])\n+def test_weighted_percentile_frequency_weight_semantics(\n+    global_random_seed, percentile_rank, average\n+):\n+    \"\"\"Check integer weights give the same result as repeating values.\"\"\"\n+    rng = np.random.RandomState(global_random_seed)\n+    x = rng.randint(20, size=10)\n+    weights = rng.choice(5, size=10)\n \n-def test_weighted_median_equal_weights(global_random_seed):\n-    \"\"\"Checks `_weighted_percentile(percentile_rank=50)` is the same as `np.median`.\n+    x_repeated = np.repeat(x, weights)\n+    percentile_weights = _weighted_percentile(\n+        x, weights, percentile_rank, average=average\n+    )\n+    percentile_repeated = _weighted_percentile(\n+        x_repeated, np.ones_like(x_repeated), percentile_rank, average=average\n+    )\n+    assert percentile_weights == approx(percentile_repeated)\n+    # Also check `percentile_rank=50` matches `median`\n+    if percentile_rank == 50 and average:\n+        assert percentile_weights == approx(np.median(x_repeated))\n \n-    `sample_weights` are all 1s and the number of samples is odd.\n-    When number of samples is odd, `_weighted_percentile` always falls on a single\n-    observation (not between 2 values, in which case the lower value would be taken)\n-    and is thus equal to `np.median`.\n-    For an even number of samples, this check will not always hold as (note that\n-    for some other percentile methods it will always hold). See #17370 for details.\n-    \"\"\"\n-    rng = np.random.RandomState(global_random_seed)\n-    x = rng.randint(10, size=11)\n-    weights = np.ones(x.shape)\n-    median = np.median(x)\n-    w_median = _weighted_percentile(x, weights)\n-    assert median == approx(w_median)\n \n+@pytest.mark.parametrize(\"constant\", [5, 8])\n+@pytest.mark.parametrize(\"average\", [True, False])\n+@pytest.mark.parametrize(\"percentile_rank\", [20, 35, 50, 61])\n+def test_weighted_percentile_constant_multiplier(\n+    global_random_seed, percentile_rank, average, constant\n+):\n+    \"\"\"Check multiplying weights by a constant does not change the result.\n \n-def test_weighted_median_integer_weights(global_random_seed):\n-    # Checks average weighted percentile_rank=0.5 is same as median when manually weight\n-    # data\n+    Note scale invariance does not always hold when multiplying by a\n+    float due to cumulative sum numerical error (which grows proportional to n).\n+    \"\"\"\n     rng = np.random.RandomState(global_random_seed)\n-    x = rng.randint(20, size=10)\n-    weights = rng.choice(5, size=10)\n-    x_manual = np.repeat(x, weights)\n-    median = np.median(x_manual)\n-    w_median = _averaged_weighted_percentile(x, weights)\n-    assert median == approx(w_median)\n+    x = rng.randint(20, size=20)\n+    weights = rng.choice(5, size=20)\n+    weights_multiplied = weights * constant\n+\n+    percentile = _weighted_percentile(x, weights, percentile_rank, average=average)\n+    percentile_multiplier = _weighted_percentile(\n+        x, weights_multiplied, percentile_rank, average=average\n+    )\n+    assert percentile == approx(percentile_multiplier)\n \n \n-def test_weighted_percentile_2d(global_random_seed):\n+@pytest.mark.parametrize(\"average\", [True, False])\n+def test_weighted_percentile_2d(global_random_seed, average):\n+    \"\"\"Check `_weighted_percentile` behaviour is correct when `array` is 2D.\"\"\"\n     # Check for when array 2D and sample_weight 1D\n     rng = np.random.RandomState(global_random_seed)\n     x1 = rng.randint(10, size=10)\n@@ -135,16 +189,21 @@ def test_weighted_percentile_2d(global_random_seed):\n     x2 = rng.randint(20, size=10)\n     x_2d = np.vstack((x1, x2)).T\n \n-    w_median = _weighted_percentile(x_2d, w1)\n-    p_axis_0 = [_weighted_percentile(x_2d[:, i], w1) for i in range(x_2d.shape[1])]\n+    w_median = _weighted_percentile(x_2d, w1, average=average)\n+    p_axis_0 = [\n+        _weighted_percentile(x_2d[:, i], w1, average=average)\n+        for i in range(x_2d.shape[1])\n+    ]\n     assert_allclose(w_median, p_axis_0)\n+\n     # Check when array and sample_weight both 2D\n     w2 = rng.choice(5, size=10)\n     w_2d = np.vstack((w1, w2)).T\n \n-    w_median = _weighted_percentile(x_2d, w_2d)\n+    w_median = _weighted_percentile(x_2d, w_2d, average=average)\n     p_axis_0 = [\n-        _weighted_percentile(x_2d[:, i], w_2d[:, i]) for i in range(x_2d.shape[1])\n+        _weighted_percentile(x_2d[:, i], w_2d[:, i], average=average)\n+        for i in range(x_2d.shape[1])\n     ]\n     assert_allclose(w_median, p_axis_0)\n \n@@ -234,12 +293,18 @@ def test_weighted_percentile_array_api_consistency(\n         assert result_xp_np.dtype == np.float64\n \n \n+@pytest.mark.parametrize(\"average\", [True, False])\n @pytest.mark.parametrize(\"sample_weight_ndim\", [1, 2])\n-def test_weighted_percentile_nan_filtered(sample_weight_ndim, global_random_seed):\n-    \"\"\"Test that calling _weighted_percentile on an array with nan values returns\n-    the same results as calling _weighted_percentile on a filtered version of the data.\n+def test_weighted_percentile_nan_filtered(\n+    global_random_seed, sample_weight_ndim, average\n+):\n+    \"\"\"Test `_weighted_percentile` ignores NaNs.\n+\n+    Calling `_weighted_percentile` on an array with nan values returns the same\n+    results as calling `_weighted_percentile` on a filtered version of the data.\n     We test both with sample_weight of the same shape as the data and with\n-    one-dimensional sample_weight.\"\"\"\n+    one-dimensional sample_weight.\n+    \"\"\"\n \n     rng = np.random.RandomState(global_random_seed)\n     array_with_nans = rng.rand(100, 10)\n@@ -252,7 +317,7 @@ def test_weighted_percentile_nan_filtered(sample_weight_ndim, global_random_seed\n         sample_weight = rng.randint(1, 6, size=(100,))\n \n     # Find the weighted percentile on the array with nans:\n-    results = _weighted_percentile(array_with_nans, sample_weight, 30)\n+    results = _weighted_percentile(array_with_nans, sample_weight, 30, average=average)\n \n     # Find the weighted percentile on the filtered array:\n     filtered_array = [\n@@ -269,7 +334,9 @@ def test_weighted_percentile_nan_filtered(sample_weight_ndim, global_random_seed\n \n     expected_results = np.array(\n         [\n-            _weighted_percentile(filtered_array[col], filtered_weights[col], 30)\n+            _weighted_percentile(\n+                filtered_array[col], filtered_weights[col], 30, average=average\n+            )\n             for col in range(array_with_nans.shape[1])\n         ]\n     )\n@@ -306,19 +373,34 @@ def test_weighted_percentile_all_nan_column():\n     reason=\"np.quantile only accepts weights since version 2.0\",\n )\n @pytest.mark.parametrize(\"percentile\", [66, 10, 50])\n-def test_weighted_percentile_like_numpy_quantile(percentile, global_random_seed):\n-    \"\"\"Check that _weighted_percentile delivers equivalent results as np.quantile\n-    with weights.\"\"\"\n+@pytest.mark.parametrize(\"average\", [False, True])\n+@pytest.mark.parametrize(\"uniform_weight\", [False, True])\n+def test_weighted_percentile_like_numpy_quantile(\n+    percentile, average, uniform_weight, global_random_seed\n+):\n+    \"\"\"Check `_weighted_percentile` is equivalent to `np.quantile` with weights.\"\"\"\n+    # TODO: remove the following skip once no longer applicable.\n+    if average and not uniform_weight:\n+        pytest.skip(\n+            \"np.quantile does not support weights with method='averaged_inverted_cdf'\"\n+        )\n \n     rng = np.random.RandomState(global_random_seed)\n     array = rng.rand(10, 100)\n-    sample_weight = rng.randint(1, 6, size=(10, 100))\n+    if uniform_weight:\n+        sample_weight = np.ones_like(array) * rng.randint(1, 6, size=1)\n+    else:\n+        sample_weight = rng.randint(1, 6, size=(10, 100))\n \n     percentile_weighted_percentile = _weighted_percentile(\n-        array, sample_weight, percentile\n+        array, sample_weight, percentile, average=average\n     )\n     percentile_numpy_quantile = np.quantile(\n-        array, percentile / 100, weights=sample_weight, axis=0, method=\"inverted_cdf\"\n+        array,\n+        percentile / 100,\n+        weights=sample_weight if not uniform_weight else None,\n+        method=\"averaged_inverted_cdf\" if average else \"inverted_cdf\",\n+        axis=0,\n     )\n \n     assert_array_equal(percentile_weighted_percentile, percentile_numpy_quantile)\n@@ -329,24 +411,40 @@ def test_weighted_percentile_like_numpy_quantile(percentile, global_random_seed)\n     reason=\"np.nanquantile only accepts weights since version 2.0\",\n )\n @pytest.mark.parametrize(\"percentile\", [66, 10, 50])\n-def test_weighted_percentile_like_numpy_nanquantile(percentile, global_random_seed):\n-    \"\"\"Check that _weighted_percentile delivers equivalent results as np.nanquantile\n-    with weights.\"\"\"\n+@pytest.mark.parametrize(\"average\", [False, True])\n+@pytest.mark.parametrize(\"uniform_weight\", [False, True])\n+def test_weighted_percentile_like_numpy_nanquantile(\n+    percentile, average, uniform_weight, global_random_seed\n+):\n+    \"\"\"Check `_weighted_percentile` equivalent to `np.nanquantile` with weights.\"\"\"\n+    # TODO: remove the following skip once no longer applicable.\n+    if average and not uniform_weight:\n+        pytest.skip(\n+            \"np.nanquantile does not support weights with \"\n+            \"method='averaged_inverted_cdf'\"\n+        )\n \n     rng = np.random.RandomState(global_random_seed)\n     array_with_nans = rng.rand(10, 100)\n     array_with_nans[rng.rand(*array_with_nans.shape) < 0.5] = np.nan\n-    sample_weight = rng.randint(1, 6, size=(10, 100))\n+    if uniform_weight:\n+        sample_weight = np.ones_like(array_with_nans) * rng.randint(\n+            1,\n+            6,\n+            size=1,\n+        )\n+    else:\n+        sample_weight = rng.randint(1, 6, size=(10, 100))\n \n     percentile_weighted_percentile = _weighted_percentile(\n-        array_with_nans, sample_weight, percentile\n+        array_with_nans, sample_weight, percentile, average=average\n     )\n     percentile_numpy_nanquantile = np.nanquantile(\n         array_with_nans,\n         percentile / 100,\n-        weights=sample_weight,\n+        weights=sample_weight if not uniform_weight else None,\n+        method=\"averaged_inverted_cdf\" if average else \"inverted_cdf\",\n         axis=0,\n-        method=\"inverted_cdf\",\n     )\n \n     assert_array_equal(percentile_weighted_percentile, percentile_numpy_nanquantile)\n",
  "fail_to_pass": [
    "test_weighted_percentile_matches_median",
    "test_weighted_percentile_matches_numpy",
    "test_weighted_percentile_plus_one_clip_max",
    "test_weighted_percentile_all_zero_weights",
    "test_weighted_percentile_ignores_zero_weight",
    "test_weighted_percentile_frequency_weight_semantics",
    "test_weighted_percentile_constant_multiplier",
    "test_weighted_percentile_2d",
    "test_weighted_percentile_nan_filtered",
    "test_weighted_percentile_like_numpy_quantile",
    "test_weighted_percentile_like_numpy_nanquantile"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_regression.py",
    "sklearn/preprocessing/_discretization.py",
    "sklearn/utils/stats.py",
    "sklearn/utils/tests/test_stats.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-07-17T11:21:00Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31775",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/30945"
}