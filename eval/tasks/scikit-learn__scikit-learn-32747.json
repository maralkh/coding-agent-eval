{
  "id": "scikit-learn__scikit-learn-32747",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "cef41a27381bc5cd5dd75a12e2f8e0420f8086bd",
  "issue_number": 32748,
  "issue_title": "LogisticRegressionCV bug when one fold has not all classes",
  "issue_body": "I may be missing something but it seems like the coefficient for a given class does not stay at zero when a class is missing.\n\nI took the Iris dataset with the well known issue that `y` is ordered with 3 classes so that if you use `cv=KFold(3)` you will get the issue that each training fold will have 2 of the 3 classes, and the the test fold has the third class.\n\n```py\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import KFold\n\nX, y = load_iris(return_X_y=True)\n\n# Error occurs on `main` but on on this branch\nlr = LogisticRegressionCV(cv=KFold(3), Cs=[1], fit_intercept=False).fit(X, y)\nlr.coefs_paths_\n```\n\nAs far as I can see there are no zero-coefficient anywhere?\n```\n{np.int64(0): array([[[ 0.36617189,  0.4181025 , -1.80054529, -1.59885258,\n          11.94538259]],\n \n        [[-0.27013869,  0.19659815, -1.0394569 , -0.4717853 ,\n           8.11303679]],\n \n        [[-0.2259761 ,  0.55491805, -1.32935417, -0.56475712,\n           6.7920903 ]]]),\n np.int64(1): array([[[-0.36863165, -0.41910848,  1.80385762,  1.60144714,\n          -4.03491417]],\n \n        [[ 0.26501596, -0.1994853 ,  1.03704383,  0.47085702,\n          -1.69095992]],\n \n        [[ 0.22447638, -0.55428594,  1.33474803,  0.56733333,\n          -0.29154238]]]),\n np.int64(2): array([[[ 2.45975917e-03,  1.00598025e-03, -3.31232754e-03,\n          -2.59456480e-03, -7.91046843e+00]],\n \n        [[ 5.12273225e-03,  2.88714428e-03,  2.41307063e-03,\n           9.28278209e-04, -6.42207687e+00]],\n \n        [[ 1.49971712e-03, -6.32115481e-04, -5.39386365e-03,\n          -2.57621573e-03, -6.50054792e+00]]])}\n```\n\nFor completeness, the same snippet fails on `main` so I guess this could be turned into a test for the new behaviour if it becomes clearer what to test exactly (maybe a smoke test is enough?) ...\n```\nValueError: cannot reshape array of size 15 into shape (3,1,3,newaxis)\n```\n\n> [!WARNING]\n> @lesteve spotted a hickup (call it bug) that needs fixing.\n\n#### Checking the assumptions\nSmuggling in\n```python\nc, f = np.unique(y_train, return_counts=True)\ncf = {int(ci):int(fi) for ci, fi in zip(c, f)}\nprint(f\"y_train counts={cf}\")\n```\nright before `coefs, Cs, n_iter = _logistic_regression_path(` around line 683 gives\n```\ny_train counts={1: 50, 2: 50}\ny_train counts={0: 50, 2: 50}\ny_train counts={0: 50, 1: 50}\n```\nThis confirms: Class i is missing in fold i.\n\n#### Why are the coefficients of all classes changed?\nBecause they enter the objective function via the penalty. Therefore, the above code comment should be modified.\n\n#### Why there is a bug to fix?\nInside function `_logistic_regression_path`, around line 280\n```py\nle = LabelEncoder()\n```\nThis should instead read\n```py\nle = LabelEncoder().fit(classes)\n```\nand the subsequent `fit_transform` should be changed to `transform` (this fixes a bug and makes it faster).\n\n#### Remaining tasks\nEven if this is fixed, there remains a bug if one passes another scorer, e.g.\n```py\nlr = LogisticRegressionCV(cv=KFold(3), Cs=[1], fit_intercept=False,scoring=\"neg_brier_score\").fit(X, y)\n```\n```\nValueError: y_true contains only one label (0). Please provide the list of all expected class labels explicitly through the labels argument.\n```\nI would postpone fixing this to another future PR, maybe not part of 1.8. The situation is already better if the above is fixed.\n\n_Originally posted by @lorentzenchr in https://github.com/scikit-learn/scikit-learn/pull/32073#discussion_r2539697181_\n            ",
  "pr_number": 32747,
  "pr_title": "FIX LogisticRegressionCV with CV split with missing class labels",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/32747.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/32747.fix.rst\nnew file mode 100644\nindex 0000000000000..38e560d6f6f75\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/32747.fix.rst\n@@ -0,0 +1,4 @@\n+- :class:`linear_model.LogisticRegressionCV` is able to handle CV splits where\n+  some class labels are missing in some folds. Before, it raised an error whenever a\n+  class label were missing in a fold.\n+  By :user:`Christian Lorentzen <lorentzenchr>\ndiff --git a/sklearn/linear_model/_logistic.py b/sklearn/linear_model/_logistic.py\nindex d6bdc1c715af0..baddda1a105dc 100644\n--- a/sklearn/linear_model/_logistic.py\n+++ b/sklearn/linear_model/_logistic.py\n@@ -276,12 +276,12 @@ def _logistic_regression_path(\n \n     random_state = check_random_state(random_state)\n \n-    le = LabelEncoder()\n+    le = LabelEncoder().fit(classes)\n     if class_weight is not None:\n         class_weight_ = compute_class_weight(\n             class_weight, classes=classes, y=y, sample_weight=sample_weight\n         )\n-        sample_weight *= class_weight_[le.fit_transform(y)]\n+        sample_weight *= class_weight_[le.transform(y)]\n \n     if is_binary:\n         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)\n@@ -297,7 +297,7 @@ def _logistic_regression_path(\n         # All solvers capable of a multinomial need LabelEncoder, not LabelBinarizer,\n         # i.e. y as a 1d-array of integers. LabelEncoder also saves memory\n         # compared to LabelBinarizer, especially when n_classes is large.\n-        Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)\n+        Y_multi = le.transform(y).astype(X.dtype, copy=False)\n         # It is important that w0 is F-contiguous.\n         w0 = np.zeros(\n             (classes.size, n_features + int(fit_intercept)), order=\"F\", dtype=X.dtype\n@@ -676,9 +676,10 @@ def _log_reg_scoring_path(\n         sw_train = sample_weight[train]\n         sw_test = sample_weight[test]\n \n-    # Note: We pass classes for the whole dataset to avoid inconsistencies, i.e.\n-    # different number of classes in different folds. This way, if a class is empty\n-    # in a fold, _logistic_regression_path will initialize it to zero and not change.\n+    # Note: We pass classes for the whole dataset to avoid inconsistencies,\n+    # i.e. different number of classes in different folds. This way, if a class\n+    # is not present in a fold, _logistic_regression_path will still return\n+    # coefficients associated to this class.\n     coefs, Cs, n_iter = _logistic_regression_path(\n         X_train,\n         y_train,\n@@ -721,6 +722,9 @@ def _log_reg_scoring_path(\n         else:\n             score_params = score_params or {}\n             score_params = _check_method_params(X=X, params=score_params, indices=test)\n+            # FIXME: If scoring = \"neg_brier_score\" and if not all class labels\n+            # are present in y_test, the following fails. Maybe we can pass\n+            # \"labels=classes\" to the call of scoring.\n             scores.append(scoring(log_reg, X_test, y_test, **score_params))\n     return coefs, Cs, np.array(scores), n_iter\n \ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\nindex 3c8b81fb62a15..33e386bc04e13 100644\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -23,9 +23,10 @@\n     _log_reg_scoring_path,\n     _logistic_regression_path,\n )\n-from sklearn.metrics import get_scorer, log_loss\n+from sklearn.metrics import brier_score_loss, get_scorer, log_loss, make_scorer\n from sklearn.model_selection import (\n     GridSearchCV,\n+    KFold,\n     LeaveOneGroupOut,\n     StratifiedKFold,\n     cross_val_score,\n@@ -647,19 +648,19 @@ def test_logistic_cv_sparse(global_random_seed, csr_container):\n @pytest.mark.parametrize(\"use_legacy_attributes\", [True, False])\n def test_multinomial_cv_iris(use_legacy_attributes):\n     # Test that multinomial LogisticRegressionCV is correct using the iris dataset.\n-    train, target = iris.data, iris.target\n-    n_samples, n_features = train.shape\n+    X, y = iris.data, iris.target\n+    n_samples, n_features = X.shape\n \n     # The cv indices from stratified kfold\n     n_cv = 2\n     cv = StratifiedKFold(n_cv)\n-    precomputed_folds = list(cv.split(train, target))\n+    precomputed_folds = list(cv.split(X, y))\n \n     # Train clf on the original dataset\n     clf = LogisticRegressionCV(\n         cv=precomputed_folds, solver=\"newton-cholesky\", use_legacy_attributes=True\n     )\n-    clf.fit(train, target)\n+    clf.fit(X, y)\n \n     # Test the shape of various attributes.\n     assert clf.coef_.shape == (3, n_features)\n@@ -674,7 +675,7 @@ def test_multinomial_cv_iris(use_legacy_attributes):\n     clf_ovr = GridSearchCV(\n         OneVsRestClassifier(LogisticRegression(solver=\"newton-cholesky\")),\n         {\"estimator__C\": np.logspace(-4, 4, num=10)},\n-    ).fit(train, target)\n+    ).fit(X, y)\n     for solver in [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"]:\n         max_iter = 500 if solver in [\"sag\", \"saga\"] else 30\n         clf_multi = LogisticRegressionCV(\n@@ -687,11 +688,11 @@ def test_multinomial_cv_iris(use_legacy_attributes):\n         )\n         if solver == \"lbfgs\":\n             # lbfgs requires scaling to avoid convergence warnings\n-            train = scale(train)\n+            X = scale(X)\n \n-        clf_multi.fit(train, target)\n-        multi_score = clf_multi.score(train, target)\n-        ovr_score = clf_ovr.score(train, target)\n+        clf_multi.fit(X, y)\n+        multi_score = clf_multi.score(X, y)\n+        ovr_score = clf_ovr.score(X, y)\n         assert multi_score > ovr_score\n \n         # Test attributes of LogisticRegressionCV\n@@ -737,10 +738,48 @@ def test_multinomial_cv_iris(use_legacy_attributes):\n                 # Note that we have to exclude the intercept, hence the ':-1'\n                 # on the last dimension\n                 coefs = clf_multi.coefs_paths_[fold, 0, :, :, :-1]\n-                coefs = coefs.reshape(len(clf_multi.Cs_), -1)\n-                norms = np.sum(coefs * coefs, axis=1)  # L2 norm for each C\n+                norms = np.sum(coefs * coefs, axis=(-2, -1))  # L2 norm for each C\n                 assert np.all(np.diff(norms) >= 0)\n \n+    # Test CV folds with missing class labels:\n+    # The iris target variable has 3 classes and is ordered such that a simple\n+    # CV split with 3 folds separates the classes.\n+    cv = KFold(n_splits=3)\n+    # Check this assumption.\n+    classes = np.unique(y)\n+    assert len(classes) == 3\n+    for train, test in cv.split(X, y):\n+        assert len(np.unique(y[train])) == 2\n+        assert len(np.unique(y[test])) == 1\n+        assert set(y[train]) & set(y[test]) == set()\n+\n+    clf = LogisticRegressionCV(cv=cv, use_legacy_attributes=False).fit(X, y)\n+    # We expect accuracy to be exactly 0 because train and test sets have\n+    # non-overlapping labels\n+    assert np.all(clf.scores_ == 0.0)\n+\n+    # We use a proper scoring rule, i.e. the Brier score, to evaluate our classifier.\n+    # Because of a bug in LogisticRegressionCV, we need to create our own scoring\n+    # function to pass explicitly the labels.\n+    scoring = make_scorer(\n+        brier_score_loss,\n+        greater_is_better=False,\n+        response_method=\"predict_proba\",\n+        scale_by_half=True,\n+        labels=classes,\n+    )\n+    # We set small Cs, that is strong penalty as the best C is likely the smallest one.\n+    clf = LogisticRegressionCV(\n+        cv=cv, scoring=scoring, Cs=np.logspace(-6, 3, 10), use_legacy_attributes=False\n+    ).fit(X, y)\n+    assert clf.C_ == 1e-6  # smallest value of provided Cs\n+    brier_scores = -clf.scores_\n+    # We expect the scores to be bad because train and test sets have\n+    # non-overlapping labels\n+    assert np.all(brier_scores > 0.7)\n+    # But the best score should be better than the worst value of 1.\n+    assert np.min(brier_scores) < 0.8\n+\n \n def test_logistic_regression_solvers(global_random_seed):\n     \"\"\"Test solvers converge to the same result.\"\"\"\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_logistic.py",
    "sklearn/linear_model/tests/test_logistic.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-11-19T19:35:16Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32747",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/32748"
}