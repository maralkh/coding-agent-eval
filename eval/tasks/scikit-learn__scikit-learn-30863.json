{
  "id": "scikit-learn__scikit-learn-30863",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "6a2472fa5e48a53907418a427c29562a889bd1a7",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30863,
  "pr_title": "TST use global_random_seed in sklearn/linear_model/tests/test_linear_loss.py",
  "gold_patch": "diff --git a/sklearn/linear_model/tests/test_linear_loss.py b/sklearn/linear_model/tests/test_linear_loss.py\nindex ac06af9e65ac0..a273656b3dbb8 100644\n--- a/sklearn/linear_model/tests/test_linear_loss.py\n+++ b/sklearn/linear_model/tests/test_linear_loss.py\n@@ -81,10 +81,12 @@ def choice_vectorized(items, p):\n @pytest.mark.parametrize(\"fit_intercept\", [False, True])\n @pytest.mark.parametrize(\"n_features\", [0, 1, 10])\n @pytest.mark.parametrize(\"dtype\", [None, np.float32, np.float64, np.int64])\n-def test_init_zero_coef(base_loss, fit_intercept, n_features, dtype):\n+def test_init_zero_coef(\n+    base_loss, fit_intercept, n_features, dtype, global_random_seed\n+):\n     \"\"\"Test that init_zero_coef initializes coef correctly.\"\"\"\n     loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)\n-    rng = np.random.RandomState(42)\n+    rng = np.random.RandomState(global_random_seed)\n     X = rng.normal(size=(5, n_features))\n     coef = loss.init_zero_coef(X, dtype=dtype)\n     if loss.base_loss.is_multiclass:\n@@ -108,12 +110,17 @@ def test_init_zero_coef(base_loss, fit_intercept, n_features, dtype):\n @pytest.mark.parametrize(\"l2_reg_strength\", [0, 1])\n @pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\n def test_loss_grad_hess_are_the_same(\n-    base_loss, fit_intercept, sample_weight, l2_reg_strength, csr_container\n+    base_loss,\n+    fit_intercept,\n+    sample_weight,\n+    l2_reg_strength,\n+    csr_container,\n+    global_random_seed,\n ):\n     \"\"\"Test that loss and gradient are the same across different functions.\"\"\"\n     loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)\n     X, y, coef = random_X_y_coef(\n-        linear_model_loss=loss, n_samples=10, n_features=5, seed=42\n+        linear_model_loss=loss, n_samples=10, n_features=5, seed=global_random_seed\n     )\n     X_old, y_old, coef_old = X.copy(), y.copy(), coef.copy()\n \n@@ -198,14 +205,17 @@ def test_loss_grad_hess_are_the_same(\n @pytest.mark.parametrize(\"l2_reg_strength\", [0, 1])\n @pytest.mark.parametrize(\"X_container\", CSR_CONTAINERS + [None])\n def test_loss_gradients_hessp_intercept(\n-    base_loss, sample_weight, l2_reg_strength, X_container\n+    base_loss, sample_weight, l2_reg_strength, X_container, global_random_seed\n ):\n     \"\"\"Test that loss and gradient handle intercept correctly.\"\"\"\n     loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=False)\n     loss_inter = LinearModelLoss(base_loss=base_loss(), fit_intercept=True)\n     n_samples, n_features = 10, 5\n     X, y, coef = random_X_y_coef(\n-        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42\n+        linear_model_loss=loss,\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        seed=global_random_seed,\n     )\n \n     X[:, -1] = 1  # make last column of 1 to mimic intercept term\n@@ -241,7 +251,7 @@ def test_loss_gradients_hessp_intercept(\n     g_inter_corrected.T[-1] += l2_reg_strength * coef.T[-1]\n     assert_allclose(g, g_inter_corrected)\n \n-    s = np.random.RandomState(42).randn(*coef.shape)\n+    s = np.random.RandomState(global_random_seed).randn(*coef.shape)\n     h = hessp(s)\n     h_inter = hessp_inter(s)\n     h_inter_corrected = h_inter\n@@ -254,7 +264,7 @@ def test_loss_gradients_hessp_intercept(\n @pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\n @pytest.mark.parametrize(\"l2_reg_strength\", [0, 1])\n def test_gradients_hessians_numerically(\n-    base_loss, fit_intercept, sample_weight, l2_reg_strength\n+    base_loss, fit_intercept, sample_weight, l2_reg_strength, global_random_seed\n ):\n     \"\"\"Test gradients and hessians with numerical derivatives.\n \n@@ -264,7 +274,10 @@ def test_gradients_hessians_numerically(\n     loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)\n     n_samples, n_features = 10, 5\n     X, y, coef = random_X_y_coef(\n-        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42\n+        linear_model_loss=loss,\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        seed=global_random_seed,\n     )\n     coef = coef.ravel(order=\"F\")  # this is important only for multinomial loss\n \n@@ -335,14 +348,17 @@ def test_gradients_hessians_numerically(\n \n \n @pytest.mark.parametrize(\"fit_intercept\", [False, True])\n-def test_multinomial_coef_shape(fit_intercept):\n+def test_multinomial_coef_shape(fit_intercept, global_random_seed):\n     \"\"\"Test that multinomial LinearModelLoss respects shape of coef.\"\"\"\n     loss = LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept)\n     n_samples, n_features = 10, 5\n     X, y, coef = random_X_y_coef(\n-        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42\n+        linear_model_loss=loss,\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        seed=global_random_seed,\n     )\n-    s = np.random.RandomState(42).randn(*coef.shape)\n+    s = np.random.RandomState(global_random_seed).randn(*coef.shape)\n \n     l, g = loss.loss_gradient(coef, X, y)\n     g1 = loss.gradient(coef, X, y)\n@@ -373,7 +389,7 @@ def test_multinomial_coef_shape(fit_intercept):\n \n \n @pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\n-def test_multinomial_hessian_3_classes(sample_weight):\n+def test_multinomial_hessian_3_classes(sample_weight, global_random_seed):\n     \"\"\"Test multinomial hessian for 3 classes and 2 points.\n \n     For n_classes = 3 and n_samples = 2, we have\n@@ -391,7 +407,10 @@ def test_multinomial_hessian_3_classes(sample_weight):\n         base_loss=HalfMultinomialLoss(n_classes=n_classes), fit_intercept=False\n     )\n     X, y, coef = random_X_y_coef(\n-        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42\n+        linear_model_loss=loss,\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        seed=global_random_seed,\n     )\n     coef = coef.ravel(order=\"F\")  # this is important only for multinomial loss\n \n",
  "fail_to_pass": [
    "test_init_zero_coef",
    "test_multinomial_coef_shape",
    "test_multinomial_hessian_3_classes"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/tests/test_linear_loss.py"
  ],
  "difficulty": "easy",
  "created_at": "2025-02-19T19:58:50Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30863",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}