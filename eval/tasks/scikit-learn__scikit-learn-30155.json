{
  "id": "scikit-learn__scikit-learn-30155",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "d2e123d29237c9428e85e79ca6ac6331422039a3",
  "issue_number": 25646,
  "issue_title": "feat: Support sample weight for MLP",
  "issue_body": "<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https://github.com/blog/1506-closing-issues-via-pull-requests\r\n-->\r\nsample_weight support will contributes to class_weight support, so this PR contributes to the issue https://github.com/scikit-learn/scikit-learn/issues/9113, and the stalled PR https://github.com/scikit-learn/scikit-learn/pull/11723 \r\n\r\n#### What does this implement/fix? Explain your changes.\r\n\r\n- [x] Add sample_weight support for MLP\r\n\r\n \r\n\r\n   - [x] Add sample_weight support for multiple loss functions:\r\n     - [x] log_loss\r\n     - [x] binary_log_loss\r\n     - [x]  squared_loss\r\n   - [x]  Add sample_weight parameter to fit and partial_fit\r\n     - [x]  Propagate to relevant functions\r\n     - [x]  Apply sample_weight to gradient calculation\r\n     - [x]  Validate sample_weight\r\n     - [x]  Handle zero weights\r\n   - [x]  Add tests\r\n     - [x]  Test directly loss functions sample_weight invariance\r\n     - [x]  Test fit validation of sample_weight\r\n     - [x]  Test sample weight effect for both MLPClassifier and MLPRegressor\r\n     - [x] Test sample weight together with early stopping\r\n\r\n\r\n#### Any other comments? \r\nI have a draft PR https://github.com/scikit-learn/scikit-learn/pull/25326 to support class weight for MLP which will depend on this PR for sample weight support first. \r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n",
  "pr_number": 30155,
  "pr_title": "ENH add sample weight support to MLP",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.neural_network/30155.feature.rst b/doc/whats_new/upcoming_changes/sklearn.neural_network/30155.feature.rst\nnew file mode 100644\nindex 0000000000000..4fcf738072e5e\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.neural_network/30155.feature.rst\n@@ -0,0 +1,3 @@\n+- Added support for `sample_weight` in :class:`neural_network.MLPClassifier` and\n+  :class:`neural_network.MLPRegressor`.\n+  By :user:`Zach Shu <zshu115x>` and :user:`Christian Lorentzen <lorentzenchr>`\ndiff --git a/sklearn/neural_network/_base.py b/sklearn/neural_network/_base.py\nindex 505b62f0154e9..98f2d50c4a57e 100644\n--- a/sklearn/neural_network/_base.py\n+++ b/sklearn/neural_network/_base.py\n@@ -153,7 +153,7 @@ def inplace_relu_derivative(Z, delta):\n }\n \n \n-def squared_loss(y_true, y_pred):\n+def squared_loss(y_true, y_pred, sample_weight=None):\n     \"\"\"Compute the squared loss for regression.\n \n     Parameters\n@@ -164,15 +164,20 @@ def squared_loss(y_true, y_pred):\n     y_pred : array-like or label indicator matrix\n         Predicted values, as returned by a regression estimator.\n \n+    sample_weight : array-like of shape (n_samples,), default=None\n+        Sample weights.\n+\n     Returns\n     -------\n     loss : float\n         The degree to which the samples are correctly predicted.\n     \"\"\"\n-    return ((y_true - y_pred) ** 2).mean() / 2\n+    return (\n+        0.5 * np.average((y_true - y_pred) ** 2, weights=sample_weight, axis=0).mean()\n+    )\n \n \n-def log_loss(y_true, y_prob):\n+def log_loss(y_true, y_prob, sample_weight=None):\n     \"\"\"Compute Logistic loss for classification.\n \n     Parameters\n@@ -184,6 +189,9 @@ def log_loss(y_true, y_prob):\n         Predicted probabilities, as returned by a classifier's\n         predict_proba method.\n \n+    sample_weight : array-like of shape (n_samples,), default=None\n+        Sample weights.\n+\n     Returns\n     -------\n     loss : float\n@@ -197,10 +205,10 @@ def log_loss(y_true, y_prob):\n     if y_true.shape[1] == 1:\n         y_true = np.append(1 - y_true, y_true, axis=1)\n \n-    return -xlogy(y_true, y_prob).sum() / y_prob.shape[0]\n+    return -np.average(xlogy(y_true, y_prob), weights=sample_weight, axis=0).sum()\n \n \n-def binary_log_loss(y_true, y_prob):\n+def binary_log_loss(y_true, y_prob, sample_weight=None):\n     \"\"\"Compute binary logistic loss for classification.\n \n     This is identical to log_loss in binary classification case,\n@@ -215,6 +223,9 @@ def binary_log_loss(y_true, y_prob):\n         Predicted probabilities, as returned by a classifier's\n         predict_proba method.\n \n+    sample_weight : array-like of shape (n_samples,), default=None\n+        Sample weights.\n+\n     Returns\n     -------\n     loss : float\n@@ -222,10 +233,11 @@ def binary_log_loss(y_true, y_prob):\n     \"\"\"\n     eps = np.finfo(y_prob.dtype).eps\n     y_prob = np.clip(y_prob, eps, 1 - eps)\n-    return (\n-        -(xlogy(y_true, y_prob).sum() + xlogy(1 - y_true, 1 - y_prob).sum())\n-        / y_prob.shape[0]\n-    )\n+    return -np.average(\n+        xlogy(y_true, y_prob) + xlogy(1 - y_true, 1 - y_prob),\n+        weights=sample_weight,\n+        axis=0,\n+    ).sum()\n \n \n LOSS_FUNCTIONS = {\ndiff --git a/sklearn/neural_network/_multilayer_perceptron.py b/sklearn/neural_network/_multilayer_perceptron.py\nindex 47805857b5154..6c09ca4f804e4 100644\n--- a/sklearn/neural_network/_multilayer_perceptron.py\n+++ b/sklearn/neural_network/_multilayer_perceptron.py\n@@ -4,7 +4,7 @@\n # SPDX-License-Identifier: BSD-3-Clause\n \n import warnings\n-from abc import ABCMeta, abstractmethod\n+from abc import ABC, abstractmethod\n from itertools import chain\n from numbers import Integral, Real\n \n@@ -38,7 +38,7 @@\n     unique_labels,\n )\n from ..utils.optimize import _check_optimize_result\n-from ..utils.validation import check_is_fitted, validate_data\n+from ..utils.validation import _check_sample_weight, check_is_fitted, validate_data\n from ._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\n from ._stochastic_optimizers import AdamOptimizer, SGDOptimizer\n \n@@ -50,7 +50,7 @@ def _pack(coefs_, intercepts_):\n     return np.hstack([l.ravel() for l in coefs_ + intercepts_])\n \n \n-class BaseMultilayerPerceptron(BaseEstimator, metaclass=ABCMeta):\n+class BaseMultilayerPerceptron(BaseEstimator, ABC):\n     \"\"\"Base class for MLP classification and regression.\n \n     Warning: This class should not be used directly.\n@@ -219,7 +219,7 @@ def _forward_pass_fast(self, X, check_input=True):\n         return activation\n \n     def _compute_loss_grad(\n-        self, layer, n_samples, activations, deltas, coef_grads, intercept_grads\n+        self, layer, sw_sum, activations, deltas, coef_grads, intercept_grads\n     ):\n         \"\"\"Compute the gradient of loss with respect to coefs and intercept for\n         specified layer.\n@@ -228,12 +228,20 @@ def _compute_loss_grad(\n         \"\"\"\n         coef_grads[layer] = safe_sparse_dot(activations[layer].T, deltas[layer])\n         coef_grads[layer] += self.alpha * self.coefs_[layer]\n-        coef_grads[layer] /= n_samples\n+        coef_grads[layer] /= sw_sum\n \n-        intercept_grads[layer] = np.mean(deltas[layer], 0)\n+        intercept_grads[layer] = np.sum(deltas[layer], axis=0) / sw_sum\n \n     def _loss_grad_lbfgs(\n-        self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads\n+        self,\n+        packed_coef_inter,\n+        X,\n+        y,\n+        sample_weight,\n+        activations,\n+        deltas,\n+        coef_grads,\n+        intercept_grads,\n     ):\n         \"\"\"Compute the MLP loss function and its corresponding derivatives\n         with respect to the different parameters given in the initialization.\n@@ -252,6 +260,9 @@ def _loss_grad_lbfgs(\n         y : ndarray of shape (n_samples,)\n             The target values.\n \n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n         activations : list, length = n_layers - 1\n             The ith element of the list holds the values of the ith layer.\n \n@@ -277,12 +288,14 @@ def _loss_grad_lbfgs(\n         \"\"\"\n         self._unpack(packed_coef_inter)\n         loss, coef_grads, intercept_grads = self._backprop(\n-            X, y, activations, deltas, coef_grads, intercept_grads\n+            X, y, sample_weight, activations, deltas, coef_grads, intercept_grads\n         )\n         grad = _pack(coef_grads, intercept_grads)\n         return loss, grad\n \n-    def _backprop(self, X, y, activations, deltas, coef_grads, intercept_grads):\n+    def _backprop(\n+        self, X, y, sample_weight, activations, deltas, coef_grads, intercept_grads\n+    ):\n         \"\"\"Compute the MLP loss function and its corresponding derivatives\n         with respect to each parameter: weights and bias vectors.\n \n@@ -294,6 +307,9 @@ def _backprop(self, X, y, activations, deltas, coef_grads, intercept_grads):\n         y : ndarray of shape (n_samples,)\n             The target values.\n \n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n         activations : list, length = n_layers - 1\n              The ith element of the list holds the values of the ith layer.\n \n@@ -327,36 +343,46 @@ def _backprop(self, X, y, activations, deltas, coef_grads, intercept_grads):\n         loss_func_name = self.loss\n         if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n             loss_func_name = \"binary_log_loss\"\n-        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n+        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1], sample_weight)\n         # Add L2 regularization term to loss\n         values = 0\n         for s in self.coefs_:\n             s = s.ravel()\n             values += np.dot(s, s)\n-        loss += (0.5 * self.alpha) * values / n_samples\n+        if sample_weight is None:\n+            sw_sum = n_samples\n+        else:\n+            sw_sum = sample_weight.sum()\n+        loss += (0.5 * self.alpha) * values / sw_sum\n \n         # Backward propagate\n         last = self.n_layers_ - 2\n \n-        # The calculation of delta[last] here works with following\n-        # combinations of output activation and loss function:\n+        # The calculation of delta[last] is as follows:\n+        #   delta[last] = d/dz loss(y, act(z)) = act(z) - y\n+        # with z=x@w + b being the output of the last layer before passing through the\n+        # output activation, act(z) = activations[-1].\n+        # The simple formula for delta[last] here works with following (canonical\n+        # loss-link) combinations of output activation and loss function:\n         # sigmoid and binary cross entropy, softmax and categorical cross\n         # entropy, and identity with squared loss\n         deltas[last] = activations[-1] - y\n+        if sample_weight is not None:\n+            deltas[last] *= sample_weight.reshape(-1, 1)\n \n         # Compute gradient for the last layer\n         self._compute_loss_grad(\n-            last, n_samples, activations, deltas, coef_grads, intercept_grads\n+            last, sw_sum, activations, deltas, coef_grads, intercept_grads\n         )\n \n         inplace_derivative = DERIVATIVES[self.activation]\n         # Iterate over the hidden layers\n-        for i in range(self.n_layers_ - 2, 0, -1):\n+        for i in range(last, 0, -1):\n             deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n             inplace_derivative(activations[i], deltas[i - 1])\n \n             self._compute_loss_grad(\n-                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n+                i - 1, sw_sum, activations, deltas, coef_grads, intercept_grads\n             )\n \n         return loss, coef_grads, intercept_grads\n@@ -424,7 +450,7 @@ def _init_coef(self, fan_in, fan_out, dtype):\n         intercept_init = intercept_init.astype(dtype, copy=False)\n         return coef_init, intercept_init\n \n-    def _fit(self, X, y, incremental=False):\n+    def _fit(self, X, y, sample_weight=None, incremental=False):\n         # Make sure self.hidden_layer_sizes is a list\n         hidden_layer_sizes = self.hidden_layer_sizes\n         if not hasattr(hidden_layer_sizes, \"__iter__\"):\n@@ -440,8 +466,9 @@ def _fit(self, X, y, incremental=False):\n         )\n \n         X, y = self._validate_input(X, y, incremental, reset=first_pass)\n-\n         n_samples, n_features = X.shape\n+        if sample_weight is not None:\n+            sample_weight = _check_sample_weight(sample_weight, X)\n \n         # Ensure y is 2D\n         if y.ndim == 1:\n@@ -476,6 +503,7 @@ def _fit(self, X, y, incremental=False):\n             self._fit_stochastic(\n                 X,\n                 y,\n+                sample_weight,\n                 activations,\n                 deltas,\n                 coef_grads,\n@@ -487,7 +515,14 @@ def _fit(self, X, y, incremental=False):\n         # Run the LBFGS solver\n         elif self.solver == \"lbfgs\":\n             self._fit_lbfgs(\n-                X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n+                X,\n+                y,\n+                sample_weight,\n+                activations,\n+                deltas,\n+                coef_grads,\n+                intercept_grads,\n+                layer_units,\n             )\n \n         # validate parameter weights\n@@ -501,7 +536,15 @@ def _fit(self, X, y, incremental=False):\n         return self\n \n     def _fit_lbfgs(\n-        self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n+        self,\n+        X,\n+        y,\n+        sample_weight,\n+        activations,\n+        deltas,\n+        coef_grads,\n+        intercept_grads,\n+        layer_units,\n     ):\n         # Store meta information for the parameters\n         self._coef_indptr = []\n@@ -541,7 +584,15 @@ def _fit_lbfgs(\n                 \"iprint\": iprint,\n                 \"gtol\": self.tol,\n             },\n-            args=(X, y, activations, deltas, coef_grads, intercept_grads),\n+            args=(\n+                X,\n+                y,\n+                sample_weight,\n+                activations,\n+                deltas,\n+                coef_grads,\n+                intercept_grads,\n+            ),\n         )\n         self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n         self.loss_ = opt_res.fun\n@@ -551,6 +602,7 @@ def _fit_stochastic(\n         self,\n         X,\n         y,\n+        sample_weight,\n         activations,\n         deltas,\n         coef_grads,\n@@ -586,20 +638,39 @@ def _fit_stochastic(\n             # don't stratify in multilabel classification\n             should_stratify = is_classifier(self) and self.n_outputs_ == 1\n             stratify = y if should_stratify else None\n-            X, X_val, y, y_val = train_test_split(\n-                X,\n-                y,\n-                random_state=self._random_state,\n-                test_size=self.validation_fraction,\n-                stratify=stratify,\n-            )\n+            if sample_weight is None:\n+                X_train, X_val, y_train, y_val = train_test_split(\n+                    X,\n+                    y,\n+                    random_state=self._random_state,\n+                    test_size=self.validation_fraction,\n+                    stratify=stratify,\n+                )\n+                sample_weight_train = sample_weight_val = None\n+            else:\n+                # TODO: incorporate sample_weight in sampling here.\n+                (\n+                    X_train,\n+                    X_val,\n+                    y_train,\n+                    y_val,\n+                    sample_weight_train,\n+                    sample_weight_val,\n+                ) = train_test_split(\n+                    X,\n+                    y,\n+                    sample_weight,\n+                    random_state=self._random_state,\n+                    test_size=self.validation_fraction,\n+                    stratify=stratify,\n+                )\n             if is_classifier(self):\n                 y_val = self._label_binarizer.inverse_transform(y_val)\n         else:\n-            X_val = None\n-            y_val = None\n+            X_train, y_train, sample_weight_train = X, y, sample_weight\n+            X_val = y_val = sample_weight_val = None\n \n-        n_samples = X.shape[0]\n+        n_samples = X_train.shape[0]\n         sample_idx = np.arange(n_samples, dtype=int)\n \n         if self.batch_size == \"auto\":\n@@ -624,16 +695,22 @@ def _fit_stochastic(\n                 accumulated_loss = 0.0\n                 for batch_slice in gen_batches(n_samples, batch_size):\n                     if self.shuffle:\n-                        X_batch = _safe_indexing(X, sample_idx[batch_slice])\n-                        y_batch = y[sample_idx[batch_slice]]\n+                        batch_idx = sample_idx[batch_slice]\n+                        X_batch = _safe_indexing(X_train, batch_idx)\n+                    else:\n+                        batch_idx = batch_slice\n+                        X_batch = X_train[batch_idx]\n+                    y_batch = y_train[batch_idx]\n+                    if sample_weight is None:\n+                        sample_weight_batch = None\n                     else:\n-                        X_batch = X[batch_slice]\n-                        y_batch = y[batch_slice]\n+                        sample_weight_batch = sample_weight_train[batch_idx]\n \n                     activations[0] = X_batch\n                     batch_loss, coef_grads, intercept_grads = self._backprop(\n                         X_batch,\n                         y_batch,\n+                        sample_weight_batch,\n                         activations,\n                         deltas,\n                         coef_grads,\n@@ -648,7 +725,7 @@ def _fit_stochastic(\n                     self._optimizer.update_params(params, grads)\n \n                 self.n_iter_ += 1\n-                self.loss_ = accumulated_loss / X.shape[0]\n+                self.loss_ = accumulated_loss / X_train.shape[0]\n \n                 self.t_ += n_samples\n                 self.loss_curve_.append(self.loss_)\n@@ -657,7 +734,9 @@ def _fit_stochastic(\n \n                 # update no_improvement_count based on training loss or\n                 # validation score according to early_stopping\n-                self._update_no_improvement_count(early_stopping, X_val, y_val)\n+                self._update_no_improvement_count(\n+                    early_stopping, X_val, y_val, sample_weight_val\n+                )\n \n                 # for learning rate that needs to be updated at iteration end\n                 self._optimizer.iteration_ends(self.t_)\n@@ -702,10 +781,10 @@ def _fit_stochastic(\n             self.coefs_ = self._best_coefs\n             self.intercepts_ = self._best_intercepts\n \n-    def _update_no_improvement_count(self, early_stopping, X_val, y_val):\n+    def _update_no_improvement_count(self, early_stopping, X, y, sample_weight):\n         if early_stopping:\n             # compute validation score (can be NaN), use that for stopping\n-            val_score = self._score(X_val, y_val)\n+            val_score = self._score(X, y, sample_weight=sample_weight)\n \n             self.validation_scores_.append(val_score)\n \n@@ -734,7 +813,7 @@ def _update_no_improvement_count(self, early_stopping, X_val, y_val):\n                 self.best_loss_ = self.loss_curve_[-1]\n \n     @_fit_context(prefer_skip_nested_validation=True)\n-    def fit(self, X, y):\n+    def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit the model to data matrix X and target(s) y.\n \n         Parameters\n@@ -746,12 +825,17 @@ def fit(self, X, y):\n             The target values (class labels in classification, real numbers in\n             regression).\n \n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n+            .. versionadded:: 1.7\n+\n         Returns\n         -------\n         self : object\n             Returns a trained MLP model.\n         \"\"\"\n-        return self._fit(X, y, incremental=False)\n+        return self._fit(X, y, sample_weight=sample_weight, incremental=False)\n \n     def _check_solver(self):\n         if self.solver not in _STOCHASTIC_SOLVERS:\n@@ -761,7 +845,7 @@ def _check_solver(self):\n             )\n         return True\n \n-    def _score_with_function(self, X, y, score_function):\n+    def _score_with_function(self, X, y, sample_weight, score_function):\n         \"\"\"Private score method without input validation.\"\"\"\n         # Input validation would remove feature names, so we disable it\n         y_pred = self._predict(X, check_input=False)\n@@ -769,7 +853,7 @@ def _score_with_function(self, X, y, score_function):\n         if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n             return np.nan\n \n-        return score_function(y, y_pred)\n+        return score_function(y, y_pred, sample_weight=sample_weight)\n \n     def __sklearn_tags__(self):\n         tags = super().__sklearn_tags__()\n@@ -1190,12 +1274,14 @@ def _predict(self, X, check_input=True):\n \n         return self._label_binarizer.inverse_transform(y_pred)\n \n-    def _score(self, X, y):\n-        return super()._score_with_function(X, y, score_function=accuracy_score)\n+    def _score(self, X, y, sample_weight=None):\n+        return super()._score_with_function(\n+            X, y, sample_weight=sample_weight, score_function=accuracy_score\n+        )\n \n     @available_if(lambda est: est._check_solver())\n     @_fit_context(prefer_skip_nested_validation=True)\n-    def partial_fit(self, X, y, classes=None):\n+    def partial_fit(self, X, y, sample_weight=None, classes=None):\n         \"\"\"Update the model with a single iteration over the given data.\n \n         Parameters\n@@ -1206,6 +1292,11 @@ def partial_fit(self, X, y, classes=None):\n         y : array-like of shape (n_samples,)\n             The target values.\n \n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n+            .. versionadded:: 1.7\n+\n         classes : array of shape (n_classes,), default=None\n             Classes across all calls to partial_fit.\n             Can be obtained via `np.unique(y_all)`, where y_all is the\n@@ -1226,7 +1317,7 @@ def partial_fit(self, X, y, classes=None):\n             else:\n                 self._label_binarizer.fit(classes)\n \n-        return self._fit(X, y, incremental=True)\n+        return self._fit(X, y, sample_weight=sample_weight, incremental=True)\n \n     def predict_log_proba(self, X):\n         \"\"\"Return the log of probability estimates.\n@@ -1632,8 +1723,10 @@ def _predict(self, X, check_input=True):\n             return y_pred.ravel()\n         return y_pred\n \n-    def _score(self, X, y):\n-        return super()._score_with_function(X, y, score_function=r2_score)\n+    def _score(self, X, y, sample_weight=None):\n+        return super()._score_with_function(\n+            X, y, sample_weight=sample_weight, score_function=r2_score\n+        )\n \n     def _validate_input(self, X, y, incremental, reset):\n         X, y = validate_data(\n@@ -1652,7 +1745,7 @@ def _validate_input(self, X, y, incremental, reset):\n \n     @available_if(lambda est: est._check_solver)\n     @_fit_context(prefer_skip_nested_validation=True)\n-    def partial_fit(self, X, y):\n+    def partial_fit(self, X, y, sample_weight=None):\n         \"\"\"Update the model with a single iteration over the given data.\n \n         Parameters\n@@ -1663,9 +1756,14 @@ def partial_fit(self, X, y):\n         y : ndarray of shape (n_samples,)\n             The target values.\n \n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n+            .. versionadded:: 1.6\n+\n         Returns\n         -------\n         self : object\n             Trained MLP model.\n         \"\"\"\n-        return self._fit(X, y, incremental=True)\n+        return self._fit(X, y, sample_weight=sample_weight, incremental=True)\ndiff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py\nindex 969b452d687fd..bd0af1f06d011 100644\n--- a/sklearn/neural_network/tests/test_mlp.py\n+++ b/sklearn/neural_network/tests/test_mlp.py\n@@ -229,7 +229,7 @@ def test_gradient():\n             # analytically compute the gradients\n             def loss_grad_fun(t):\n                 return mlp._loss_grad_lbfgs(\n-                    t, X, Y, activations, deltas, coef_grads, intercept_grads\n+                    t, X, Y, None, activations, deltas, coef_grads, intercept_grads\n                 )\n \n             [value, grad] = loss_grad_fun(theta)\n@@ -1019,3 +1019,30 @@ def test_mlp_diverging_loss():\n     # In python, float(\"nan\") != float(\"nan\")\n     assert str(mlp.validation_scores_[-1]) == str(np.nan)\n     assert isinstance(mlp.validation_scores_[-1], float)\n+\n+\n+def test_mlp_sample_weight_with_early_stopping():\n+    # Test code path for inner validation set splitting.\n+    X, y = make_regression(\n+        n_samples=100,\n+        n_features=2,\n+        n_informative=2,\n+        random_state=42,\n+    )\n+    sw = np.ones_like(y)\n+    params = dict(\n+        hidden_layer_sizes=10,\n+        solver=\"adam\",\n+        early_stopping=True,\n+        tol=1e-2,\n+        learning_rate_init=0.01,\n+        batch_size=10,\n+        random_state=42,\n+    )\n+    m1 = MLPRegressor(\n+        **params,\n+    )\n+    m1.fit(X, y, sample_weight=sw)\n+\n+    m2 = MLPRegressor(**params).fit(X, y, sample_weight=None)\n+    assert_allclose(m1.predict(X), m2.predict(X))\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex c46213b417090..7f9cab13cf5b0 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -590,6 +590,16 @@\n         ],\n     },\n     MDS: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1, n_init=2)},\n+    MLPClassifier: {\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n+            dict(solver=\"lbfgs\"),\n+        ]\n+    },\n+    MLPRegressor: {\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n+            dict(solver=\"sgd\", tol=1e-2, random_state=42),\n+        ]\n+    },\n     MiniBatchDictionaryLearning: {\n         \"check_dict_unchanged\": dict(batch_size=10, max_iter=5, n_components=1)\n     },\n",
  "fail_to_pass": [
    "test_mlp_sample_weight_with_early_stopping"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/neural_network/_base.py",
    "sklearn/neural_network/_multilayer_perceptron.py",
    "sklearn/neural_network/tests/test_mlp.py",
    "sklearn/utils/_test_common/instance_generator.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-10-26T16:18:49Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30155",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/25646"
}