{
  "id": "scikit-learn__scikit-learn-31414",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "1fae098375f9670c5b472d5a3ca2d6bec5939b56",
  "issue_number": 31165,
  "issue_title": "FIX Use sample weight to draw samples in Bagging estimators",
  "issue_body": "Part of https://github.com/scikit-learn/scikit-learn/issues/16298. \r\n\r\n#### What does this implement/fix? Explain your changes.\r\n\r\nIn `Bagging` estimators, `sample_weight` is now used to draw the samples and are no longer forwarded to the underlying estimators. \r\n",
  "pr_number": 31414,
  "pr_title": "FIX Draw indices using sample_weight in Bagging",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst b/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst\nnew file mode 100644\nindex 0000000000000..6a881a3174850\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.ensemble/31414.fix.rst\n@@ -0,0 +1,7 @@\n+- :class:`ensemble.BaggingClassfier`, :class:`ensemble.BaggingRegressor`\n+  and :class:`ensemble.IsolationForest` now use `sample_weight` to draw\n+  the samples instead of forwarding them multiplied by a uniformly sampled\n+  mask to the underlying estimators. Furthermore, `max_samples` is now\n+  interpreted as a fraction of `sample_weight.sum()` instead of `X.shape[0]`\n+  when passed as a float.\n+  By :user:`Antoine Baker <antoinebaker>`.\ndiff --git a/sklearn/ensemble/_bagging.py b/sklearn/ensemble/_bagging.py\nindex 34b613b15281a..b727c7f233975 100644\n--- a/sklearn/ensemble/_bagging.py\n+++ b/sklearn/ensemble/_bagging.py\n@@ -72,6 +72,7 @@ def _generate_bagging_indices(\n     n_samples,\n     max_features,\n     max_samples,\n+    sample_weight,\n ):\n     \"\"\"Randomly draw feature and sample indices.\"\"\"\n     # Get valid random state\n@@ -81,18 +82,37 @@ def _generate_bagging_indices(\n     feature_indices = _generate_indices(\n         random_state, bootstrap_features, n_features, max_features\n     )\n-    sample_indices = _generate_indices(\n-        random_state, bootstrap_samples, n_samples, max_samples\n-    )\n+    if sample_weight is None:\n+        sample_indices = _generate_indices(\n+            random_state, bootstrap_samples, n_samples, max_samples\n+        )\n+    else:\n+        normalized_sample_weight = sample_weight / np.sum(sample_weight)\n+        sample_indices = random_state.choice(\n+            n_samples,\n+            max_samples,\n+            replace=bootstrap_samples,\n+            p=normalized_sample_weight,\n+        )\n \n     return feature_indices, sample_indices\n \n \n+def _consumes_sample_weight(estimator):\n+    if _routing_enabled():\n+        request_or_router = get_routing_for_object(estimator)\n+        consumes_sample_weight = request_or_router.consumes(\"fit\", (\"sample_weight\",))\n+    else:\n+        consumes_sample_weight = has_fit_parameter(estimator, \"sample_weight\")\n+    return consumes_sample_weight\n+\n+\n def _parallel_build_estimators(\n     n_estimators,\n     ensemble,\n     X,\n     y,\n+    sample_weight,\n     seeds,\n     total_n_estimators,\n     verbose,\n@@ -108,22 +128,12 @@ def _parallel_build_estimators(\n     bootstrap_features = ensemble.bootstrap_features\n     has_check_input = has_fit_parameter(ensemble.estimator_, \"check_input\")\n     requires_feature_indexing = bootstrap_features or max_features != n_features\n+    consumes_sample_weight = _consumes_sample_weight(ensemble.estimator_)\n \n     # Build estimators\n     estimators = []\n     estimators_features = []\n \n-    # TODO: (slep6) remove if condition for unrouted sample_weight when metadata\n-    # routing can't be disabled.\n-    support_sample_weight = has_fit_parameter(ensemble.estimator_, \"sample_weight\")\n-    if not _routing_enabled() and (\n-        not support_sample_weight and fit_params.get(\"sample_weight\") is not None\n-    ):\n-        raise ValueError(\n-            \"The base estimator doesn't support sample weight, but sample_weight is \"\n-            \"passed to the fit method.\"\n-        )\n-\n     for i in range(n_estimators):\n         if verbose > 1:\n             print(\n@@ -139,7 +149,8 @@ def _parallel_build_estimators(\n         else:\n             estimator_fit = estimator.fit\n \n-        # Draw random feature, sample indices\n+        # Draw random feature, sample indices (using normalized sample_weight\n+        # as probabilites if provided).\n         features, indices = _generate_bagging_indices(\n             random_state,\n             bootstrap_features,\n@@ -148,45 +159,22 @@ def _parallel_build_estimators(\n             n_samples,\n             max_features,\n             max_samples,\n+            sample_weight,\n         )\n \n         fit_params_ = fit_params.copy()\n \n-        # TODO(SLEP6): remove if condition for unrouted sample_weight when metadata\n-        # routing can't be disabled.\n-        # 1. If routing is enabled, we will check if the routing supports sample\n-        # weight and use it if it does.\n-        # 2. If routing is not enabled, we will check if the base\n-        # estimator supports sample_weight and use it if it does.\n-\n         # Note: Row sampling can be achieved either through setting sample_weight or\n-        # by indexing. The former is more efficient. Therefore, use this method\n+        # by indexing. The former is more memory efficient. Therefore, use this method\n         # if possible, otherwise use indexing.\n-        if _routing_enabled():\n-            request_or_router = get_routing_for_object(ensemble.estimator_)\n-            consumes_sample_weight = request_or_router.consumes(\n-                \"fit\", (\"sample_weight\",)\n-            )\n-        else:\n-            consumes_sample_weight = support_sample_weight\n         if consumes_sample_weight:\n-            # Draw sub samples, using sample weights, and then fit\n-            curr_sample_weight = _check_sample_weight(\n-                fit_params_.pop(\"sample_weight\", None), X\n-            ).copy()\n-\n-            if bootstrap:\n-                sample_counts = np.bincount(indices, minlength=n_samples)\n-                curr_sample_weight *= sample_counts\n-            else:\n-                not_indices_mask = ~indices_to_mask(indices, n_samples)\n-                curr_sample_weight[not_indices_mask] = 0\n-\n-            fit_params_[\"sample_weight\"] = curr_sample_weight\n+            # Row sampling by setting sample_weight\n+            indices_as_sample_weight = np.bincount(indices, minlength=n_samples)\n+            fit_params_[\"sample_weight\"] = indices_as_sample_weight\n             X_ = X[:, features] if requires_feature_indexing else X\n             estimator_fit(X_, y, **fit_params_)\n         else:\n-            # cannot use sample_weight, so use indexing\n+            # Row sampling by indexing\n             y_ = _safe_indexing(y, indices)\n             X_ = _safe_indexing(X, indices)\n             fit_params_ = _check_method_params(X, params=fit_params_, indices=indices)\n@@ -354,9 +342,11 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n             regression).\n \n         sample_weight : array-like of shape (n_samples,), default=None\n-            Sample weights. If None, then samples are equally weighted.\n-            Note that this is supported only if the base estimator supports\n-            sample weighting.\n+            Sample weights. If None, then samples are equally weighted. Used as\n+            probabilities to sample the training set. Note that the expected\n+            frequency semantics for the `sample_weight` parameter are only\n+            fulfilled when sampling with replacement `bootstrap=True`.\n+\n         **fit_params : dict\n             Parameters to pass to the underlying estimators.\n \n@@ -386,6 +376,15 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n             multi_output=True,\n         )\n \n+        if sample_weight is not None:\n+            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)\n+\n+            if not self.bootstrap:\n+                warn(\n+                    f\"When fitting {self.__class__.__name__} with sample_weight \"\n+                    f\"it is recommended to use bootstrap=True, got {self.bootstrap}.\"\n+                )\n+\n         return self._fit(\n             X,\n             y,\n@@ -435,8 +434,6 @@ def _fit(\n \n         sample_weight : array-like of shape (n_samples,), default=None\n             Sample weights. If None, then samples are equally weighted.\n-            Note that this is supported only if the base estimator supports\n-            sample weighting.\n \n         **fit_params : dict, default=None\n             Parameters to pass to the :term:`fit` method of the underlying\n@@ -457,18 +454,11 @@ def _fit(\n         # Check parameters\n         self._validate_estimator(self._get_estimator())\n \n-        if sample_weight is not None:\n-            fit_params[\"sample_weight\"] = sample_weight\n-\n         if _routing_enabled():\n             routed_params = process_routing(self, \"fit\", **fit_params)\n         else:\n             routed_params = Bunch()\n             routed_params.estimator = Bunch(fit=fit_params)\n-            if \"sample_weight\" in fit_params:\n-                routed_params.estimator.fit[\"sample_weight\"] = fit_params[\n-                    \"sample_weight\"\n-                ]\n \n         if max_depth is not None:\n             self.estimator_.max_depth = max_depth\n@@ -476,11 +466,26 @@ def _fit(\n         # Validate max_samples\n         if max_samples is None:\n             max_samples = self.max_samples\n-        elif not isinstance(max_samples, numbers.Integral):\n-            max_samples = int(max_samples * X.shape[0])\n \n-        if max_samples > X.shape[0]:\n-            raise ValueError(\"max_samples must be <= n_samples\")\n+        if not isinstance(max_samples, numbers.Integral):\n+            if sample_weight is None:\n+                max_samples = max(int(max_samples * X.shape[0]), 1)\n+            else:\n+                sw_sum = np.sum(sample_weight)\n+                if sw_sum <= 1:\n+                    raise ValueError(\n+                        f\"The total sum of sample weights is {sw_sum}, which prevents \"\n+                        \"resampling with a fractional value for max_samples=\"\n+                        f\"{max_samples}. Either pass max_samples as an integer or \"\n+                        \"use a larger sample_weight.\"\n+                    )\n+                max_samples = max(int(max_samples * sw_sum), 1)\n+\n+        if not self.bootstrap and max_samples > X.shape[0]:\n+            raise ValueError(\n+                f\"Effective max_samples={max_samples} must be <= n_samples=\"\n+                f\"{X.shape[0]} to be able to sample without replacement.\"\n+            )\n \n         # Store validated integer row sampling value\n         self._max_samples = max_samples\n@@ -499,6 +504,11 @@ def _fit(\n         # Store validated integer feature sampling value\n         self._max_features = max_features\n \n+        # Store sample_weight (needed in _get_estimators_indices). Note that\n+        # we intentionally do not materialize `sample_weight=None` as an array\n+        # of ones to avoid unnecessarily cluttering trained estimator pickles.\n+        self._sample_weight = sample_weight\n+\n         # Other checks\n         if not self.bootstrap and self.oob_score:\n             raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n@@ -552,6 +562,7 @@ def _fit(\n                 self,\n                 X,\n                 y,\n+                sample_weight,\n                 seeds[starts[i] : starts[i + 1]],\n                 total_n_estimators,\n                 verbose=self.verbose,\n@@ -596,6 +607,7 @@ def _get_estimators_indices(self):\n                 self._n_samples,\n                 self._max_features,\n                 self._max_samples,\n+                self._sample_weight,\n             )\n \n             yield feature_indices, sample_indices\n@@ -726,7 +738,8 @@ class BaggingClassifier(ClassifierMixin, BaseBagging):\n         replacement by default, see `bootstrap` for more details).\n \n         - If int, then draw `max_samples` samples.\n-        - If float, then draw `max_samples * X.shape[0]` samples.\n+        - If float, then draw `max_samples * X.shape[0]` unweighted samples\n+          or `max_samples * sample_weight.sum()` weighted samples.\n \n     max_features : int or float, default=1.0\n         The number of features to draw from X to train each base estimator (\n@@ -737,8 +750,10 @@ class BaggingClassifier(ClassifierMixin, BaseBagging):\n         - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n \n     bootstrap : bool, default=True\n-        Whether samples are drawn with replacement. If False, sampling\n-        without replacement is performed.\n+        Whether samples are drawn with replacement. If False, sampling without\n+        replacement is performed. If fitting with `sample_weight`, it is\n+        strongly recommended to choose True, as only drawing with replacement\n+        will ensure the expected frequency semantics of `sample_weight`.\n \n     bootstrap_features : bool, default=False\n         Whether features are drawn with replacement.\n@@ -1245,8 +1260,10 @@ class BaggingRegressor(RegressorMixin, BaseBagging):\n         - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n \n     bootstrap : bool, default=True\n-        Whether samples are drawn with replacement. If False, sampling\n-        without replacement is performed.\n+        Whether samples are drawn with replacement. If False, sampling without\n+        replacement is performed. If fitting with `sample_weight`, it is\n+        strongly recommended to choose True, as only drawing with replacement\n+        will ensure the expected frequency semantics of `sample_weight`.\n \n     bootstrap_features : bool, default=False\n         Whether features are drawn with replacement.\ndiff --git a/sklearn/ensemble/_iforest.py b/sklearn/ensemble/_iforest.py\nindex 4e5287af7f699..31c5491ccb6c9 100644\n--- a/sklearn/ensemble/_iforest.py\n+++ b/sklearn/ensemble/_iforest.py\n@@ -20,7 +20,12 @@\n from ..utils._chunking import get_chunk_n_rows\n from ..utils._param_validation import Interval, RealNotInt, StrOptions\n from ..utils.parallel import Parallel, delayed\n-from ..utils.validation import _num_samples, check_is_fitted, validate_data\n+from ..utils.validation import (\n+    _check_sample_weight,\n+    _num_samples,\n+    check_is_fitted,\n+    validate_data,\n+)\n from ._bagging import BaseBagging\n \n __all__ = [\"IsolationForest\"]\n@@ -317,6 +322,10 @@ def fit(self, X, y=None, sample_weight=None):\n         X = validate_data(\n             self, X, accept_sparse=[\"csc\"], dtype=tree_dtype, ensure_all_finite=False\n         )\n+\n+        if sample_weight is not None:\n+            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)\n+\n         if issparse(X):\n             # Pre-sort indices to avoid that each individual tree of the\n             # ensemble sorts the indices.\n@@ -350,7 +359,7 @@ def fit(self, X, y=None, sample_weight=None):\n         super()._fit(\n             X,\n             y,\n-            max_samples,\n+            max_samples=max_samples,\n             max_depth=max_depth,\n             sample_weight=sample_weight,\n             check_input=False,\ndiff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py\nindex 2cb9336bfd759..67fb5c763606f 100644\n--- a/sklearn/ensemble/tests/test_bagging.py\n+++ b/sklearn/ensemble/tests/test_bagging.py\n@@ -5,6 +5,7 @@\n # Authors: The scikit-learn developers\n # SPDX-License-Identifier: BSD-3-Clause\n \n+import re\n from itertools import cycle, product\n \n import joblib\n@@ -42,7 +43,11 @@\n )\n from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n from sklearn.utils import check_random_state\n-from sklearn.utils._testing import assert_array_almost_equal, assert_array_equal\n+from sklearn.utils._testing import (\n+    assert_allclose,\n+    assert_array_almost_equal,\n+    assert_array_equal,\n+)\n from sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS\n \n rng = check_random_state(0)\n@@ -589,28 +594,6 @@ def test_bagging_with_pipeline():\n     assert isinstance(estimator[0].steps[-1][1].random_state, int)\n \n \n-class DummyZeroEstimator(BaseEstimator):\n-    def fit(self, X, y):\n-        self.classes_ = np.unique(y)\n-        return self\n-\n-    def predict(self, X):\n-        return self.classes_[np.zeros(X.shape[0], dtype=int)]\n-\n-\n-def test_bagging_sample_weight_unsupported_but_passed():\n-    estimator = BaggingClassifier(DummyZeroEstimator())\n-    rng = check_random_state(0)\n-\n-    estimator.fit(iris.data, iris.target).predict(iris.data)\n-    with pytest.raises(ValueError):\n-        estimator.fit(\n-            iris.data,\n-            iris.target,\n-            sample_weight=rng.randint(10, size=(iris.data.shape[0])),\n-        )\n-\n-\n def test_warm_start(random_state=42):\n     # Test if fitting incrementally with warm start gives a forest of the\n     # right size and the same results as a normal fit.\n@@ -692,6 +675,138 @@ def test_warm_start_with_oob_score_fails():\n         clf.fit(X, y)\n \n \n+def test_warning_bootstrap_sample_weight():\n+    X, y = iris.data, iris.target\n+    sample_weight = np.ones_like(y)\n+    clf = BaggingClassifier(bootstrap=False)\n+    warn_msg = (\n+        \"When fitting BaggingClassifier with sample_weight \"\n+        \"it is recommended to use bootstrap=True\"\n+    )\n+    with pytest.warns(UserWarning, match=warn_msg):\n+        clf.fit(X, y, sample_weight=sample_weight)\n+\n+    X, y = diabetes.data, diabetes.target\n+    sample_weight = np.ones_like(y)\n+    reg = BaggingRegressor(bootstrap=False)\n+    warn_msg = (\n+        \"When fitting BaggingRegressor with sample_weight \"\n+        \"it is recommended to use bootstrap=True\"\n+    )\n+    with pytest.warns(UserWarning, match=warn_msg):\n+        reg.fit(X, y, sample_weight=sample_weight)\n+\n+\n+def test_invalid_sample_weight_max_samples_bootstrap_combinations():\n+    X, y = iris.data, iris.target\n+\n+    # Case 1: small weights and fractional max_samples would lead to sampling\n+    # less than 1 sample, which is not allowed.\n+    clf = BaggingClassifier(max_samples=1.0)\n+    sample_weight = np.ones_like(y) / (2 * len(y))\n+    expected_msg = (\n+        r\"The total sum of sample weights is 0.5(\\d*), which prevents resampling with \"\n+        r\"a fractional value for max_samples=1\\.0\\. Either pass max_samples as an \"\n+        r\"integer or use a larger sample_weight\\.\"\n+    )\n+    with pytest.raises(ValueError, match=expected_msg):\n+        clf.fit(X, y, sample_weight=sample_weight)\n+\n+    # Case 2: large weights and bootstrap=False would lead to sampling without\n+    # replacement more than the number of samples, which is not allowed.\n+    clf = BaggingClassifier(bootstrap=False, max_samples=1.0)\n+    sample_weight = np.ones_like(y)\n+    sample_weight[-1] = 2\n+    expected_msg = re.escape(\n+        \"max_samples=151 must be <= n_samples=150 to be able to sample without \"\n+        \"replacement.\"\n+    )\n+    with pytest.raises(ValueError, match=expected_msg):\n+        with pytest.warns(\n+            UserWarning, match=\"When fitting BaggingClassifier with sample_weight\"\n+        ):\n+            clf.fit(X, y, sample_weight=sample_weight)\n+\n+\n+class EstimatorAcceptingSampleWeight(BaseEstimator):\n+    \"\"\"Fake estimator accepting sample_weight\"\"\"\n+\n+    def fit(self, X, y, sample_weight=None):\n+        \"\"\"Record values passed during fit\"\"\"\n+        self.X_ = X\n+        self.y_ = y\n+        self.sample_weight_ = sample_weight\n+\n+    def predict(self, X):\n+        pass\n+\n+\n+class EstimatorRejectingSampleWeight(BaseEstimator):\n+    \"\"\"Fake estimator rejecting sample_weight\"\"\"\n+\n+    def fit(self, X, y):\n+        \"\"\"Record values passed during fit\"\"\"\n+        self.X_ = X\n+        self.y_ = y\n+\n+    def predict(self, X):\n+        pass\n+\n+\n+@pytest.mark.parametrize(\"bagging_class\", [BaggingRegressor, BaggingClassifier])\n+@pytest.mark.parametrize(\"accept_sample_weight\", [False, True])\n+@pytest.mark.parametrize(\"metadata_routing\", [False, True])\n+@pytest.mark.parametrize(\"max_samples\", [10, 0.8])\n+def test_draw_indices_using_sample_weight(\n+    bagging_class, accept_sample_weight, metadata_routing, max_samples\n+):\n+    X = np.arange(100).reshape(-1, 1)\n+    y = np.repeat([0, 1], 50)\n+    # all indices except 4 and 5 have zero weight\n+    sample_weight = np.zeros(100)\n+    sample_weight[4] = 1\n+    sample_weight[5] = 2\n+    if accept_sample_weight:\n+        base_estimator = EstimatorAcceptingSampleWeight()\n+    else:\n+        base_estimator = EstimatorRejectingSampleWeight()\n+\n+    n_samples, n_features = X.shape\n+\n+    if isinstance(max_samples, float):\n+        # max_samples passed as a fraction of the input data. Since\n+        # sample_weight are provided, the effective number of samples is the\n+        # sum of the sample weights.\n+        expected_integer_max_samples = int(max_samples * sample_weight.sum())\n+    else:\n+        expected_integer_max_samples = max_samples\n+\n+    with config_context(enable_metadata_routing=metadata_routing):\n+        # TODO(slep006): remove block when default routing is implemented\n+        if metadata_routing and accept_sample_weight:\n+            base_estimator = base_estimator.set_fit_request(sample_weight=True)\n+        bagging = bagging_class(base_estimator, max_samples=max_samples, n_estimators=4)\n+        bagging.fit(X, y, sample_weight=sample_weight)\n+        for estimator, samples in zip(bagging.estimators_, bagging.estimators_samples_):\n+            counts = np.bincount(samples, minlength=n_samples)\n+            assert sum(counts) == len(samples) == expected_integer_max_samples\n+            # only indices 4 and 5 should appear\n+            assert np.isin(samples, [4, 5]).all()\n+            if accept_sample_weight:\n+                # sampled indices represented through weighting\n+                assert estimator.X_.shape == (n_samples, n_features)\n+                assert estimator.y_.shape == (n_samples,)\n+                assert_allclose(estimator.X_, X)\n+                assert_allclose(estimator.y_, y)\n+                assert_allclose(estimator.sample_weight_, counts)\n+            else:\n+                # sampled indices represented through indexing\n+                assert estimator.X_.shape == (expected_integer_max_samples, n_features)\n+                assert estimator.y_.shape == (expected_integer_max_samples,)\n+                assert_allclose(estimator.X_, X[samples])\n+                assert_allclose(estimator.y_, y[samples])\n+\n+\n def test_oob_score_removed_on_warm_start():\n     X, y = make_hastie_10_2(n_samples=100, random_state=1)\n \ndiff --git a/sklearn/tests/test_metaestimators_metadata_routing.py b/sklearn/tests/test_metaestimators_metadata_routing.py\nindex f4ed228ec2f9d..2120c8a0c51f6 100644\n--- a/sklearn/tests/test_metaestimators_metadata_routing.py\n+++ b/sklearn/tests/test_metaestimators_metadata_routing.py\n@@ -330,7 +330,7 @@\n         \"y\": y,\n         \"preserves_metadata\": False,\n         \"estimator_routing_methods\": [\n-            \"fit\",\n+            (\"fit\", [\"metadata\"]),\n             \"predict\",\n             \"predict_proba\",\n             \"predict_log_proba\",\n@@ -349,7 +349,7 @@\n         \"X\": X,\n         \"y\": y,\n         \"preserves_metadata\": False,\n-        \"estimator_routing_methods\": [\"fit\", \"predict\"],\n+        \"estimator_routing_methods\": [(\"fit\", [\"metadata\"]), \"predict\"],\n     },\n     {\n         \"metaestimator\": RidgeCV,\n@@ -459,7 +459,13 @@\n - X: X-data to fit and predict\n - y: y-data to fit\n - estimator_routing_methods: list of all methods to check for routing metadata\n-  to the sub-estimator\n+  to the sub-estimator. Each value is either a str or a tuple:\n+    - str: the name of the method, all metadata in this method must be routed to the\n+      sub-estimator\n+    - tuple: the name of the method, the second element is a list of metadata keys\n+      to be passed to the sub-estimator. This is useful if certain metadata such as\n+      `sample_weight` are never routed and only consumed, such as in `BaggingClassifier`\n+      and `BaggingRegressor`.\n - preserves_metadata:\n     - True (default): the metaestimator passes the metadata to the\n       sub-estimator without modification. We check that the values recorded by\n@@ -562,6 +568,32 @@ def get_init_args(metaestimator_info, sub_estimator_consumes):\n     )\n \n \n+def filter_metadata_in_routing_methods(estimator_routing_methods):\n+    \"\"\"Process estimator_routing_methods and return a dict.\n+\n+    Parameters\n+    ----------\n+    estimator_routing_methods : list of str or tuple\n+        The estimator_routing_methods info from METAESTIMATORS.\n+\n+    Returns\n+    -------\n+    routing_methods : dict\n+        The dictionary is of the form {\"method\": [\"metadata\", ...]}.\n+        It specifies the list of metadata keys for each routing method.\n+        By default the list includes `sample_weight` and `metadata`.\n+    \"\"\"\n+    res = dict()\n+    for method_spec in estimator_routing_methods:\n+        if isinstance(method_spec, str):\n+            method = method_spec\n+            metadata = [\"sample_weight\", \"metadata\"]\n+        else:\n+            method, metadata = method_spec\n+        res[method] = metadata\n+    return res\n+\n+\n def set_requests(obj, *, method_mapping, methods, metadata_name, value=True):\n     \"\"\"Call `set_{method}_request` on a list of methods from the sub-estimator.\n \n@@ -662,10 +694,12 @@ def test_error_on_missing_requests_for_sub_estimator(metaestimator):\n     metaestimator_class = metaestimator[\"metaestimator\"]\n     X = metaestimator[\"X\"]\n     y = metaestimator[\"y\"]\n-    routing_methods = metaestimator[\"estimator_routing_methods\"]\n+    routing_methods = filter_metadata_in_routing_methods(\n+        metaestimator[\"estimator_routing_methods\"]\n+    )\n \n-    for method_name in routing_methods:\n-        for key in [\"sample_weight\", \"metadata\"]:\n+    for method_name, metadata_keys in routing_methods.items():\n+        for key in metadata_keys:\n             kwargs, (estimator, _), (scorer, _), *_ = get_init_args(\n                 metaestimator, sub_estimator_consumes=True\n             )\n@@ -721,12 +755,14 @@ def test_setting_request_on_sub_estimator_removes_error(metaestimator):\n     metaestimator_class = metaestimator[\"metaestimator\"]\n     X = metaestimator[\"X\"]\n     y = metaestimator[\"y\"]\n-    routing_methods = metaestimator[\"estimator_routing_methods\"]\n+    routing_methods = filter_metadata_in_routing_methods(\n+        metaestimator[\"estimator_routing_methods\"]\n+    )\n     method_mapping = metaestimator.get(\"method_mapping\", {})\n     preserves_metadata = metaestimator.get(\"preserves_metadata\", True)\n \n-    for method_name in routing_methods:\n-        for key in [\"sample_weight\", \"metadata\"]:\n+    for method_name, metadata_keys in routing_methods.items():\n+        for key in metadata_keys:\n             val = {\"sample_weight\": sample_weight, \"metadata\": metadata}[key]\n             method_kwargs = {key: val}\n \n@@ -797,8 +833,9 @@ def set_request(estimator, method_name):\n     metaestimator_class = metaestimator[\"metaestimator\"]\n     X = metaestimator[\"X\"]\n     y = metaestimator[\"y\"]\n-    routing_methods = metaestimator[\"estimator_routing_methods\"]\n-\n+    routing_methods = filter_metadata_in_routing_methods(\n+        metaestimator[\"estimator_routing_methods\"]\n+    )\n     for method_name in routing_methods:\n         kwargs, (estimator, _), (_, _), (_, _) = get_init_args(\n             metaestimator, sub_estimator_consumes=False\n",
  "fail_to_pass": [
    "test_warning_bootstrap_sample_weight",
    "test_invalid_sample_weight_max_samples_bootstrap_combinations",
    "test_draw_indices_using_sample_weight"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/ensemble/_bagging.py",
    "sklearn/ensemble/_iforest.py",
    "sklearn/ensemble/tests/test_bagging.py",
    "sklearn/tests/test_metaestimators_metadata_routing.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-05-22T15:34:05Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31414",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/31165"
}