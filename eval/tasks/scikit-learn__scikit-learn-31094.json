{
  "id": "scikit-learn__scikit-learn-31094",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "0cf0968c24fdf8c66f744fd0a91d7e72109f0dfa",
  "issue_number": 31093,
  "issue_title": "The covariance matrix is incorrect in BayesianRidge",
  "issue_body": "### Describe the bug\n\nThe posterior covariance matrix in `BayesianRidge`, attribute `sigma_`,  is incorrect when `n_features > n_samples`. This is because the posterior covariance requires the full svd, while the current code uses the reduced svd.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn import datasets\n\n# on main\nX, y = datasets.make_regression(n_samples=10, n_features=20)\nn_features = X.shape[1]\nreg = BayesianRidge(fit_intercept=False).fit(X, y)\ncovariance_matrix = np.linalg.inv(\n    reg.lambda_ * np.identity(n_features) + reg.alpha_ * np.dot(X.T, X)\n)\nnp.allclose(reg.sigma_, covariance_matrix)\n```\n\n### Expected Results\n\nTrue\n\n### Actual Results\n\nFalse\n\n### Versions\n\n```shell\n1.7.dev0\n```",
  "pr_number": 31094,
  "pr_title": "FIX Covariance matrix in BayesianRidge",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/31094.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/31094.fix.rst\nnew file mode 100644\nindex 0000000000000..b65d96bccd7d2\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/31094.fix.rst\n@@ -0,0 +1,3 @@\n+- :class:`linear_model.BayesianRidge` now uses the full SVD to correctly estimate\n+  the posterior covariance matrix `sigma_` when `n_samples < n_features`.\n+  By :user:`Antoine Baker <antoinebaker>`\ndiff --git a/sklearn/linear_model/_bayes.py b/sklearn/linear_model/_bayes.py\nindex 27ce01d0e75d5..adf515d44d1d9 100644\n--- a/sklearn/linear_model/_bayes.py\n+++ b/sklearn/linear_model/_bayes.py\n@@ -293,8 +293,19 @@ def fit(self, X, y, sample_weight=None):\n         coef_old_ = None\n \n         XT_y = np.dot(X.T, y)\n-        U, S, Vh = linalg.svd(X, full_matrices=False)\n+        # Let M, N = n_samples, n_features and K = min(M, N).\n+        # The posterior covariance matrix needs Vh_full: (N, N).\n+        # The full SVD is only required when n_samples < n_features.\n+        # When n_samples < n_features, K=M and full_matrices=True\n+        # U: (M, M), S: M, Vh_full: (N, N), Vh: (M, N)\n+        # When n_samples > n_features, K=N and full_matrices=False\n+        # U: (M, N), S: N, Vh_full: (N, N), Vh: (N, N)\n+        U, S, Vh_full = linalg.svd(X, full_matrices=(n_samples < n_features))\n+        K = len(S)\n         eigen_vals_ = S**2\n+        eigen_vals_full = np.zeros(n_features, dtype=dtype)\n+        eigen_vals_full[0:K] = eigen_vals_\n+        Vh = Vh_full[0:K, :]\n \n         # Convergence loop of the bayesian ridge regression\n         for iter_ in range(self.max_iter):\n@@ -353,11 +364,10 @@ def fit(self, X, y, sample_weight=None):\n             self.scores_.append(s)\n             self.scores_ = np.array(self.scores_)\n \n-        # posterior covariance is given by 1/alpha_ * scaled_sigma_\n-        scaled_sigma_ = np.dot(\n-            Vh.T, Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis]\n+        # posterior covariance\n+        self.sigma_ = np.dot(\n+            Vh_full.T, Vh_full / (alpha_ * eigen_vals_full + lambda_)[:, np.newaxis]\n         )\n-        self.sigma_ = (1.0 / alpha_) * scaled_sigma_\n \n         self._set_intercept(X_offset_, y_offset_, X_scale_)\n \ndiff --git a/sklearn/linear_model/tests/test_bayes.py b/sklearn/linear_model/tests/test_bayes.py\nindex 6fae1536582c8..9f7fabb749f52 100644\n--- a/sklearn/linear_model/tests/test_bayes.py\n+++ b/sklearn/linear_model/tests/test_bayes.py\n@@ -11,6 +11,7 @@\n from sklearn.utils import check_random_state\n from sklearn.utils._testing import (\n     _convert_container,\n+    assert_allclose,\n     assert_almost_equal,\n     assert_array_almost_equal,\n     assert_array_less,\n@@ -94,6 +95,22 @@ def test_bayesian_ridge_parameter():\n     assert_almost_equal(rr_model.intercept_, br_model.intercept_)\n \n \n+@pytest.mark.parametrize(\"n_samples, n_features\", [(10, 20), (20, 10)])\n+def test_bayesian_covariance_matrix(n_samples, n_features, global_random_seed):\n+    \"\"\"Check the posterior covariance matrix sigma_\n+\n+    Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/31093\n+    \"\"\"\n+    X, y = datasets.make_regression(\n+        n_samples, n_features, random_state=global_random_seed\n+    )\n+    reg = BayesianRidge(fit_intercept=False).fit(X, y)\n+    covariance_matrix = np.linalg.inv(\n+        reg.lambda_ * np.identity(n_features) + reg.alpha_ * np.dot(X.T, X)\n+    )\n+    assert_allclose(reg.sigma_, covariance_matrix, rtol=1e-6)\n+\n+\n def test_bayesian_sample_weights():\n     # Test correctness of the sample_weights method\n     X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n",
  "fail_to_pass": [
    "test_bayesian_covariance_matrix"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_bayes.py",
    "sklearn/linear_model/tests/test_bayes.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-03-27T16:14:18Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31094",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/31093"
}