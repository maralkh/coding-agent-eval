{
  "id": "scikit-learn__scikit-learn-32917",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "a4db9a9dc45bbd39e397ea4ebf9c5dcc6a658500",
  "issue_number": 32909,
  "issue_title": "FEA Add array API support for `average_precision_score`",
  "issue_body": "#### Reference Issues/PRs\r\ntowards #26024\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nAdds array API support to `average_precision_score`\r\n\r\n#### AI usage disclosure\r\n<!--\r\nIf AI tools were involved in creating this PR, please check all boxes that apply\r\nbelow and make sure that you adhere to our Automated Contributions Policy:\r\nhttps://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy\r\n-->\r\nI used AI assistance for:\r\n- [ ] Code generation (e.g., when writing an implementation or fixing a bug)\r\n- [ ] Test/benchmark generation\r\n- [ ] Documentation (including examples)\r\n- [x] Research and understanding\r\n\r\n\r\n#### Any other comments?\r\nWIP\r\n\r\nTODO:\r\n- [x] add array API in `average_precision_score`\r\n- [x] add common metrics test\r\n- [x] add average_precision_score to the array API docs\r\n- [x] removed old fix for numpy<= 1.24.1 from `average_precision_score` as well as from `_check_set_wise_labels` which allows to have `present_labels` as an array instead of a list\r\n- [x] use `LabelBinarizer` instead of `label_binarize` # TODO: check if in fact necessary\r\n- [x] add array API in `_average_binary_score`\r\n- [x] fix remaining test failures\r\n- [x] add changelog\r\n- [ ] make it accept mixed namespace inputs and add to new test in #32755 if it is merged first",
  "pr_number": 32917,
  "pr_title": "MNT cleanup old numpy workaround in metrics functions",
  "gold_patch": "diff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex 06db74153b776..81f8712af0232 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -39,7 +39,6 @@\n     _is_xp_namespace,\n     _isin,\n     _max_precision_float_dtype,\n-    _tolist,\n     _union1d,\n     get_namespace,\n     get_namespace_and_device,\n@@ -1862,9 +1861,7 @@ def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):\n \n     y_true, y_pred = attach_unique(y_true, y_pred)\n     y_type, y_true, y_pred, _ = _check_targets(y_true, y_pred)\n-    # Convert to Python primitive type to avoid NumPy type / Python str\n-    # comparison. See https://github.com/numpy/numpy/issues/6784\n-    present_labels = _tolist(unique_labels(y_true, y_pred))\n+    present_labels = unique_labels(y_true, y_pred)\n     if average == \"binary\":\n         if y_type == \"binary\":\n             if pos_label not in present_labels:\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex eeb88f8bb0d98..d1f8dab912acf 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -239,9 +239,7 @@ def _binary_uninterpolated_average_precision(\n \n     y_type = type_of_target(y_true, input_name=\"y_true\")\n \n-    # Convert to Python primitive type to avoid NumPy type / Python str\n-    # comparison. See https://github.com/numpy/numpy/issues/6784\n-    present_labels = np.unique(y_true).tolist()\n+    present_labels = np.unique(y_true)\n \n     if y_type == \"binary\":\n         if len(present_labels) == 2 and pos_label not in present_labels:\ndiff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\nindex 958dfa5fd86f6..6642fce16d64e 100644\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -1571,7 +1571,7 @@ def test_multilabel_hamming_loss():\n def test_jaccard_score_validation():\n     y_true = np.array([0, 1, 0, 1, 1])\n     y_pred = np.array([0, 1, 0, 1, 1])\n-    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"\n+    err_msg = re.escape(\"pos_label=2 is not a valid label. It should be one of [0 1]\")\n     with pytest.raises(ValueError, match=err_msg):\n         jaccard_score(y_true, y_pred, average=\"binary\", pos_label=2)\n \ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 6beb495af194b..b64d200a01522 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -1966,8 +1966,8 @@ def test_metrics_pos_label_error_str(metric, y_pred_threshold, dtype_y_str):\n         \"specified: either make y_true take value in {0, 1} or {-1, 1} or \"\n         \"pass pos_label explicit\"\n     )\n-    err_msg_pos_label_1 = (\n-        r\"pos_label=1 is not a valid label. It should be one of \\['eggs', 'spam'\\]\"\n+    err_msg_pos_label_1 = re.escape(\n+        \"pos_label=1 is not a valid label. It should be one of ['eggs' 'spam']\"\n     )\n \n     pos_label_default = signature(metric).parameters[\"pos_label\"].default\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex 0a108ecfb1cca..821537571ea1a 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -1191,7 +1191,7 @@ def test_average_precision_score_binary_pos_label_errors():\n     # Raise an error when pos_label is not in binary y_true\n     y_true = np.array([0, 1])\n     y_pred = np.array([0, 1])\n-    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"\n+    err_msg = re.escape(\"pos_label=2 is not a valid label. It should be one of [0 1]\")\n     with pytest.raises(ValueError, match=err_msg):\n         average_precision_score(y_true, y_pred, pos_label=2)\n \ndiff --git a/sklearn/utils/_array_api.py b/sklearn/utils/_array_api.py\nindex 9b6cc6d9774ba..8bcf8bde132ea 100644\n--- a/sklearn/utils/_array_api.py\n+++ b/sklearn/utils/_array_api.py\n@@ -1104,14 +1104,6 @@ def _bincount(array, weights=None, minlength=None, xp=None):\n     return xp.asarray(bin_out, device=device(array))\n \n \n-def _tolist(array, xp=None):\n-    xp, _ = get_namespace(array, xp=xp)\n-    if _is_numpy_namespace(xp):\n-        return array.tolist()\n-    array_np = _convert_to_numpy(array, xp=xp)\n-    return [element.item() for element in array_np]\n-\n-\n def _logsumexp(array, axis=None, xp=None):\n     # TODO replace by scipy.special.logsumexp when\n     # https://github.com/scipy/scipy/pull/22683 is part of a release.\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/_ranking.py",
    "sklearn/metrics/tests/test_classification.py",
    "sklearn/metrics/tests/test_common.py",
    "sklearn/metrics/tests/test_ranking.py",
    "sklearn/utils/_array_api.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-12-17T14:28:15Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32917",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/32909"
}