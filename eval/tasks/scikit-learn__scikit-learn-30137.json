{
  "id": "scikit-learn__scikit-learn-30137",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "2144b9045565fc1e663f97416a3f4cf0f4d23908",
  "issue_number": 30131,
  "issue_title": "LinearRegression on sparse matrices is not sample weight consistent",
  "issue_body": "Part of #16298.\r\n\r\n### Describe the bug\r\n\r\nWhen using a sparse container like `csr_array` for `X`, `LinearRegression` even fails to give the same coefficients for unit or no sample weight, and more generally fails the `test_linear_regression_sample_weight_consitency` checks. In that setting, the underlying solver is `scipy.sparse.linalg.lsqr`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.utils.fixes import csr_array\r\nfrom sklearn.datasets import make_regression\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.utils._testing import assert_allclose\r\n\r\nX, y = make_regression(100, 100, random_state=42)\r\nX = csr_array(X)\r\nreg = LinearRegression(fit_intercept=True)\r\nreg.fit(X, y)\r\ncoef1 = reg.coef_\r\nreg.fit(X, y, sample_weight=np.ones_like(y))\r\ncoef2 = reg.coef_\r\nassert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe `assert_allclose` should pass.\r\n\r\n### Actual Results\r\n\r\n```Python Traceback\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=1e-09\r\n\r\nMismatched elements: 100 / 100 (100%)\r\nMax absolute difference among violations: 0.00165048\r\nMax relative difference among violations: 0.02621317\r\n ACTUAL: array([-2.450778e-01,  2.917985e+01,  1.678916e+00,  7.534454e+01,\r\n        1.241587e+01,  1.076716e+00, -4.975206e-01, -9.262295e-01,\r\n       -1.373931e+00, -1.624112e-01, -8.644422e-01, -5.986218e-01,...\r\n DESIRED: array([-2.452359e-01,  2.918078e+01,  1.678681e+00,  7.534410e+01,\r\n        1.241459e+01,  1.076624e+00, -4.962305e-01, -9.257701e-01,\r\n       -1.373862e+00, -1.622824e-01, -8.652183e-01, -5.981715e-01,...\r\n```\r\n\r\nThe test also fails for `fit_intercept=False`. Note that this test and other sample weight consistency checks pass if we do not wrap `X` in a sparse container.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:32:50) [Clang 16.0.6 ]\r\nexecutable: /Users/abaker/miniforge3/envs/sklearn-dev/bin/python\r\n   machine: macOS-14.5-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 73.0.1\r\n        numpy: 2.1.0\r\n        scipy: 1.14.1\r\n       Cython: 3.0.11\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libopenblas.0.dylib\r\n        version: 0.3.27\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libomp.dylib\r\n        version: None\r\n```\r\n\r\nEDIT: discovered while working on #30040 for the case of dense inputs.",
  "pr_number": 30137,
  "pr_title": "Check sample weight equivalence on sparse data",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.utils/29818.api.rst b/doc/whats_new/upcoming_changes/sklearn.utils/29818.api.rst\nindex df30e3af6ee6e..e7a92f8c49b1e 100644\n--- a/doc/whats_new/upcoming_changes/sklearn.utils/29818.api.rst\n+++ b/doc/whats_new/upcoming_changes/sklearn.utils/29818.api.rst\n@@ -1,4 +1,7 @@\n-- :func:`check_estimators.check_sample_weights_invariance` replaced by\n-  :func:`check_estimators.check_sample_weight_equivalence` which uses\n-  integer (including zero) weights.\n+- `utils.estimator_checks.check_sample_weights_invariance`\n+  replaced by\n+  `utils.estimator_checks.check_sample_weight_equivalence_on_dense_data`\n+  which uses integer (including zero) weights and\n+  `utils.estimator_checks.check_sample_weight_equivalence_on_sparse_data`\n+  which does the same on sparse data.\n   By :user:`Antoine Baker <antoinebaker>`\ndiff --git a/doc/whats_new/upcoming_changes/sklearn.utils/30137.api.rst b/doc/whats_new/upcoming_changes/sklearn.utils/30137.api.rst\nnew file mode 100644\nindex 0000000000000..e7a92f8c49b1e\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.utils/30137.api.rst\n@@ -0,0 +1,7 @@\n+- `utils.estimator_checks.check_sample_weights_invariance`\n+  replaced by\n+  `utils.estimator_checks.check_sample_weight_equivalence_on_dense_data`\n+  which uses integer (including zero) weights and\n+  `utils.estimator_checks.check_sample_weight_equivalence_on_sparse_data`\n+  which does the same on sparse data.\n+  By :user:`Antoine Baker <antoinebaker>`\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex e74afd28a0dc3..49422947a0fe7 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -506,19 +506,30 @@\n     BisectingKMeans: {\"check_dict_unchanged\": dict(max_iter=5, n_clusters=1, n_init=2)},\n     CCA: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     DecisionTreeRegressor: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(criterion=\"squared_error\"),\n             dict(criterion=\"absolute_error\"),\n             dict(criterion=\"friedman_mse\"),\n             dict(criterion=\"poisson\"),\n-        ]\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(criterion=\"squared_error\"),\n+            dict(criterion=\"absolute_error\"),\n+            dict(criterion=\"friedman_mse\"),\n+            dict(criterion=\"poisson\"),\n+        ],\n     },\n     DecisionTreeClassifier: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(criterion=\"gini\"),\n             dict(criterion=\"log_loss\"),\n             dict(criterion=\"entropy\"),\n-        ]\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(criterion=\"gini\"),\n+            dict(criterion=\"log_loss\"),\n+            dict(criterion=\"entropy\"),\n+        ],\n     },\n     DictionaryLearning: {\n         \"check_dict_unchanged\": dict(\n@@ -529,10 +540,10 @@\n     FastICA: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     FeatureAgglomeration: {\"check_dict_unchanged\": dict(n_clusters=1)},\n     GammaRegressor: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"newton-cholesky\"),\n             dict(solver=\"lbfgs\"),\n-        ]\n+        ],\n     },\n     GaussianMixture: {\"check_dict_unchanged\": dict(max_iter=5, n_init=2)},\n     GaussianRandomProjection: {\"check_dict_unchanged\": dict(n_components=1)},\n@@ -547,12 +558,15 @@\n     LinearDiscriminantAnalysis: {\"check_dict_unchanged\": dict(n_components=1)},\n     LocallyLinearEmbedding: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     LogisticRegression: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"lbfgs\"),\n             dict(solver=\"liblinear\"),\n             dict(solver=\"newton-cg\"),\n             dict(solver=\"newton-cholesky\"),\n-        ]\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(solver=\"liblinear\"),\n+        ],\n     },\n     MDS: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1, n_init=2)},\n     MiniBatchDictionaryLearning: {\n@@ -579,38 +593,45 @@\n     PLSRegression: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     PLSSVD: {\"check_dict_unchanged\": dict(n_components=1)},\n     PoissonRegressor: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"newton-cholesky\"),\n             dict(solver=\"lbfgs\"),\n-        ]\n+        ],\n     },\n     PolynomialCountSketch: {\"check_dict_unchanged\": dict(n_components=1)},\n     QuantileRegressor: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(quantile=0.5),\n             dict(quantile=0.75),\n             dict(solver=\"highs-ds\"),\n             dict(solver=\"highs-ipm\"),\n-        ]\n+        ],\n     },\n     RBFSampler: {\"check_dict_unchanged\": dict(n_components=1)},\n     Ridge: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"svd\"),\n             dict(solver=\"cholesky\"),\n             dict(solver=\"sparse_cg\"),\n             dict(solver=\"lsqr\"),\n             dict(solver=\"lbfgs\", positive=True),\n-        ]\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(solver=\"sparse_cg\"),\n+            dict(solver=\"lsqr\"),\n+        ],\n     },\n     RidgeClassifier: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"svd\"),\n             dict(solver=\"cholesky\"),\n             dict(solver=\"sparse_cg\"),\n             dict(solver=\"lsqr\"),\n-            dict(solver=\"lbfgs\", positive=True),\n-        ]\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(solver=\"sparse_cg\"),\n+            dict(solver=\"lsqr\"),\n+        ],\n     },\n     SkewedChi2Sampler: {\"check_dict_unchanged\": dict(n_components=1)},\n     SparsePCA: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n@@ -623,13 +644,22 @@\n     },\n     SpectralCoclustering: {\"check_dict_unchanged\": dict(n_clusters=1, n_init=2)},\n     SpectralEmbedding: {\"check_dict_unchanged\": dict(eigen_tol=1e-05, n_components=1)},\n+    StandardScaler: {\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n+            dict(with_mean=True),\n+            dict(with_mean=False),\n+        ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [\n+            dict(with_mean=False),\n+        ],\n+    },\n     TSNE: {\"check_dict_unchanged\": dict(n_components=1, perplexity=2)},\n     TruncatedSVD: {\"check_dict_unchanged\": dict(n_components=1)},\n     TweedieRegressor: {\n-        \"check_sample_weight_equivalence\": [\n+        \"check_sample_weight_equivalence_on_dense_data\": [\n             dict(solver=\"newton-cholesky\"),\n             dict(solver=\"lbfgs\"),\n-        ]\n+        ],\n     },\n }\n \n@@ -741,31 +771,46 @@ def _yield_instances_for_check(check, estimator_orig):\n PER_ESTIMATOR_XFAIL_CHECKS = {\n     AdaBoostClassifier: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     AdaBoostRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     BaggingClassifier: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     BaggingRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     BayesianRidge: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -775,13 +820,19 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     BisectingKMeans: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     CategoricalNB: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -806,21 +857,30 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     FixedThresholdClassifier: {\n         \"check_classifiers_train\": \"Threshold at probability 0.5 does not hold\",\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n             \"Due to the cross-validation and sample ordering, removing a sample\"\n             \" is not strictly equal to putting is weight to zero. Specific unit\"\n             \" tests are added for TunedThresholdClassifierCV specifically.\"\n         ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n     },\n     GradientBoostingClassifier: {\n         # TODO: investigate failure see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     GradientBoostingRegressor: {\n         # TODO: investigate failure see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -852,34 +912,51 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     HistGradientBoostingClassifier: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     HistGradientBoostingRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     IsolationForest: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     KBinsDiscretizer: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     KernelDensity: {\n-        \"check_sample_weight_equivalence\": \"sample_weight must have positive values\"\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight must have positive values\"\n+        ),\n     },\n     KMeans: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -894,13 +971,19 @@ def _yield_instances_for_check(check, estimator_orig):\n         # running the equivalence check even if n_features > n_samples. Maybe\n         # this is is not the case and a different choice of solver could fix\n         # this problem.\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     LinearSVC: {\n         # TODO: replace by a statistical test when _dual=True, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n         \"check_non_transformer_estimators_n_iter\": (\n@@ -909,19 +992,28 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     LinearSVR: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     LogisticRegression: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     MiniBatchKMeans: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -930,7 +1022,10 @@ def _yield_instances_for_check(check, estimator_orig):\n         # TODO: fix sample_weight handling of this estimator when probability=False\n         # TODO: replace by a statistical test when probability=True\n         # see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n         \"check_classifiers_one_label_sample_weights\": (\n@@ -939,7 +1034,10 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     NuSVR: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -950,13 +1048,19 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     OneClassSVM: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     Perceptron: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -975,13 +1079,19 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     RandomForestClassifier: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     RandomForestRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -991,13 +1101,19 @@ def _yield_instances_for_check(check, estimator_orig):\n     },\n     RandomTreesEmbedding: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     RANSACRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -1012,28 +1128,40 @@ def _yield_instances_for_check(check, estimator_orig):\n         )\n     },\n     RidgeCV: {\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n             \"GridSearchCV does not forward the weights to the scorer by default.\"\n         ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n     },\n     SelfTrainingClassifier: {\n         \"check_non_transformer_estimators_n_iter\": \"n_iter_ can be 0.\"\n     },\n     SGDClassifier: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     SGDOneClassSVM: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     SGDRegressor: {\n         # TODO: replace by a statistical test, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n@@ -1070,19 +1198,25 @@ def _yield_instances_for_check(check, estimator_orig):\n         # TODO: fix sample_weight handling of this estimator when probability=False\n         # TODO: replace by a statistical test when probability=True\n         # see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     SVR: {\n         # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n+            \"sample_weight is not equivalent to removing/repeating samples.\"\n+        ),\n+        \"check_sample_weight_equivalence_on_sparse_data\": (\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n     TunedThresholdClassifierCV: {\n         \"check_classifiers_train\": \"Threshold at probability 0.5 does not hold\",\n-        \"check_sample_weight_equivalence\": (\n+        \"check_sample_weight_equivalence_on_dense_data\": (\n             \"Due to the cross-validation and sample ordering, removing a sample\"\n             \" is not strictly equal to putting is weight to zero. Specific unit\"\n             \" tests are added for TunedThresholdClassifierCV specifically.\"\ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\nindex abf272e955bc2..6bb6524974a3a 100644\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -163,7 +163,11 @@ def _yield_checks(estimator):\n             # We skip pairwise because the data is not pairwise\n             yield check_sample_weights_shape\n             yield check_sample_weights_not_overwritten\n-            yield check_sample_weight_equivalence\n+            yield check_sample_weight_equivalence_on_dense_data\n+            # FIXME: filter on tags.input_tags.sparse\n+            # (estimator accepts sparse arrays)\n+            # once issue #30139 is fixed.\n+            yield check_sample_weight_equivalence_on_sparse_data\n \n     # Check that all estimator yield informative messages when\n     # trained on empty datasets\n@@ -1407,7 +1411,7 @@ def check_sample_weights_shape(name, estimator_orig):\n \n \n @ignore_warnings(category=FutureWarning)\n-def check_sample_weight_equivalence(name, estimator_orig):\n+def _check_sample_weight_equivalence(name, estimator_orig, sparse_container):\n     # check that setting sample_weight to zero / integer is equivalent\n     # to removing / repeating corresponding samples.\n     estimator_weighted = clone(estimator_orig)\n@@ -1422,13 +1426,13 @@ def check_sample_weight_equivalence(name, estimator_orig):\n     # Use random integers (including zero) as weights.\n     sw = rng.randint(0, 5, size=n_samples)\n \n-    X_weigthed = X\n+    X_weighted = X\n     y_weighted = y\n     # repeat samples according to weights\n-    X_repeated = X_weigthed.repeat(repeats=sw, axis=0)\n+    X_repeated = X_weighted.repeat(repeats=sw, axis=0)\n     y_repeated = y_weighted.repeat(repeats=sw)\n \n-    X_weigthed, y_weighted, sw = shuffle(X_weigthed, y_weighted, sw, random_state=0)\n+    X_weighted, y_weighted, sw = shuffle(X_weighted, y_weighted, sw, random_state=0)\n \n     # when the estimator has an internal CV scheme\n     # we only use weights / repetitions in a specific CV group (here group=0)\n@@ -1437,10 +1441,10 @@ def check_sample_weight_equivalence(name, estimator_orig):\n             [np.full_like(y_weighted, 0), np.full_like(y, 1), np.full_like(y, 2)]\n         )\n         sw = np.hstack([sw, np.ones_like(y), np.ones_like(y)])\n-        X_weigthed = np.vstack([X_weigthed, X, X])\n+        X_weighted = np.vstack([X_weighted, X, X])\n         y_weighted = np.hstack([y_weighted, y, y])\n         splits_weighted = list(\n-            LeaveOneGroupOut().split(X_weigthed, groups=groups_weighted)\n+            LeaveOneGroupOut().split(X_weighted, groups=groups_weighted)\n         )\n         estimator_weighted.set_params(cv=splits_weighted)\n \n@@ -1457,8 +1461,13 @@ def check_sample_weight_equivalence(name, estimator_orig):\n     y_weighted = _enforce_estimator_tags_y(estimator_weighted, y_weighted)\n     y_repeated = _enforce_estimator_tags_y(estimator_repeated, y_repeated)\n \n+    # convert to sparse X if needed\n+    if sparse_container is not None:\n+        X_weighted = sparse_container(X_weighted)\n+        X_repeated = sparse_container(X_repeated)\n+\n     estimator_repeated.fit(X_repeated, y=y_repeated, sample_weight=None)\n-    estimator_weighted.fit(X_weigthed, y=y_weighted, sample_weight=sw)\n+    estimator_weighted.fit(X_weighted, y=y_weighted, sample_weight=sw)\n \n     for method in [\"predict_proba\", \"decision_function\", \"predict\", \"transform\"]:\n         if hasattr(estimator_orig, method):\n@@ -1472,6 +1481,22 @@ def check_sample_weight_equivalence(name, estimator_orig):\n             assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n \n \n+def check_sample_weight_equivalence_on_dense_data(name, estimator_orig):\n+    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n+\n+\n+def check_sample_weight_equivalence_on_sparse_data(name, estimator_orig):\n+    if SPARSE_ARRAY_PRESENT:\n+        sparse_container = sparse.csr_array\n+    else:\n+        sparse_container = sparse.csr_matrix\n+    # FIXME: remove the catch once issue #30139 is fixed.\n+    try:\n+        _check_sample_weight_equivalence(name, estimator_orig, sparse_container)\n+    except TypeError:\n+        return\n+\n+\n def check_sample_weights_not_overwritten(name, estimator_orig):\n     # check that estimators don't override the passed sample_weight parameter\n     estimator = clone(estimator_orig)\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/utils/_test_common/instance_generator.py",
    "sklearn/utils/estimator_checks.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-10-23T10:16:54Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30137",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30131"
}