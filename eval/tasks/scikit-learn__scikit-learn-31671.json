{
  "id": "scikit-learn__scikit-learn-31671",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "de3816631818fa905ba14a26c2fa721aa91ffa09",
  "issue_number": 26024,
  "issue_title": "Make more of the \"tools\" of scikit-learn Array API compatible",
  "issue_body": "\ud83d\udea8 \ud83d\udea7 This issue requires a bit of patience and experience to contribute to \ud83d\udea7 \ud83d\udea8 \r\n\r\n- Original issue introducing array API in scikit-learn: #22352\r\n- array API official doc/spec: https://data-apis.org/array-api/\r\n- scikit-learn doc: https://scikit-learn.org/dev/modules/array_api.html\r\n\r\nPlease mention this issue when you create a PR, but please don't write \"closes #26024\" or \"fixes #26024\".\r\n\r\nscikit-learn contains lots of useful tools, in addition to the many estimators it has. For example [metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics), [pipelines](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline), [pre-processing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) and [mode selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). These are useful to and used by people who do not necessarily use an estimator from scikit-learn. This is great.\r\n\r\nThe fact that many users install scikit-learn \"just\" to use `train_test_split` is a testament to how useful it is to provide easy to use tools that do the right(!) thing. Instead of everyone implementing them from scratch because it is \"easy\" and making mistakes along the way.\r\n\r\nIn this issue I'd like to collect and track work related to making it easier to use all these \"tools\" from scikit-learn even if you are not using Numpy arrays for your data. In particular thanks to the Array API standard it should be \"not too much work\" to make things usable with data that is in an array that conforms to the Array API standard.\r\n\r\nThere is work in #25956 and #22554 which adds the basic infrastructure needed to use \"array API arrays\".\r\n\r\nThe goal of this issue is to make code like the following work:\r\n```python\r\n>>> from sklearn.preprocessing import MinMaxScaler\r\n>>> from sklearn import config_context\r\n>>> from sklearn.datasets import make_classification\r\n>>> import torch\r\n>>> X_np, y_np = make_classification(random_state=0)\r\n>>> X_torch = torch.asarray(X_np, device=\"cuda\", dtype=torch.float32)\r\n>>> y_torch = torch.asarray(y_np, device=\"cuda\", dtype=torch.float32)\r\n\r\n>>> with config_context(array_api_dispatch=True):\r\n...     # For example using MinMaxScaler on PyTorch tensors\r\n...     scale = MinMaxScaler()\r\n...     X_trans = scale.fit_transform(X_torch, y_torch)\r\n...     assert type(X_trans) == type(X_torch)\r\n...     assert X_trans.device == X_torch.device\r\n```\r\n\r\nThe first step (or maybe part of the first) is to check which of them already \"just work\". After that is done we can start the work (one PR per class/function) making changes.\r\n\r\n\r\n## Guidelines for testing\r\n\r\nGeneral comment: most of the time when we add array API support to a function in scikit-learn, we do not touch the existing (numpy-only) tests to make sure that the PR does not change the default behavior of scikit-learn on traditional inputs when array API is not enabled.\r\n\r\nIn the case of an estimator, it can be enough to add the `array_api_support=True` estimator tag in a method named `__sklearn_tags__`. For metric functions, just register it in the `array_api_metric_checkers` in `sklearn/metrics/tests/test_common.py` to include it in the common test.\r\n\r\nFor other kinds of functions not covered by existing common tests, or when the array API support depends heavily on non-default values, it might be required to add one or more new test functions to the related module-level test file. The general testing scheme is the following:\r\n\r\n- generate some random test data with numpy or `sklearn.datasets.make_*`;\r\n- call the function once on the numpy inputs without enabling array API dispatch;\r\n- convert the inputs to a namespace / device combo passed as parameter to the test;\r\n- call the function with array API dispatching enabled (under a `with sklearn.config_context(array_api_dispatch=True)` block\r\n- check that the results are on the same namespace and device as the input\r\n- convert back the output to a numpy array using `_convert_to_numpy`\r\n- compare the original / reference numpy results and the `xp` computation results converted back to numpy using `assert_allclose` or similar.\r\n\r\nThose tests should have `array_api` somewhere in their name to makes sure that we can run all the array API compliance tests with a keyword search in the pytest command line, e.g.:\r\n\r\n```\r\npytest -k array_api sklearn/some/subpackage\r\n```\r\n\r\nIn particular, for cost reasons, our CUDA GPU CI only runs `pytest -k array_api sklearn`. So it's very important to respect this naming conventions, otherwise we will not tests all what we are supposed to test on CUDA.\r\n\r\nMore generally, look at merged array API pull requests to see how testing is typically handled.",
  "pr_number": 31671,
  "pr_title": "ENH add Array API support for `d2_pinball_score` and `d2_absolute_error_score`",
  "gold_patch": "diff --git a/doc/modules/array_api.rst b/doc/modules/array_api.rst\nindex 7771cc92f338c..b3cf4dd7476f0 100644\n--- a/doc/modules/array_api.rst\n+++ b/doc/modules/array_api.rst\n@@ -164,8 +164,10 @@ Metrics\n - :func:`sklearn.metrics.cluster.calinski_harabasz_score`\n - :func:`sklearn.metrics.cohen_kappa_score`\n - :func:`sklearn.metrics.confusion_matrix`\n+- :func:`sklearn.metrics.d2_absolute_error_score`\n - :func:`sklearn.metrics.d2_brier_score`\n - :func:`sklearn.metrics.d2_log_loss_score`\n+- :func:`sklearn.metrics.d2_pinball_score`\n - :func:`sklearn.metrics.d2_tweedie_score`\n - :func:`sklearn.metrics.det_curve`\n - :func:`sklearn.metrics.explained_variance_score`\ndiff --git a/doc/whats_new/upcoming_changes/array-api/31671.feature.rst b/doc/whats_new/upcoming_changes/array-api/31671.feature.rst\nnew file mode 100644\nindex 0000000000000..f9d6a6aecb0b0\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/array-api/31671.feature.rst\n@@ -0,0 +1,3 @@\n+- :func:`sklearn.metrics.d2_absolute_error_score` and\n+  :func:`sklearn.metrics.d2_pinball_score` now support array API compatible inputs.\n+  By :user:`Virgil Chan <virchan>`.\ndiff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/31671.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/31671.fix.rst\nnew file mode 100644\nindex 0000000000000..9bfcd7827bedd\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/31671.fix.rst\n@@ -0,0 +1,8 @@\n+- :func:`metrics.d2_pinball_score` and :func:`metrics.d2_absolute_error_score` now\n+  always use the `\"averaged_inverted_cdf\"` quantile method, both with and\n+  without sample weights. Previously, the `\"linear\"` quantile method was used only\n+  for the unweighted case leading the surprising discrepancies when comparing the\n+  results with unit weights. Note that all quantile interpolation methods are\n+  asymptotically equivalent in the large sample limit, but this fix can cause score\n+  value changes on small evaluation sets (without weights).\n+  By :user:`Virgil Chan <virchan>`.\ndiff --git a/sklearn/metrics/_regression.py b/sklearn/metrics/_regression.py\nindex 955014484fc5d..855912ca2d4a4 100644\n--- a/sklearn/metrics/_regression.py\n+++ b/sklearn/metrics/_regression.py\n@@ -936,7 +936,7 @@ def median_absolute_error(\n     return float(_average(output_errors, weights=multioutput, xp=xp))\n \n \n-def _assemble_r2_explained_variance(\n+def _assemble_fraction_of_explained_deviance(\n     numerator, denominator, n_outputs, multioutput, force_finite, xp, device\n ):\n     \"\"\"Common part used by explained variance score and :math:`R^2` score.\"\"\"\n@@ -1121,7 +1121,7 @@ def explained_variance_score(\n         (y_true - y_true_avg) ** 2, weights=sample_weight, axis=0, xp=xp\n     )\n \n-    return _assemble_r2_explained_variance(\n+    return _assemble_fraction_of_explained_deviance(\n         numerator=numerator,\n         denominator=denominator,\n         n_outputs=y_true.shape[1],\n@@ -1300,7 +1300,7 @@ def r2_score(\n         axis=0,\n     )\n \n-    return _assemble_r2_explained_variance(\n+    return _assemble_fraction_of_explained_deviance(\n         numerator=numerator,\n         denominator=denominator,\n         n_outputs=y_true.shape[1],\n@@ -1779,9 +1779,9 @@ def d2_pinball_score(\n     >>> d2_pinball_score(y_true, y_pred)\n     0.5\n     >>> d2_pinball_score(y_true, y_pred, alpha=0.9)\n-    0.772...\n+    0.666...\n     >>> d2_pinball_score(y_true, y_pred, alpha=0.1)\n-    -1.045...\n+    -1.999...\n     >>> d2_pinball_score(y_true, y_true, alpha=0.1)\n     1.0\n \n@@ -1803,9 +1803,14 @@ def d2_pinball_score(\n     >>> grid.best_params_\n     {'fit_intercept': True}\n     \"\"\"\n-    _, y_true, y_pred, sample_weight, multioutput = _check_reg_targets(\n+    xp, _, device_ = get_namespace_and_device(\n         y_true, y_pred, sample_weight, multioutput\n     )\n+    _, y_true, y_pred, sample_weight, multioutput = (\n+        _check_reg_targets_with_floating_dtype(\n+            y_true, y_pred, sample_weight, multioutput, xp=xp\n+        )\n+    )\n \n     if _num_samples(y_pred) < 2:\n         msg = \"D^2 score is not well-defined with less than two samples.\"\n@@ -1821,16 +1826,18 @@ def d2_pinball_score(\n     )\n \n     if sample_weight is None:\n-        y_quantile = np.tile(\n-            np.percentile(y_true, q=alpha * 100, axis=0), (len(y_true), 1)\n-        )\n-    else:\n-        y_quantile = np.tile(\n-            _weighted_percentile(\n-                y_true, sample_weight=sample_weight, percentile_rank=alpha * 100\n-            ),\n-            (len(y_true), 1),\n-        )\n+        sample_weight = xp.ones([y_true.shape[0]], dtype=y_true.dtype, device=device_)\n+\n+    y_quantile = xp.tile(\n+        _weighted_percentile(\n+            y_true,\n+            sample_weight=sample_weight,\n+            percentile_rank=alpha * 100,\n+            average=True,\n+            xp=xp,\n+        ),\n+        (y_true.shape[0], 1),\n+    )\n \n     denominator = mean_pinball_loss(\n         y_true,\n@@ -1840,25 +1847,15 @@ def d2_pinball_score(\n         multioutput=\"raw_values\",\n     )\n \n-    nonzero_numerator = numerator != 0\n-    nonzero_denominator = denominator != 0\n-    valid_score = nonzero_numerator & nonzero_denominator\n-    output_scores = np.ones(y_true.shape[1])\n-\n-    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n-    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.0\n-\n-    if isinstance(multioutput, str):\n-        if multioutput == \"raw_values\":\n-            # return scores individually\n-            return output_scores\n-        else:  # multioutput == \"uniform_average\"\n-            # passing None as weights to np.average results in uniform mean\n-            avg_weights = None\n-    else:\n-        avg_weights = multioutput\n-\n-    return float(np.average(output_scores, weights=avg_weights))\n+    return _assemble_fraction_of_explained_deviance(\n+        numerator=numerator,\n+        denominator=denominator,\n+        n_outputs=y_true.shape[1],\n+        multioutput=multioutput,\n+        force_finite=True,\n+        xp=xp,\n+        device=device_,\n+    )\n \n \n @validate_params(\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 34bfbc8b26252..6007a279eedb5 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -148,6 +148,11 @@\n     \"mean_compound_poisson_deviance\": partial(mean_tweedie_deviance, power=1.4),\n     \"d2_tweedie_score\": partial(d2_tweedie_score, power=1.4),\n     \"d2_pinball_score\": d2_pinball_score,\n+    # The default `alpha=0.5` (median) masks differences between quantile methods,\n+    # so we also test `alpha=0.1` and `alpha=0.9` to ensure correctness\n+    # for non-median quantiles.\n+    \"d2_pinball_score_01\": partial(d2_pinball_score, alpha=0.1),\n+    \"d2_pinball_score_09\": partial(d2_pinball_score, alpha=0.9),\n     \"d2_absolute_error_score\": d2_absolute_error_score,\n }\n \n@@ -492,6 +497,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"mean_absolute_percentage_error\",\n     \"mean_pinball_loss\",\n     \"d2_pinball_score\",\n+    \"d2_pinball_score_01\",\n+    \"d2_pinball_score_09\",\n     \"d2_absolute_error_score\",\n }\n \n@@ -563,6 +570,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"mean_compound_poisson_deviance\",\n     \"d2_tweedie_score\",\n     \"d2_pinball_score\",\n+    \"d2_pinball_score_01\",\n+    \"d2_pinball_score_09\",\n     \"d2_absolute_error_score\",\n     \"mean_absolute_percentage_error\",\n }\n@@ -2358,6 +2367,22 @@ def check_array_api_metric_pairwise(metric, array_namespace, device, dtype_name)\n         check_array_api_regression_metric,\n         check_array_api_regression_metric_multioutput,\n     ],\n+    d2_absolute_error_score: [\n+        check_array_api_regression_metric,\n+        check_array_api_regression_metric_multioutput,\n+    ],\n+    d2_pinball_score: [\n+        check_array_api_regression_metric,\n+        check_array_api_regression_metric_multioutput,\n+    ],\n+    partial(d2_pinball_score, alpha=0.1): [\n+        check_array_api_regression_metric,\n+        check_array_api_regression_metric_multioutput,\n+    ],\n+    partial(d2_pinball_score, alpha=0.9): [\n+        check_array_api_regression_metric,\n+        check_array_api_regression_metric_multioutput,\n+    ],\n     d2_tweedie_score: [\n         check_array_api_regression_metric,\n     ],\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_regression.py",
    "sklearn/metrics/tests/test_common.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-06-28T05:04:27Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31671",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/26024"
}