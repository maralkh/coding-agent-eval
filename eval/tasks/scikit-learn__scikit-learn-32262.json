{
  "id": "scikit-learn__scikit-learn-32262",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "b4da3a8ad8a4743fdf139ca16fd3f7435b77471d",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 32262,
  "pr_title": "MNT Clean-up deprecations for 1.8: algorithm param in AdaBoostClassifier",
  "gold_patch": "diff --git a/sklearn/ensemble/_weight_boosting.py b/sklearn/ensemble/_weight_boosting.py\nindex 4fb07d6a9fef4..c734746036457 100644\n--- a/sklearn/ensemble/_weight_boosting.py\n+++ b/sklearn/ensemble/_weight_boosting.py\n@@ -36,7 +36,7 @@\n from sklearn.metrics import accuracy_score, r2_score\n from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n from sklearn.utils import _safe_indexing, check_random_state\n-from sklearn.utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n+from sklearn.utils._param_validation import HasMethods, Interval, StrOptions\n from sklearn.utils.extmath import softmax\n from sklearn.utils.metadata_routing import (\n     _raise_for_unsupported_routing,\n@@ -318,27 +318,6 @@ def __sklearn_tags__(self):\n         return tags\n \n \n-def _samme_proba(estimator, n_classes, X):\n-    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n-\n-    References\n-    ----------\n-    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n-\n-    \"\"\"\n-    proba = estimator.predict_proba(X)\n-\n-    # Displace zero probabilities so the log is defined.\n-    # Also fix negative elements which may occur with\n-    # negative sample weights.\n-    np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n-    log_proba = np.log(proba)\n-\n-    return (n_classes - 1) * (\n-        log_proba - (1.0 / n_classes) * log_proba.sum(axis=1)[:, np.newaxis]\n-    )\n-\n-\n class AdaBoostClassifier(\n     _RoutingNotSupportedMixin, ClassifierMixin, BaseWeightBoosting\n ):\n@@ -379,13 +358,6 @@ class AdaBoostClassifier(\n         a trade-off between the `learning_rate` and `n_estimators` parameters.\n         Values must be in the range `(0.0, inf)`.\n \n-    algorithm : {'SAMME'}, default='SAMME'\n-        Use the SAMME discrete boosting algorithm.\n-\n-        .. deprecated:: 1.6\n-            `algorithm` is deprecated and will be removed in version 1.8. This\n-            estimator only implements the 'SAMME' algorithm.\n-\n     random_state : int, RandomState instance or None, default=None\n         Controls the random seed given at each `estimator` at each\n         boosting iteration.\n@@ -487,19 +459,12 @@ class AdaBoostClassifier(\n     refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n     \"\"\"\n \n-    # TODO(1.8): remove \"algorithm\" entry\n-    _parameter_constraints: dict = {\n-        **BaseWeightBoosting._parameter_constraints,\n-        \"algorithm\": [StrOptions({\"SAMME\"}), Hidden(StrOptions({\"deprecated\"}))],\n-    }\n-\n     def __init__(\n         self,\n         estimator=None,\n         *,\n         n_estimators=50,\n         learning_rate=1.0,\n-        algorithm=\"deprecated\",\n         random_state=None,\n     ):\n         super().__init__(\n@@ -509,19 +474,10 @@ def __init__(\n             random_state=random_state,\n         )\n \n-        self.algorithm = algorithm\n-\n     def _validate_estimator(self):\n         \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n         super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))\n \n-        if self.algorithm != \"deprecated\":\n-            warnings.warn(\n-                \"The parameter 'algorithm' is deprecated in 1.6 and has no effect. \"\n-                \"It will be removed in version 1.8.\",\n-                FutureWarning,\n-            )\n-\n         if not has_fit_parameter(self.estimator_, \"sample_weight\"):\n             raise ValueError(\n                 f\"{self.estimator.__class__.__name__} doesn't support sample_weight.\"\ndiff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py\nindex 55825c438d76b..2a430cbf9aec9 100644\n--- a/sklearn/ensemble/tests/test_weight_boosting.py\n+++ b/sklearn/ensemble/tests/test_weight_boosting.py\n@@ -9,7 +9,6 @@\n from sklearn.base import BaseEstimator, clone\n from sklearn.dummy import DummyClassifier, DummyRegressor\n from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n-from sklearn.ensemble._weight_boosting import _samme_proba\n from sklearn.linear_model import LinearRegression\n from sklearn.model_selection import GridSearchCV, train_test_split\n from sklearn.svm import SVC, SVR\n@@ -52,35 +51,6 @@\n )\n \n \n-def test_samme_proba():\n-    # Test the `_samme_proba` helper function.\n-\n-    # Define some example (bad) `predict_proba` output.\n-    probs = np.array(\n-        [[1, 1e-6, 0], [0.19, 0.6, 0.2], [-999, 0.51, 0.5], [1e-6, 1, 1e-9]]\n-    )\n-    probs /= np.abs(probs.sum(axis=1))[:, np.newaxis]\n-\n-    # _samme_proba calls estimator.predict_proba.\n-    # Make a mock object so I can control what gets returned.\n-    class MockEstimator:\n-        def predict_proba(self, X):\n-            assert_array_equal(X.shape, probs.shape)\n-            return probs\n-\n-    mock = MockEstimator()\n-\n-    samme_proba = _samme_proba(mock, 3, np.ones_like(probs))\n-\n-    assert_array_equal(samme_proba.shape, probs.shape)\n-    assert np.isfinite(samme_proba).all()\n-\n-    # Make sure that the correct elements come out as smallest --\n-    # `_samme_proba` should preserve the ordering in each example.\n-    assert_array_equal(np.argmin(samme_proba, axis=1), [2, 0, 0, 2])\n-    assert_array_equal(np.argmax(samme_proba, axis=1), [0, 1, 1, 1])\n-\n-\n def test_oneclass_adaboost_proba():\n     # Test predict_proba robustness for one class label input.\n     # In response to issue #7501\n@@ -630,10 +600,3 @@ def test_adaboost_decision_function(global_random_seed):\n \n     for y_score in clf.staged_decision_function(X):\n         assert_allclose(y_score.sum(axis=1), 0, atol=1e-8)\n-\n-\n-# TODO(1.8): remove\n-def test_deprecated_algorithm():\n-    adaboost_clf = AdaBoostClassifier(n_estimators=1, algorithm=\"SAMME\")\n-    with pytest.warns(FutureWarning, match=\"The parameter 'algorithm' is deprecated\"):\n-        adaboost_clf.fit(X, y_class)\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/ensemble/_weight_boosting.py",
    "sklearn/ensemble/tests/test_weight_boosting.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-09-24T09:27:03Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32262",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}