{
  "id": "scikit-learn__scikit-learn-31065",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "812ff67e6725a8ca207a37f5ed4bfeafc5d1265d",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 31065,
  "pr_title": "FIX Fix adjusted_mutual_info_score numerical issue",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/31065.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/31065.fix.rst\nnew file mode 100644\nindex 0000000000000..82126da7852cc\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/31065.fix.rst\n@@ -0,0 +1,3 @@\n+- Fix :func:`metrics.adjusted_mutual_info_score` numerical issue when number of\n+  classes and samples is low.\n+  By :user:`Hleb Levitski <glevv>`\ndiff --git a/sklearn/metrics/cluster/_supervised.py b/sklearn/metrics/cluster/_supervised.py\nindex 0f56513abca8e..bb903b70749dd 100644\n--- a/sklearn/metrics/cluster/_supervised.py\n+++ b/sklearn/metrics/cluster/_supervised.py\n@@ -1033,6 +1033,9 @@ def adjusted_mutual_info_score(\n         or classes.shape[0] == clusters.shape[0] == 0\n     ):\n         return 1.0\n+    # if there is only one class or one cluster return 0.0.\n+    elif classes.shape[0] == 1 or clusters.shape[0] == 1:\n+        return 0.0\n \n     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n     # Calculate the MI for the two clusterings\n@@ -1051,8 +1054,13 @@ def adjusted_mutual_info_score(\n         denominator = min(denominator, -np.finfo(\"float64\").eps)\n     else:\n         denominator = max(denominator, np.finfo(\"float64\").eps)\n-    ami = (mi - emi) / denominator\n-    return float(ami)\n+    # The same applies analogously to mi and emi.\n+    numerator = mi - emi\n+    if numerator < 0:\n+        numerator = min(numerator, -np.finfo(\"float64\").eps)\n+    else:\n+        numerator = max(numerator, np.finfo(\"float64\").eps)\n+    return float(numerator / denominator)\n \n \n @validate_params(\ndiff --git a/sklearn/metrics/cluster/tests/test_supervised.py b/sklearn/metrics/cluster/tests/test_supervised.py\nindex 417ae3ea4897f..6c68c0a85f698 100644\n--- a/sklearn/metrics/cluster/tests/test_supervised.py\n+++ b/sklearn/metrics/cluster/tests/test_supervised.py\n@@ -40,21 +40,19 @@\n ]\n \n \n-def test_error_messages_on_wrong_input():\n-    for score_func in score_funcs:\n-        expected = (\n-            r\"Found input variables with inconsistent numbers of samples: \\[2, 3\\]\"\n-        )\n-        with pytest.raises(ValueError, match=expected):\n-            score_func([0, 1], [1, 1, 1])\n+@pytest.mark.parametrize(\"score_func\", score_funcs)\n+def test_error_messages_on_wrong_input(score_func):\n+    expected = r\"Found input variables with inconsistent numbers of samples: \\[2, 3\\]\"\n+    with pytest.raises(ValueError, match=expected):\n+        score_func([0, 1], [1, 1, 1])\n \n-        expected = r\"labels_true must be 1D: shape is \\(2\"\n-        with pytest.raises(ValueError, match=expected):\n-            score_func([[0, 1], [1, 0]], [1, 1, 1])\n+    expected = r\"labels_true must be 1D: shape is \\(2\"\n+    with pytest.raises(ValueError, match=expected):\n+        score_func([[0, 1], [1, 0]], [1, 1, 1])\n \n-        expected = r\"labels_pred must be 1D: shape is \\(2\"\n-        with pytest.raises(ValueError, match=expected):\n-            score_func([0, 1, 0], [[1, 1], [0, 0]])\n+    expected = r\"labels_pred must be 1D: shape is \\(2\"\n+    with pytest.raises(ValueError, match=expected):\n+        score_func([0, 1, 0], [[1, 1], [0, 0]])\n \n \n def test_generalized_average():\n@@ -67,39 +65,50 @@ def test_generalized_average():\n     assert means[0] == means[1] == means[2] == means[3]\n \n \n-def test_perfect_matches():\n-    for score_func in score_funcs:\n-        assert score_func([], []) == pytest.approx(1.0)\n-        assert score_func([0], [1]) == pytest.approx(1.0)\n-        assert score_func([0, 0, 0], [0, 0, 0]) == pytest.approx(1.0)\n-        assert score_func([0, 1, 0], [42, 7, 42]) == pytest.approx(1.0)\n-        assert score_func([0.0, 1.0, 0.0], [42.0, 7.0, 42.0]) == pytest.approx(1.0)\n-        assert score_func([0.0, 1.0, 2.0], [42.0, 7.0, 2.0]) == pytest.approx(1.0)\n-        assert score_func([0, 1, 2], [42, 7, 2]) == pytest.approx(1.0)\n-    score_funcs_with_changing_means = [\n+@pytest.mark.parametrize(\"score_func\", score_funcs)\n+def test_perfect_matches(score_func):\n+    assert score_func([], []) == pytest.approx(1.0)\n+    assert score_func([0], [1]) == pytest.approx(1.0)\n+    assert score_func([0, 0, 0], [0, 0, 0]) == pytest.approx(1.0)\n+    assert score_func([0, 1, 0], [42, 7, 42]) == pytest.approx(1.0)\n+    assert score_func([0.0, 1.0, 0.0], [42.0, 7.0, 42.0]) == pytest.approx(1.0)\n+    assert score_func([0.0, 1.0, 2.0], [42.0, 7.0, 2.0]) == pytest.approx(1.0)\n+    assert score_func([0, 1, 2], [42, 7, 2]) == pytest.approx(1.0)\n+\n+\n+@pytest.mark.parametrize(\n+    \"score_func\",\n+    [\n         normalized_mutual_info_score,\n         adjusted_mutual_info_score,\n-    ]\n-    means = {\"min\", \"geometric\", \"arithmetic\", \"max\"}\n-    for score_func in score_funcs_with_changing_means:\n-        for mean in means:\n-            assert score_func([], [], average_method=mean) == pytest.approx(1.0)\n-            assert score_func([0], [1], average_method=mean) == pytest.approx(1.0)\n-            assert score_func(\n-                [0, 0, 0], [0, 0, 0], average_method=mean\n-            ) == pytest.approx(1.0)\n-            assert score_func(\n-                [0, 1, 0], [42, 7, 42], average_method=mean\n-            ) == pytest.approx(1.0)\n-            assert score_func(\n-                [0.0, 1.0, 0.0], [42.0, 7.0, 42.0], average_method=mean\n-            ) == pytest.approx(1.0)\n-            assert score_func(\n-                [0.0, 1.0, 2.0], [42.0, 7.0, 2.0], average_method=mean\n-            ) == pytest.approx(1.0)\n-            assert score_func(\n-                [0, 1, 2], [42, 7, 2], average_method=mean\n-            ) == pytest.approx(1.0)\n+    ],\n+)\n+@pytest.mark.parametrize(\"average_method\", [\"min\", \"geometric\", \"arithmetic\", \"max\"])\n+def test_perfect_matches_with_changing_means(score_func, average_method):\n+    assert score_func([], [], average_method=average_method) == pytest.approx(1.0)\n+    assert score_func([0], [1], average_method=average_method) == pytest.approx(1.0)\n+    assert score_func(\n+        [0, 0, 0], [0, 0, 0], average_method=average_method\n+    ) == pytest.approx(1.0)\n+    assert score_func(\n+        [0, 1, 0], [42, 7, 42], average_method=average_method\n+    ) == pytest.approx(1.0)\n+    assert score_func(\n+        [0.0, 1.0, 0.0], [42.0, 7.0, 42.0], average_method=average_method\n+    ) == pytest.approx(1.0)\n+    assert score_func(\n+        [0.0, 1.0, 2.0], [42.0, 7.0, 2.0], average_method=average_method\n+    ) == pytest.approx(1.0)\n+    assert score_func(\n+        [0, 1, 2], [42, 7, 2], average_method=average_method\n+    ) == pytest.approx(1.0)\n+    # Non-regression tests for: https://github.com/scikit-learn/scikit-learn/issues/30950\n+    assert score_func([0, 1], [0, 1], average_method=average_method) == pytest.approx(\n+        1.0\n+    )\n+    assert score_func(\n+        [0, 1, 2, 3], [0, 1, 2, 3], average_method=average_method\n+    ) == pytest.approx(1.0)\n \n \n def test_homogeneous_but_not_complete_labeling():\n@@ -306,12 +315,13 @@ def test_exactly_zero_info_score():\n         labels_a, labels_b = (np.ones(i, dtype=int), np.arange(i, dtype=int))\n         assert normalized_mutual_info_score(labels_a, labels_b) == pytest.approx(0.0)\n         assert v_measure_score(labels_a, labels_b) == pytest.approx(0.0)\n-        assert adjusted_mutual_info_score(labels_a, labels_b) == pytest.approx(0.0)\n+        assert adjusted_mutual_info_score(labels_a, labels_b) == 0.0\n         assert normalized_mutual_info_score(labels_a, labels_b) == pytest.approx(0.0)\n         for method in [\"min\", \"geometric\", \"arithmetic\", \"max\"]:\n-            assert adjusted_mutual_info_score(\n-                labels_a, labels_b, average_method=method\n-            ) == pytest.approx(0.0)\n+            assert (\n+                adjusted_mutual_info_score(labels_a, labels_b, average_method=method)\n+                == 0.0\n+            )\n             assert normalized_mutual_info_score(\n                 labels_a, labels_b, average_method=method\n             ) == pytest.approx(0.0)\n",
  "fail_to_pass": [
    "test_error_messages_on_wrong_input",
    "test_perfect_matches",
    "test_perfect_matches_with_changing_means"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/cluster/_supervised.py",
    "sklearn/metrics/cluster/tests/test_supervised.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-03-24T21:27:12Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31065",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}