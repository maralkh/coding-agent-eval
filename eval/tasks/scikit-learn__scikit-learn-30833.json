{
  "id": "scikit-learn__scikit-learn-30833",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "0b43f728b65a14a74a53bcf0a9692efc163993a3",
  "issue_number": 30808,
  "issue_title": "Add metadata routing params support in the predict method of `BaggingClassifier/Regressor`",
  "issue_body": "### Describe the workflow you want to enable\n\nHello! I'm trying to use metadata routing with `BaggingClassifier` and `BaggingRegressor` however it is implemented for the `fit` method, not the `predict` one. I am wondering if there is a particular reason for not doing it on the predict function or if this is a feature that could be added. This would enable situations like the following, which currently gives an error:\n\n```python\nimport numpy as np\nimport sklearn\nfrom sklearn import ensemble\nfrom sklearn.base import BaseEstimator\n\nsklearn.set_config(enable_metadata_routing=True)\n\n\nclass CustomEstimator(BaseEstimator):\n    def fit(self, X, y, foo):\n        return self\n\n    def predict(self, X, bar):\n        return np.zeros(X.shape[0])\n\n\nestimator = CustomEstimator()\nestimator.set_fit_request(foo=True)\nestimator.set_predict_request(bar=True)\nmodel = ensemble.BaggingRegressor(estimator)\n\nn, p = 10, 2\nrng = np.random.default_rng(0)\nx = rng.random((n, p))\ny = rng.integers(0, 2, n)\n\nmodel.fit(x, y, foo=True)\nmodel.predict(x, bar=True). # TypeError: BaggingRegressor.predict() got an unexpected keyword argument 'bar'\n\n```\n\n### Describe your proposed solution\n\nSimilar to the fit method, something like:\n```python\nif _routing_enabled():\n    routed_params = process_routing(self, \"predict\", **predict_params)\n```\n\nHowever, I don't have enough understanding of the metadata routing implementation to know exactly what should be done.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nI tried to have a look at the history of PRs/Issues to find a discussion around this point, but could not find it in the PR introducing the metadata routing to these estimators (#28432).",
  "pr_number": 30833,
  "pr_title": "FEA Add metadata routing through predict methods of BaggingClassifier and BaggingRegressor",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/metadata-routing/30833.feature.rst b/doc/whats_new/upcoming_changes/metadata-routing/30833.feature.rst\nnew file mode 100644\nindex 0000000000000..e46420e9ee2d2\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/metadata-routing/30833.feature.rst\n@@ -0,0 +1,4 @@\n+- :class:`ensemble.BaggingClassifier` and :class:`ensemble.BaggingRegressor` now support\n+  metadata routing through their `predict`, `predict_proba`, `predict_log_proba` and\n+  `decision_function` methods and pass `**params` to the underlying estimators.\n+  By :user:`Stefanie Senger <StefanieSenger>`.\ndiff --git a/sklearn/ensemble/_bagging.py b/sklearn/ensemble/_bagging.py\nindex 20013e1f6d000..901c63c9250bc 100644\n--- a/sklearn/ensemble/_bagging.py\n+++ b/sklearn/ensemble/_bagging.py\n@@ -202,14 +202,23 @@ def _parallel_build_estimators(\n     return estimators, estimators_features\n \n \n-def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n+def _parallel_predict_proba(\n+    estimators,\n+    estimators_features,\n+    X,\n+    n_classes,\n+    predict_params=None,\n+    predict_proba_params=None,\n+):\n     \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n     n_samples = X.shape[0]\n     proba = np.zeros((n_samples, n_classes))\n \n     for estimator, features in zip(estimators, estimators_features):\n         if hasattr(estimator, \"predict_proba\"):\n-            proba_estimator = estimator.predict_proba(X[:, features])\n+            proba_estimator = estimator.predict_proba(\n+                X[:, features], **(predict_params or {})\n+            )\n \n             if n_classes == len(estimator.classes_):\n                 proba += proba_estimator\n@@ -221,7 +230,9 @@ def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n \n         else:\n             # Resort to voting\n-            predictions = estimator.predict(X[:, features])\n+            predictions = estimator.predict(\n+                X[:, features], **(predict_proba_params or {})\n+            )\n \n             for i in range(n_samples):\n                 proba[i, predictions[i]] += 1\n@@ -229,7 +240,7 @@ def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n     return proba\n \n \n-def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n+def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes, params):\n     \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n     n_samples = X.shape[0]\n     log_proba = np.empty((n_samples, n_classes))\n@@ -237,7 +248,7 @@ def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n     all_classes = np.arange(n_classes, dtype=int)\n \n     for estimator, features in zip(estimators, estimators_features):\n-        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n+        log_proba_estimator = estimator.predict_log_proba(X[:, features], **params)\n \n         if n_classes == len(estimator.classes_):\n             log_proba = np.logaddexp(log_proba, log_proba_estimator)\n@@ -254,18 +265,18 @@ def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n     return log_proba\n \n \n-def _parallel_decision_function(estimators, estimators_features, X):\n+def _parallel_decision_function(estimators, estimators_features, X, params):\n     \"\"\"Private function used to compute decisions within a job.\"\"\"\n     return sum(\n-        estimator.decision_function(X[:, features])\n+        estimator.decision_function(X[:, features], **params)\n         for estimator, features in zip(estimators, estimators_features)\n     )\n \n \n-def _parallel_predict_regression(estimators, estimators_features, X):\n+def _parallel_predict_regression(estimators, estimators_features, X, params):\n     \"\"\"Private function used to compute predictions within a job.\"\"\"\n     return sum(\n-        estimator.predict(X[:, features])\n+        estimator.predict(X[:, features], **params)\n         for estimator, features in zip(estimators, estimators_features)\n     )\n \n@@ -615,10 +626,47 @@ def get_metadata_routing(self):\n             routing information.\n         \"\"\"\n         router = MetadataRouter(owner=self.__class__.__name__)\n-        router.add(\n-            estimator=self._get_estimator(),\n-            method_mapping=MethodMapping().add(callee=\"fit\", caller=\"fit\"),\n+\n+        method_mapping = MethodMapping()\n+        method_mapping.add(caller=\"fit\", callee=\"fit\").add(\n+            caller=\"decision_function\", callee=\"decision_function\"\n         )\n+\n+        # the router needs to be built depending on whether the sub-estimator has a\n+        # `predict_proba` method (as BaggingClassifier decides dynamically at runtime):\n+        if hasattr(self._get_estimator(), \"predict_proba\"):\n+            (\n+                method_mapping.add(caller=\"predict\", callee=\"predict_proba\").add(\n+                    caller=\"predict_proba\", callee=\"predict_proba\"\n+                )\n+            )\n+\n+        else:\n+            (\n+                method_mapping.add(caller=\"predict\", callee=\"predict\").add(\n+                    caller=\"predict_proba\", callee=\"predict\"\n+                )\n+            )\n+\n+        # the router needs to be built depending on whether the sub-estimator has a\n+        # `predict_log_proba` method (as BaggingClassifier decides dynamically at\n+        # runtime):\n+        if hasattr(self._get_estimator(), \"predict_log_proba\"):\n+            method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_log_proba\")\n+\n+        else:\n+            # if `predict_log_proba` is not available in BaggingClassifier's\n+            # sub-estimator, the routing should go to its `predict_proba` if it is\n+            # available or else to its `predict` method; according to how\n+            # `sample_weight` is passed to the respective methods dynamically at\n+            # runtime:\n+            if hasattr(self._get_estimator(), \"predict_proba\"):\n+                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict_proba\")\n+\n+            else:\n+                method_mapping.add(caller=\"predict_log_proba\", callee=\"predict\")\n+\n+        router.add(estimator=self._get_estimator(), method_mapping=method_mapping)\n         return router\n \n     @abstractmethod\n@@ -882,7 +930,7 @@ def _validate_y(self, y):\n \n         return y\n \n-    def predict(self, X):\n+    def predict(self, X, **params):\n         \"\"\"Predict class for X.\n \n         The predicted class of an input sample is computed as the class with\n@@ -895,15 +943,28 @@ def predict(self, X):\n             The training input samples. Sparse matrices are accepted only if\n             they are supported by the base estimator.\n \n+        **params : dict\n+            Parameters routed to the `predict_proba` (if available) or the `predict`\n+            method (otherwise) of the sub-estimators via the metadata routing API.\n+\n+            .. versionadded:: 1.7\n+\n+                Only available if\n+                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n+                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n+                details.\n+\n         Returns\n         -------\n         y : ndarray of shape (n_samples,)\n             The predicted classes.\n         \"\"\"\n-        predicted_probabilitiy = self.predict_proba(X)\n+        _raise_for_params(params, self, \"predict\")\n+\n+        predicted_probabilitiy = self.predict_proba(X, **params)\n         return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)), axis=0)\n \n-    def predict_proba(self, X):\n+    def predict_proba(self, X, **params):\n         \"\"\"Predict class probabilities for X.\n \n         The predicted class probabilities of an input sample is computed as\n@@ -919,12 +980,25 @@ def predict_proba(self, X):\n             The training input samples. Sparse matrices are accepted only if\n             they are supported by the base estimator.\n \n+        **params : dict\n+            Parameters routed to the `predict_proba` (if available) or the `predict`\n+            method (otherwise) of the sub-estimators via the metadata routing API.\n+\n+            .. versionadded:: 1.7\n+\n+                Only available if\n+                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n+                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n+                details.\n+\n         Returns\n         -------\n         p : ndarray of shape (n_samples, n_classes)\n             The class probabilities of the input samples. The order of the\n             classes corresponds to that in the attribute :term:`classes_`.\n         \"\"\"\n+        _raise_for_params(params, self, \"predict_proba\")\n+\n         check_is_fitted(self)\n         # Check data\n         X = validate_data(\n@@ -936,6 +1010,12 @@ def predict_proba(self, X):\n             reset=False,\n         )\n \n+        if _routing_enabled():\n+            routed_params = process_routing(self, \"predict_proba\", **params)\n+        else:\n+            routed_params = Bunch()\n+            routed_params.estimator = Bunch(predict_proba=Bunch())\n+\n         # Parallel loop\n         n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n \n@@ -947,6 +1027,8 @@ def predict_proba(self, X):\n                 self.estimators_features_[starts[i] : starts[i + 1]],\n                 X,\n                 self.n_classes_,\n+                predict_params=routed_params.estimator.get(\"predict\", None),\n+                predict_proba_params=routed_params.estimator.get(\"predict_proba\", None),\n             )\n             for i in range(n_jobs)\n         )\n@@ -956,7 +1038,7 @@ def predict_proba(self, X):\n \n         return proba\n \n-    def predict_log_proba(self, X):\n+    def predict_log_proba(self, X, **params):\n         \"\"\"Predict class log-probabilities for X.\n \n         The predicted class log-probabilities of an input sample is computed as\n@@ -969,13 +1051,29 @@ def predict_log_proba(self, X):\n             The training input samples. Sparse matrices are accepted only if\n             they are supported by the base estimator.\n \n+        **params : dict\n+            Parameters routed to the `predict_log_proba`, the `predict_proba` or the\n+            `proba` method of the sub-estimators via the metadata routing API. The\n+            routing is tried in the mentioned order depending on whether this method is\n+            available on the sub-estimator.\n+\n+            .. versionadded:: 1.7\n+\n+                Only available if\n+                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n+                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n+                details.\n+\n         Returns\n         -------\n         p : ndarray of shape (n_samples, n_classes)\n             The class log-probabilities of the input samples. The order of the\n             classes corresponds to that in the attribute :term:`classes_`.\n         \"\"\"\n+        _raise_for_params(params, self, \"predict_log_proba\")\n+\n         check_is_fitted(self)\n+\n         if hasattr(self.estimator_, \"predict_log_proba\"):\n             # Check data\n             X = validate_data(\n@@ -987,6 +1085,12 @@ def predict_log_proba(self, X):\n                 reset=False,\n             )\n \n+            if _routing_enabled():\n+                routed_params = process_routing(self, \"predict_log_proba\", **params)\n+            else:\n+                routed_params = Bunch()\n+                routed_params.estimator = Bunch(predict_log_proba=Bunch())\n+\n             # Parallel loop\n             n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n \n@@ -996,6 +1100,7 @@ def predict_log_proba(self, X):\n                     self.estimators_features_[starts[i] : starts[i + 1]],\n                     X,\n                     self.n_classes_,\n+                    params=routed_params.estimator.predict_log_proba,\n                 )\n                 for i in range(n_jobs)\n             )\n@@ -1009,14 +1114,14 @@ def predict_log_proba(self, X):\n             log_proba -= np.log(self.n_estimators)\n \n         else:\n-            log_proba = np.log(self.predict_proba(X))\n+            log_proba = np.log(self.predict_proba(X, **params))\n \n         return log_proba\n \n     @available_if(\n         _estimator_has(\"decision_function\", delegates=(\"estimators_\", \"estimator\"))\n     )\n-    def decision_function(self, X):\n+    def decision_function(self, X, **params):\n         \"\"\"Average of the decision functions of the base classifiers.\n \n         Parameters\n@@ -1025,6 +1130,17 @@ def decision_function(self, X):\n             The training input samples. Sparse matrices are accepted only if\n             they are supported by the base estimator.\n \n+        **params : dict\n+            Parameters routed to the `decision_function` method of the sub-estimators\n+            via the metadata routing API.\n+\n+            .. versionadded:: 1.7\n+\n+                Only available if\n+                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n+                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n+                details.\n+\n         Returns\n         -------\n         score : ndarray of shape (n_samples, k)\n@@ -1033,6 +1149,8 @@ def decision_function(self, X):\n             ``classes_``. Regression and binary classification are special\n             cases with ``k == 1``, otherwise ``k==n_classes``.\n         \"\"\"\n+        _raise_for_params(params, self, \"decision_function\")\n+\n         check_is_fitted(self)\n \n         # Check data\n@@ -1045,6 +1163,12 @@ def decision_function(self, X):\n             reset=False,\n         )\n \n+        if _routing_enabled():\n+            routed_params = process_routing(self, \"decision_function\", **params)\n+        else:\n+            routed_params = Bunch()\n+            routed_params.estimator = Bunch(decision_function=Bunch())\n+\n         # Parallel loop\n         n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n \n@@ -1053,6 +1177,7 @@ def decision_function(self, X):\n                 self.estimators_[starts[i] : starts[i + 1]],\n                 self.estimators_features_[starts[i] : starts[i + 1]],\n                 X,\n+                params=routed_params.estimator.decision_function,\n             )\n             for i in range(n_jobs)\n         )\n@@ -1251,7 +1376,7 @@ def __init__(\n             verbose=verbose,\n         )\n \n-    def predict(self, X):\n+    def predict(self, X, **params):\n         \"\"\"Predict regression target for X.\n \n         The predicted regression target of an input sample is computed as the\n@@ -1263,11 +1388,24 @@ def predict(self, X):\n             The training input samples. Sparse matrices are accepted only if\n             they are supported by the base estimator.\n \n+        **params : dict\n+            Parameters routed to the `predict` method of the sub-estimators via the\n+            metadata routing API.\n+\n+            .. versionadded:: 1.7\n+\n+                Only available if\n+                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n+                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n+                details.\n+\n         Returns\n         -------\n         y : ndarray of shape (n_samples,)\n             The predicted values.\n         \"\"\"\n+        _raise_for_params(params, self, \"predict\")\n+\n         check_is_fitted(self)\n         # Check data\n         X = validate_data(\n@@ -1279,6 +1417,12 @@ def predict(self, X):\n             reset=False,\n         )\n \n+        if _routing_enabled():\n+            routed_params = process_routing(self, \"predict\", **params)\n+        else:\n+            routed_params = Bunch()\n+            routed_params.estimator = Bunch(predict=Bunch())\n+\n         # Parallel loop\n         n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n \n@@ -1287,6 +1431,7 @@ def predict(self, X):\n                 self.estimators_[starts[i] : starts[i + 1]],\n                 self.estimators_features_[starts[i] : starts[i + 1]],\n                 X,\n+                params=routed_params.estimator.predict,\n             )\n             for i in range(n_jobs)\n         )\ndiff --git a/sklearn/ensemble/tests/test_bagging.py b/sklearn/ensemble/tests/test_bagging.py\nindex f5386804d77d7..d8b1ce9091043 100644\n--- a/sklearn/ensemble/tests/test_bagging.py\n+++ b/sklearn/ensemble/tests/test_bagging.py\n@@ -11,7 +11,7 @@\n import numpy as np\n import pytest\n \n-import sklearn\n+from sklearn import config_context\n from sklearn.base import BaseEstimator\n from sklearn.datasets import load_diabetes, load_iris, make_hastie_10_2\n from sklearn.dummy import DummyClassifier, DummyRegressor\n@@ -33,6 +33,13 @@\n from sklearn.preprocessing import FunctionTransformer, scale\n from sklearn.random_projection import SparseRandomProjection\n from sklearn.svm import SVC, SVR\n+from sklearn.tests.metadata_routing_common import (\n+    ConsumingClassifierWithOnlyPredict,\n+    ConsumingClassifierWithoutPredictLogProba,\n+    ConsumingClassifierWithoutPredictProba,\n+    _Registry,\n+    check_recorded_metadata,\n+)\n from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n from sklearn.utils import check_random_state\n from sklearn.utils._testing import assert_array_almost_equal, assert_array_equal\n@@ -944,6 +951,11 @@ def test_bagging_allow_nan_tag(bagging, expected_allow_nan):\n     assert bagging.__sklearn_tags__().input_tags.allow_nan == expected_allow_nan\n \n \n+# Metadata Routing Tests\n+# ======================\n+\n+\n+@config_context(enable_metadata_routing=True)\n @pytest.mark.parametrize(\n     \"model\",\n     [\n@@ -957,8 +969,62 @@ def test_bagging_allow_nan_tag(bagging, expected_allow_nan):\n )\n def test_bagging_with_metadata_routing(model):\n     \"\"\"Make sure that metadata routing works with non-default estimator.\"\"\"\n-    with sklearn.config_context(enable_metadata_routing=True):\n-        model.fit(iris.data, iris.target)\n+    model.fit(iris.data, iris.target)\n+\n+\n+@pytest.mark.parametrize(\n+    \"sub_estimator, caller, callee\",\n+    [\n+        (ConsumingClassifierWithoutPredictProba, \"predict\", \"predict\"),\n+        (\n+            ConsumingClassifierWithoutPredictLogProba,\n+            \"predict_log_proba\",\n+            \"predict_proba\",\n+        ),\n+        (ConsumingClassifierWithOnlyPredict, \"predict_log_proba\", \"predict\"),\n+    ],\n+)\n+@config_context(enable_metadata_routing=True)\n+def test_metadata_routing_with_dynamic_method_selection(sub_estimator, caller, callee):\n+    \"\"\"Test that metadata routing works in `BaggingClassifier` with dynamic selection of\n+    the sub-estimator's methods. Here we test only specific test cases, where\n+    sub-estimator methods are not present and are not tested with `ConsumingClassifier`\n+    (which possesses all the methods) in\n+    sklearn/tests/test_metaestimators_metadata_routing.py: `BaggingClassifier.predict()`\n+    dynamically routes to `predict` if the sub-estimator doesn't have `predict_proba`\n+    and `BaggingClassifier.predict_log_proba()` dynamically routes to `predict_proba` if\n+    the sub-estimator doesn't have `predict_log_proba`, or to `predict`, if it doesn't\n+    have it.\n+    \"\"\"\n+    X = np.array([[0, 2], [1, 4], [2, 6]])\n+    y = [1, 2, 3]\n+    sample_weight, metadata = [1], \"a\"\n+    registry = _Registry()\n+    estimator = sub_estimator(registry=registry)\n+    set_callee_request = \"set_\" + callee + \"_request\"\n+    getattr(estimator, set_callee_request)(sample_weight=True, metadata=True)\n+\n+    bagging = BaggingClassifier(estimator=estimator)\n+    bagging.fit(X, y)\n+    getattr(bagging, caller)(\n+        X=np.array([[1, 1], [1, 3], [0, 2]]),\n+        sample_weight=sample_weight,\n+        metadata=metadata,\n+    )\n+\n+    assert len(registry)\n+    for estimator in registry:\n+        check_recorded_metadata(\n+            obj=estimator,\n+            method=callee,\n+            parent=caller,\n+            sample_weight=sample_weight,\n+            metadata=metadata,\n+        )\n+\n+\n+# End of Metadata Routing Tests\n+# =============================\n \n \n @pytest.mark.parametrize(\ndiff --git a/sklearn/tests/metadata_routing_common.py b/sklearn/tests/metadata_routing_common.py\nindex 98503652df6f0..c4af13ef66344 100644\n--- a/sklearn/tests/metadata_routing_common.py\n+++ b/sklearn/tests/metadata_routing_common.py\n@@ -218,9 +218,9 @@ def predict(self, X):\n \n     def predict_proba(self, X):\n         # dummy probabilities to support predict_proba\n-        y_proba = np.empty(shape=(len(X), 2))\n-        y_proba[: len(X) // 2, :] = np.asarray([1.0, 0.0])\n-        y_proba[len(X) // 2 :, :] = np.asarray([0.0, 1.0])\n+        y_proba = np.empty(shape=(len(X), len(self.classes_)), dtype=np.float32)\n+        # each row sums up to 1.0:\n+        y_proba[:] = np.random.dirichlet(alpha=np.ones(len(self.classes_)), size=len(X))\n         return y_proba\n \n     def predict_log_proba(self, X):\n@@ -298,16 +298,16 @@ def predict_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n         record_metadata_not_default(\n             self, sample_weight=sample_weight, metadata=metadata\n         )\n-        y_proba = np.empty(shape=(len(X), 2))\n-        y_proba[: len(X) // 2, :] = np.asarray([1.0, 0.0])\n-        y_proba[len(X) // 2 :, :] = np.asarray([0.0, 1.0])\n+        y_proba = np.empty(shape=(len(X), len(self.classes_)), dtype=np.float32)\n+        # each row sums up to 1.0:\n+        y_proba[:] = np.random.dirichlet(alpha=np.ones(len(self.classes_)), size=len(X))\n         return y_proba\n \n     def predict_log_proba(self, X, sample_weight=\"default\", metadata=\"default\"):\n         record_metadata_not_default(\n             self, sample_weight=sample_weight, metadata=metadata\n         )\n-        return np.zeros(shape=(len(X), 2))\n+        return self.predict_proba(X)\n \n     def decision_function(self, X, sample_weight=\"default\", metadata=\"default\"):\n         record_metadata_not_default(\n@@ -325,6 +325,46 @@ def score(self, X, y, sample_weight=\"default\", metadata=\"default\"):\n         return 1\n \n \n+class ConsumingClassifierWithoutPredictProba(ConsumingClassifier):\n+    \"\"\"ConsumingClassifier without a predict_proba method, but with predict_log_proba.\n+\n+    Used to mimic dynamic method selection such as in the `_parallel_predict_proba()`\n+    function called by `BaggingClassifier`.\n+    \"\"\"\n+\n+    @property\n+    def predict_proba(self):\n+        raise AttributeError(\"This estimator does not support predict_proba\")\n+\n+\n+class ConsumingClassifierWithoutPredictLogProba(ConsumingClassifier):\n+    \"\"\"ConsumingClassifier without a predict_log_proba method, but with predict_proba.\n+\n+    Used to mimic dynamic method selection such as in\n+    `BaggingClassifier.predict_log_proba()`.\n+    \"\"\"\n+\n+    @property\n+    def predict_log_proba(self):\n+        raise AttributeError(\"This estimator does not support predict_log_proba\")\n+\n+\n+class ConsumingClassifierWithOnlyPredict(ConsumingClassifier):\n+    \"\"\"ConsumingClassifier with only a predict method.\n+\n+    Used to mimic dynamic method selection such as in\n+    `BaggingClassifier.predict_log_proba()`.\n+    \"\"\"\n+\n+    @property\n+    def predict_proba(self):\n+        raise AttributeError(\"This estimator does not support predict_proba\")\n+\n+    @property\n+    def predict_log_proba(self):\n+        raise AttributeError(\"This estimator does not support predict_log_proba\")\n+\n+\n class ConsumingTransformer(TransformerMixin, BaseEstimator):\n     \"\"\"A transformer which accepts metadata on fit and transform.\n \ndiff --git a/sklearn/tests/test_metaestimators_metadata_routing.py b/sklearn/tests/test_metaestimators_metadata_routing.py\nindex 6947c14ff5e59..ae2a186a3c5c2 100644\n--- a/sklearn/tests/test_metaestimators_metadata_routing.py\n+++ b/sklearn/tests/test_metaestimators_metadata_routing.py\n@@ -329,7 +329,18 @@\n         \"X\": X,\n         \"y\": y,\n         \"preserves_metadata\": False,\n-        \"estimator_routing_methods\": [\"fit\"],\n+        \"estimator_routing_methods\": [\n+            \"fit\",\n+            \"predict\",\n+            \"predict_proba\",\n+            \"predict_log_proba\",\n+            \"decision_function\",\n+        ],\n+        \"method_mapping\": {\n+            \"predict\": [\"predict\", \"predict_proba\"],\n+            \"predict_proba\": [\"predict\", \"predict_proba\"],\n+            \"predict_log_proba\": [\"predict\", \"predict_proba\", \"predict_log_proba\"],\n+        },\n     },\n     {\n         \"metaestimator\": BaggingRegressor,\n@@ -338,7 +349,7 @@\n         \"X\": X,\n         \"y\": y,\n         \"preserves_metadata\": False,\n-        \"estimator_routing_methods\": [\"fit\"],\n+        \"estimator_routing_methods\": [\"fit\", \"predict\"],\n     },\n     {\n         \"metaestimator\": RidgeCV,\n",
  "fail_to_pass": [
    "test_metadata_routing_with_dynamic_method_selection"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/ensemble/_bagging.py",
    "sklearn/ensemble/tests/test_bagging.py",
    "sklearn/tests/metadata_routing_common.py",
    "sklearn/tests/test_metaestimators_metadata_routing.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-02-14T13:21:21Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30833",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30808"
}