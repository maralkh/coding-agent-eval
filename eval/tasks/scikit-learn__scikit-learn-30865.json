{
  "id": "scikit-learn__scikit-learn-30865",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "65a3e64965ca60daa3f86f2d041a91605f8115d3",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30865,
  "pr_title": "TST use global_random_seed in sklearn/metrics/tests/test_regression.py",
  "gold_patch": "diff --git a/sklearn/metrics/tests/test_regression.py b/sklearn/metrics/tests/test_regression.py\nindex ea8412d53c247..5e90727583189 100644\n--- a/sklearn/metrics/tests/test_regression.py\n+++ b/sklearn/metrics/tests/test_regression.py\n@@ -494,42 +494,44 @@ def test_regression_single_sample(metric):\n         assert np.isnan(score)\n \n \n-def test_tweedie_deviance_continuity():\n+def test_tweedie_deviance_continuity(global_random_seed):\n     n_samples = 100\n \n-    y_true = np.random.RandomState(0).rand(n_samples) + 0.1\n-    y_pred = np.random.RandomState(1).rand(n_samples) + 0.1\n+    rng = np.random.RandomState(global_random_seed)\n+\n+    y_true = rng.rand(n_samples) + 0.1\n+    y_pred = rng.rand(n_samples) + 0.1\n \n     assert_allclose(\n         mean_tweedie_deviance(y_true, y_pred, power=0 - 1e-10),\n         mean_tweedie_deviance(y_true, y_pred, power=0),\n     )\n \n-    # Ws we get closer to the limit, with 1e-12 difference the absolute\n+    # Ws we get closer to the limit, with 1e-12 difference the\n     # tolerance to pass the below check increases. There are likely\n     # numerical precision issues on the edges of different definition\n     # regions.\n     assert_allclose(\n         mean_tweedie_deviance(y_true, y_pred, power=1 + 1e-10),\n         mean_tweedie_deviance(y_true, y_pred, power=1),\n-        atol=1e-6,\n+        rtol=1e-5,\n     )\n \n     assert_allclose(\n         mean_tweedie_deviance(y_true, y_pred, power=2 - 1e-10),\n         mean_tweedie_deviance(y_true, y_pred, power=2),\n-        atol=1e-6,\n+        rtol=1e-5,\n     )\n \n     assert_allclose(\n         mean_tweedie_deviance(y_true, y_pred, power=2 + 1e-10),\n         mean_tweedie_deviance(y_true, y_pred, power=2),\n-        atol=1e-6,\n+        rtol=1e-5,\n     )\n \n \n-def test_mean_absolute_percentage_error():\n-    random_number_generator = np.random.RandomState(42)\n+def test_mean_absolute_percentage_error(global_random_seed):\n+    random_number_generator = np.random.RandomState(global_random_seed)\n     y_true = random_number_generator.exponential(size=100)\n     y_pred = 1.2 * y_true\n     assert mean_absolute_percentage_error(y_true, y_pred) == pytest.approx(0.2)\n@@ -539,7 +541,9 @@ def test_mean_absolute_percentage_error():\n     \"distribution\", [\"normal\", \"lognormal\", \"exponential\", \"uniform\"]\n )\n @pytest.mark.parametrize(\"target_quantile\", [0.05, 0.5, 0.75])\n-def test_mean_pinball_loss_on_constant_predictions(distribution, target_quantile):\n+def test_mean_pinball_loss_on_constant_predictions(\n+    distribution, target_quantile, global_random_seed\n+):\n     if not hasattr(np, \"quantile\"):\n         pytest.skip(\n             \"This test requires a more recent version of numpy \"\n@@ -548,7 +552,7 @@ def test_mean_pinball_loss_on_constant_predictions(distribution, target_quantile\n \n     # Check that the pinball loss is minimized by the empirical quantile.\n     n_samples = 3000\n-    rng = np.random.RandomState(42)\n+    rng = np.random.RandomState(global_random_seed)\n     data = getattr(rng, distribution)(size=n_samples)\n \n     # Compute the best possible pinball loss for any constant predictor:\n@@ -582,20 +586,22 @@ def objective_func(x):\n         constant_pred = np.full(n_samples, fill_value=x)\n         return mean_pinball_loss(data, constant_pred, alpha=target_quantile)\n \n-    result = optimize.minimize(objective_func, data.mean(), method=\"Nelder-Mead\")\n+    result = optimize.minimize(objective_func, data.mean())\n     assert result.success\n     # The minimum is not unique with limited data, hence the large tolerance.\n-    assert result.x == pytest.approx(best_pred, rel=1e-2)\n+    # For the normal distribution and the 0.5 quantile, the expected result is close to\n+    # 0, hence the additional use of absolute tolerance.\n+    assert_allclose(result.x, best_pred, rtol=1e-1, atol=1e-3)\n     assert result.fun == pytest.approx(best_pbl)\n \n \n-def test_dummy_quantile_parameter_tuning():\n+def test_dummy_quantile_parameter_tuning(global_random_seed):\n     # Integration test to check that it is possible to use the pinball loss to\n     # tune the hyperparameter of a quantile regressor. This is conceptually\n     # similar to the previous test but using the scikit-learn estimator and\n     # scoring API instead.\n     n_samples = 1000\n-    rng = np.random.RandomState(0)\n+    rng = np.random.RandomState(global_random_seed)\n     X = rng.normal(size=(n_samples, 5))  # Ignored\n     y = rng.exponential(size=n_samples)\n \n@@ -616,9 +622,9 @@ def test_dummy_quantile_parameter_tuning():\n         assert grid_search.best_params_[\"quantile\"] == pytest.approx(alpha)\n \n \n-def test_pinball_loss_relation_with_mae():\n+def test_pinball_loss_relation_with_mae(global_random_seed):\n     # Test that mean_pinball loss with alpha=0.5 if half of mean absolute error\n-    rng = np.random.RandomState(714)\n+    rng = np.random.RandomState(global_random_seed)\n     n = 100\n     y_true = rng.normal(size=n)\n     y_pred = y_true.copy() + rng.uniform(n)\n",
  "fail_to_pass": [
    "test_tweedie_deviance_continuity",
    "test_mean_absolute_percentage_error",
    "test_mean_pinball_loss_on_constant_predictions",
    "test_dummy_quantile_parameter_tuning",
    "test_pinball_loss_relation_with_mae"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/tests/test_regression.py"
  ],
  "difficulty": "easy",
  "created_at": "2025-02-20T04:15:46Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30865",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}