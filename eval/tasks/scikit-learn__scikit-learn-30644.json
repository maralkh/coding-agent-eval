{
  "id": "scikit-learn__scikit-learn-30644",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "311bf6badd74bb69081eb90e2643f15706d3473c",
  "issue_number": 16298,
  "issue_title": "List of estimators with known incorrect handling of `sample_weight`",
  "issue_body": "An issue with an associated common check originally discussed in https://github.com/scikit-learn/scikit-learn/pull/15015 \n\nEDIT: here is a report of statistical worst offenders:\n\nhttps://github.com/snath-xoc/sample-weight-audit-nondet/blob/main/reports/sklearn_estimators_sample_weight_audit_report.ipynb\n\n> This is a pretty simple sample_weight test that says that a weight of 0 is equivalent to not having the samples.\n> I think every failure here should be considered a bug. This is:\n\n- [ ] AdaBoostClassifier/Regressor\n- [x] BayesianRidge #30644\n- [x] CalibratedClassifierCV #29796\n- [x] CategoricalNB #31556\n- [ ] GradientBoostingClassifier/Regressor\n  - not sure if `fit` is deterministic or not with the config used in `check_sample_weight_equivalence`: to be investigated.\n    - rounding errors make it non-deterministic and break equivalence because of systematic bias to handle near-tied splits: https://github.com/scikit-learn/scikit-learn/issues/23728#issuecomment-2737437203\n- [ ] HistGradientBoostingClassifier/Regressor\n  - [ ] `check_sample_weight_equivalence` fails event when subsampling for binning is not enabled.\n  - [ ] subsampling for binning needs to be `sample_weight` aware but can then be only properly tested with a statistical test instead of `check_sample_weight_equivalence`\n  - [ ] subsampling also happens (without respecting weights) to compute non-default scoring on a subsampled training set when early stopping is enabled without a validation set\n  - [ ] #29641\n- [ ] HuberRegressor\n- [x] LinearRegression\n  - [x] #30040\n  - [x] #30131\n- [ ] LinearSVC \n  - [x] #30057\n  - [ ] apparently related to the use of `liblinear`\n- [ ] LinearSVR\n  - [ ] apparently related to the use of `liblinear`\n- [ ] LogisticRegression\n  - [ ] `lbfgs` causes `check_sample_weight_equivalence` to fail (slightly)\n  - [ ] `liblinear` with `C=0.01` causes `check_sample_weight_equivalence` to fail (slightly)\n  - [ ] https://github.com/scikit-learn/scikit-learn/pull/31675\n- [x] LogisticRegressionCV\n  - #29419\n- [x] ElasticNetCV / LassoCV #29442 and #29796 (both are needed)\n- [x] Ridge\n  - `check_sample_weight_equivalence` now passes for this estimator after lowering the `tol` value for `lsqr` and `sparse-cg` in the per-check params.\n- [x] RidgeClassifier\n  - `check_sample_weight_equivalence` now passes for this estimator after lowering the `tol` value for `lsqr` and `sparse-cg` in the per-check params.\n- [x] RidgeClassifierCV. #29796\n- [ ] DecisionTreeRegressor\n   - [ ] biased handling of near-tied splits: https://github.com/scikit-learn/scikit-learn/issues/23728#issuecomment-2737437203\n- [x] RidgeCV and RidgeClassifierCV can delegate to GridSearchCV when using a non-default `cv` (which is the case in `check_sample_weight_equivalence`) or `scoring` params.\n    - [x] fixed by #30743\n- [ ] OneClassSVM\n- [ ] NuSVC (same as SVC)\n- [ ] NuSVR\n- [ ] SVC\n  - `check_sample_weight_equivalence` fails with `probability=False`: to be investigated\n  - this expected with `probability=True` as the weights are not propagated to the internal CV implemented in libsvm\n- [ ] SVR\n- [ ] GridSearchCV / RandomizedSearchCV / ...\n   - [x] did not forward `sample_weight` to their scorer by default #30743.\n   - [ ] do this even when metadata routing is disabled\n   - [ ] implement a default routing policy for `sample_weight` in general: #26179\n\nThe following estimators have a stochastic fit, so testing for correct handling of sample weights cannot be tested with `check_sample_weight_equivalence` but instead [requires a statistical test](https://github.com/scikit-learn/scikit-learn/issues/16298#issuecomment-2331044662):\n\n- [x] BaggingClassifier/Regressor\n  - #31414\n- [x] BisectingKMeans\n- [x] KBinsDiscretizer\n  - #29907\n  - note that with a small `n_samples` and uniform or quantile strategies, the fit is deterministic.\n- [ ] SGDClassifier/Regressor\n- [ ] Perceptron (likely same bug as SGD)\n- [ ] RANSACRegressor\n  - [ ] https://github.com/scikit-learn/scikit-learn/issues/15836\n- [x] KMeans\n- [x] MiniBatchKMeans\n- [ ] IsolationForest\n- [ ] RandomForestClassifier/Regressor & ExtraTrees\n   - https://github.com/scikit-learn/scikit-learn/pull/31529\n- [ ] RandomTreesEmbedding\n\n(some might have been fixed since, need to check).\n \nThe required sample weight invariance properties (including the behavior of sw=0) were also discussed in https://github.com/scikit-learn/scikit-learn/issues/15657\n\nEDIT: expected `sample_weight` semantics have since been more generally in the [refactoring of `check_sample_weight_invariance` into `check_sample_weight_equivalence`](https://github.com/scikit-learn/scikit-learn/pull/29818) to check fitting with integer sample weights is equivalent to fitting with repeated data points with a number of repetitions that matches the original weights.",
  "pr_number": 30644,
  "pr_title": "FIX Sample weight in BayesianRidge",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/30644.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/30644.fix.rst\nnew file mode 100644\nindex 0000000000000..c9254fe350e28\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/30644.fix.rst\n@@ -0,0 +1,3 @@\n+-  The update and initialization of the hyperparameters now properly handle\n+   sample weights in :class:`linear_model.BayesianRidge`.\n+   By :user:`Antoine Baker <antoinebaker>`.\ndiff --git a/sklearn/linear_model/_bayes.py b/sklearn/linear_model/_bayes.py\nindex b6527d4f22b1f..27ce01d0e75d5 100644\n--- a/sklearn/linear_model/_bayes.py\n+++ b/sklearn/linear_model/_bayes.py\n@@ -244,9 +244,15 @@ def fit(self, X, y, sample_weight=None):\n             y_numeric=True,\n         )\n         dtype = X.dtype\n+        n_samples, n_features = X.shape\n \n+        sw_sum = n_samples\n+        y_var = y.var()\n         if sample_weight is not None:\n             sample_weight = _check_sample_weight(sample_weight, X, dtype=dtype)\n+            sw_sum = sample_weight.sum()\n+            y_mean = np.average(y, weights=sample_weight)\n+            y_var = np.average((y - y_mean) ** 2, weights=sample_weight)\n \n         X, y, X_offset_, y_offset_, X_scale_ = _preprocess_data(\n             X,\n@@ -262,16 +268,14 @@ def fit(self, X, y, sample_weight=None):\n \n         self.X_offset_ = X_offset_\n         self.X_scale_ = X_scale_\n-        n_samples, n_features = X.shape\n \n         # Initialization of the values of the parameters\n         eps = np.finfo(np.float64).eps\n-        # Add `eps` in the denominator to omit division by zero if `np.var(y)`\n-        # is zero\n+        # Add `eps` in the denominator to omit division by zero\n         alpha_ = self.alpha_init\n         lambda_ = self.lambda_init\n         if alpha_ is None:\n-            alpha_ = 1.0 / (np.var(y) + eps)\n+            alpha_ = 1.0 / (y_var + eps)\n         if lambda_ is None:\n             lambda_ = 1.0\n \n@@ -295,21 +299,28 @@ def fit(self, X, y, sample_weight=None):\n         # Convergence loop of the bayesian ridge regression\n         for iter_ in range(self.max_iter):\n             # update posterior mean coef_ based on alpha_ and lambda_ and\n-            # compute corresponding rmse\n-            coef_, rmse_ = self._update_coef_(\n+            # compute corresponding sse (sum of squared errors)\n+            coef_, sse_ = self._update_coef_(\n                 X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n             )\n             if self.compute_score:\n                 # compute the log marginal likelihood\n                 s = self._log_marginal_likelihood(\n-                    n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_\n+                    n_samples,\n+                    n_features,\n+                    sw_sum,\n+                    eigen_vals_,\n+                    alpha_,\n+                    lambda_,\n+                    coef_,\n+                    sse_,\n                 )\n                 self.scores_.append(s)\n \n             # Update alpha and lambda according to (MacKay, 1992)\n             gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n             lambda_ = (gamma_ + 2 * lambda_1) / (np.sum(coef_**2) + 2 * lambda_2)\n-            alpha_ = (n_samples - gamma_ + 2 * alpha_1) / (rmse_ + 2 * alpha_2)\n+            alpha_ = (sw_sum - gamma_ + 2 * alpha_1) / (sse_ + 2 * alpha_2)\n \n             # Check for convergence\n             if iter_ != 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:\n@@ -324,13 +335,20 @@ def fit(self, X, y, sample_weight=None):\n         # log marginal likelihood and posterior covariance\n         self.alpha_ = alpha_\n         self.lambda_ = lambda_\n-        self.coef_, rmse_ = self._update_coef_(\n+        self.coef_, sse_ = self._update_coef_(\n             X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n         )\n         if self.compute_score:\n             # compute the log marginal likelihood\n             s = self._log_marginal_likelihood(\n-                n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_\n+                n_samples,\n+                n_features,\n+                sw_sum,\n+                eigen_vals_,\n+                alpha_,\n+                lambda_,\n+                coef_,\n+                sse_,\n             )\n             self.scores_.append(s)\n             self.scores_ = np.array(self.scores_)\n@@ -378,7 +396,7 @@ def predict(self, X, return_std=False):\n     def _update_coef_(\n         self, X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n     ):\n-        \"\"\"Update posterior mean and compute corresponding rmse.\n+        \"\"\"Update posterior mean and compute corresponding sse (sum of squared errors).\n \n         Posterior mean is given by coef_ = scaled_sigma_ * X.T * y where\n         scaled_sigma_ = (lambda_/alpha_ * np.eye(n_features)\n@@ -394,12 +412,14 @@ def _update_coef_(\n                 [X.T, U / (eigen_vals_ + lambda_ / alpha_)[None, :], U.T, y]\n             )\n \n-        rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n+        # Note: we do not need to explicitly use the weights in this sum because\n+        # y and X were preprocessed by _rescale_data to handle the weights.\n+        sse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n \n-        return coef_, rmse_\n+        return coef_, sse_\n \n     def _log_marginal_likelihood(\n-        self, n_samples, n_features, eigen_vals, alpha_, lambda_, coef, rmse\n+        self, n_samples, n_features, sw_sum, eigen_vals, alpha_, lambda_, coef, sse\n     ):\n         \"\"\"Log marginal likelihood.\"\"\"\n         alpha_1 = self.alpha_1\n@@ -421,11 +441,11 @@ def _log_marginal_likelihood(\n         score += alpha_1 * log(alpha_) - alpha_2 * alpha_\n         score += 0.5 * (\n             n_features * log(lambda_)\n-            + n_samples * log(alpha_)\n-            - alpha_ * rmse\n+            + sw_sum * log(alpha_)\n+            - alpha_ * sse\n             - lambda_ * np.sum(coef**2)\n             + logdet_sigma\n-            - n_samples * log(2 * np.pi)\n+            - sw_sum * log(2 * np.pi)\n         )\n \n         return score\n@@ -684,14 +704,12 @@ def update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_):\n             coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)\n \n             # Update alpha and lambda\n-            rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n+            sse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n             gamma_ = 1.0 - lambda_[keep_lambda] * np.diag(sigma_)\n             lambda_[keep_lambda] = (gamma_ + 2.0 * lambda_1) / (\n                 (coef_[keep_lambda]) ** 2 + 2.0 * lambda_2\n             )\n-            alpha_ = (n_samples - gamma_.sum() + 2.0 * alpha_1) / (\n-                rmse_ + 2.0 * alpha_2\n-            )\n+            alpha_ = (n_samples - gamma_.sum() + 2.0 * alpha_1) / (sse_ + 2.0 * alpha_2)\n \n             # Prune the weights with a precision over a threshold\n             keep_lambda = lambda_ < self.threshold_lambda\n@@ -706,7 +724,7 @@ def update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_):\n                     + n_samples * log(alpha_)\n                     + np.sum(np.log(lambda_))\n                 )\n-                s -= 0.5 * (alpha_ * rmse_ + (lambda_ * coef_**2).sum())\n+                s -= 0.5 * (alpha_ * sse_ + (lambda_ * coef_**2).sum())\n                 self.scores_.append(s)\n \n             # Check for convergence\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex 7f9cab13cf5b0..efcf06140f3f8 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -836,15 +836,6 @@ def _yield_instances_for_check(check, estimator_orig):\n             \"sample_weight is not equivalent to removing/repeating samples.\"\n         ),\n     },\n-    BayesianRidge: {\n-        # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n-        \"check_sample_weight_equivalence_on_dense_data\": (\n-            \"sample_weight is not equivalent to removing/repeating samples.\"\n-        ),\n-        \"check_sample_weight_equivalence_on_sparse_data\": (\n-            \"sample_weight is not equivalent to removing/repeating samples.\"\n-        ),\n-    },\n     BernoulliRBM: {\n         \"check_methods_subset_invariance\": (\"fails for the decision_function method\"),\n         \"check_methods_sample_order_invariance\": (\"fails for the score_samples method\"),\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_bayes.py",
    "sklearn/utils/_test_common/instance_generator.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-01-14T13:54:31Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30644",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/16298"
}