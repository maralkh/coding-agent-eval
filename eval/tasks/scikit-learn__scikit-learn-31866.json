{
  "id": "scikit-learn__scikit-learn-31866",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "a665c603446315d59e640ddc3cf2c8693e611163",
  "issue_number": 31859,
  "issue_title": "Intercepts of Newton-Cholesky logistic regression get corrupted when warm starting",
  "issue_body": "### Describe the bug\n\nWhen using multinomial logistic regression with warm starts from a previous iteration, the final coefficients in the model are correct, but the intercepts somehow get filled with incorrect numbers somewhere.\n\nAs a result, predictions from a warm-started model differ from those of a cold-start model that has more iterations on the same data.\n\nThe issue appears to have been introduced recently as it works fine with version 1.5, but not with 1.6 or 1.7.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\n\nmodel1 = LogisticRegression(\n    solver=\"newton-cholesky\",\n    max_iter=2\n).fit(X, y)\nmodel2 = LogisticRegression(\n    solver=\"newton-cholesky\",\n    max_iter=1,\n    warm_start=True\n).fit(X, y).fit(X, y)\n\nnp.testing.assert_almost_equal(\n    model1.coef_,\n    model2.coef_\n)\n\nnp.testing.assert_almost_equal(\n    model1.predict_proba(X[:5]),\n    model2.predict_proba(X[:5])\n)\n```\n\n### Expected Results\n\nIntercepts should be the same, up to shifting by a constant if needed.\n\n### Actual Results\n\nIntercepts are different, as are predicted probabilities\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]\nexecutable: /home/david/miniforge3/bin/python\n   machine: Linux-6.12.33+deb12-amd64-x86_64-with-glibc2.36\n\nPython dependencies:\n      sklearn: 1.7.1\n          pip: 24.2\n   setuptools: 74.1.2\n        numpy: 2.0.1\n        scipy: 1.14.1\n       Cython: 3.1.0\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 20\n         prefix: libscipy_openblas\n       filepath: /home/david/.local/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: mkl\n    num_threads: 14\n         prefix: libmkl_rt\n       filepath: /home/david/miniforge3/lib/libmkl_rt.so.2\n        version: 2023.2-Product\nthreading_layer: gnu\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 20\n         prefix: libgomp\n       filepath: /home/david/miniforge3/lib/libgomp.so.1.0.0\n        version: None\n```",
  "pr_number": 31866,
  "pr_title": "FIX LogisticRegression warm start with newton-cholesky solver",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/31866.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/31866.fix.rst\nnew file mode 100644\nindex 0000000000000..ba37d75ff8e5a\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/31866.fix.rst\n@@ -0,0 +1,6 @@\n+- Fixed a bug in class:`linear_model:LogisticRegression` when used with\n+ `solver=\"newton-cholesky\"`and `warm_start=True` on multi-class problems, either\n+  with `fit_intercept=True` or with `penalty=None` (both resulting in unpenalized\n+  parameters for the solver). The coefficients and intercepts of the last class as\n+  provided by warm start were partially wrongly overwritten by zero.\n+  By :user:`Christian Lorentzen <lorentzenchr>`\ndiff --git a/sklearn/linear_model/_glm/_newton_solver.py b/sklearn/linear_model/_glm/_newton_solver.py\nindex 24f9c3bd9cadd..b0e071aa9b4f8 100644\n--- a/sklearn/linear_model/_glm/_newton_solver.py\n+++ b/sklearn/linear_model/_glm/_newton_solver.py\n@@ -469,6 +469,19 @@ def setup(self, X, y, sample_weight):\n         self.is_multinomial_no_penalty = (\n             self.linear_loss.base_loss.is_multiclass and self.l2_reg_strength == 0\n         )\n+        if self.is_multinomial_no_penalty:\n+            # See inner_solve. The provided coef might not adhere to the convention\n+            # that the last class is set to zero.\n+            # This is done by the usual freedom of a (overparametrized) multinomial to\n+            # add a constant to all classes which doesn't change predictions.\n+            n_classes = self.linear_loss.base_loss.n_classes\n+            coef = self.coef.reshape(n_classes, -1, order=\"F\")  # easier as 2d\n+            coef -= coef[-1, :]  # coef -= coef of last class\n+        elif self.is_multinomial_with_intercept:\n+            # See inner_solve. Same as above, but only for the intercept.\n+            n_classes = self.linear_loss.base_loss.n_classes\n+            # intercept -= intercept of last class\n+            self.coef[-n_classes:] -= self.coef[-1]\n \n     def update_gradient_hessian(self, X, y, sample_weight):\n         _, _, self.hessian_warning = self.linear_loss.gradient_hessian(\n@@ -518,10 +531,10 @@ def inner_solve(self, X, y, sample_weight):\n             #\n             # We choose the standard approach and set all the coefficients of the last\n             # class to zero, for all features including the intercept.\n+            # Note that coef was already dealt with in setup.\n             n_classes = self.linear_loss.base_loss.n_classes\n             n_dof = self.coef.size // n_classes  # degree of freedom per class\n             n = self.coef.size - n_dof  # effective size\n-            self.coef[n_classes - 1 :: n_classes] = 0\n             self.gradient[n_classes - 1 :: n_classes] = 0\n             self.hessian[n_classes - 1 :: n_classes, :] = 0\n             self.hessian[:, n_classes - 1 :: n_classes] = 0\n@@ -544,7 +557,7 @@ def inner_solve(self, X, y, sample_weight):\n         elif self.is_multinomial_with_intercept:\n             # Here, only intercepts are unpenalized. We again choose the last class and\n             # set its intercept to zero.\n-            self.coef[-1] = 0\n+            # Note that coef was already dealt with in setup.\n             self.gradient[-1] = 0\n             self.hessian[-1, :] = 0\n             self.hessian[:, -1] = 0\ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\nindex fdfe83889e475..e423761cbde98 100644\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -1434,9 +1434,7 @@ def test_n_iter(solver):\n     assert clf_cv.n_iter_.shape == (1, n_cv_fold, n_Cs)\n \n \n-@pytest.mark.parametrize(\n-    \"solver\", sorted(set(SOLVERS) - set([\"liblinear\", \"newton-cholesky\"]))\n-)\n+@pytest.mark.parametrize(\"solver\", sorted(set(SOLVERS) - set([\"liblinear\"])))\n @pytest.mark.parametrize(\"warm_start\", (True, False))\n @pytest.mark.parametrize(\"fit_intercept\", (True, False))\n def test_warm_start(global_random_seed, solver, warm_start, fit_intercept):\n@@ -1469,6 +1467,40 @@ def test_warm_start(global_random_seed, solver, warm_start, fit_intercept):\n         assert cum_diff > 2.0, msg\n \n \n+@pytest.mark.parametrize(\"solver\", [\"newton-cholesky\", \"newton-cg\"])\n+@pytest.mark.parametrize(\"fit_intercept\", (True, False))\n+@pytest.mark.parametrize(\"penalty\", (\"l2\", None))\n+def test_warm_start_newton_solver(global_random_seed, solver, fit_intercept, penalty):\n+    \"\"\"Test that 2 steps at once are the same as 2 single steps with warm start.\"\"\"\n+    X, y = iris.data, iris.target\n+\n+    clf1 = LogisticRegression(\n+        solver=solver,\n+        max_iter=2,\n+        fit_intercept=fit_intercept,\n+        penalty=penalty,\n+        random_state=global_random_seed,\n+    )\n+    with ignore_warnings(category=ConvergenceWarning):\n+        clf1.fit(X, y)\n+\n+    clf2 = LogisticRegression(\n+        solver=solver,\n+        max_iter=1,\n+        warm_start=True,\n+        fit_intercept=fit_intercept,\n+        penalty=penalty,\n+        random_state=global_random_seed,\n+    )\n+    with ignore_warnings(category=ConvergenceWarning):\n+        clf2.fit(X, y)\n+        clf2.fit(X, y)\n+\n+    assert_allclose(clf2.coef_, clf1.coef_)\n+    if fit_intercept:\n+        assert_allclose(clf2.intercept_, clf1.intercept_)\n+\n+\n @pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS)\n def test_saga_vs_liblinear(global_random_seed, csr_container):\n     iris = load_iris()\n",
  "fail_to_pass": [
    "test_warm_start_newton_solver"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_glm/_newton_solver.py",
    "sklearn/linear_model/tests/test_logistic.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-08-01T15:21:45Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31866",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/31859"
}