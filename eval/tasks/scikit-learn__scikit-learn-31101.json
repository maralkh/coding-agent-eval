{
  "id": "scikit-learn__scikit-learn-31101",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "0dbbac961ee251309aa7ec6deee658da83b0eaf0",
  "issue_number": 31098,
  "issue_title": "Failing CI for check_sample_weight_equivalence_on_dense_data with LinearRegerssion on debian_32bit",
  "issue_body": "Here is the last scheduled run (from 1 day ago) that passed:\n\nhttps://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75127&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092&t=a73eff7b-829e-5a65-7648-23ff8e83ea2d\n\nand here is a more recent run that failed (all CI is failing today):\n\nhttps://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75179&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092&t=a73eff7b-829e-5a65-7648-23ff8e83ea2d\n\n```\nFAILED tests/test_common.py::test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] - AssertionError: \nFAILED utils/tests/test_estimator_checks.py::test_check_estimator_clones - AssertionError: \n= 2 failed, 34214 passed, 4182 skipped, 174 xfailed, 66 xpassed, 4252 warnings in 1489.21s (0:24:49) =\n```\n\nFull failure log:\n\n<details>\n\n```\n2025-03-28T06:36:32.3433619Z =================================== FAILURES ===================================\n2025-03-28T06:36:32.3434358Z \u001b[31m\u001b[1m_ test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] _\u001b[0m\n2025-03-28T06:36:32.3434613Z \n2025-03-28T06:36:32.3434838Z estimator = LinearRegression(positive=True)\n2025-03-28T06:36:32.3435117Z check = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3435705Z request = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n2025-03-28T06:36:32.3435878Z \n2025-03-28T06:36:32.3436047Z     @parametrize_with_checks(\n2025-03-28T06:36:32.3436274Z         list(_tested_estimators()), expected_failed_checks=_get_expected_failed_checks\n2025-03-28T06:36:32.3436498Z     )\n2025-03-28T06:36:32.3436684Z     def test_estimators(estimator, check, request):\n2025-03-28T06:36:32.3436909Z         # Common tests for estimator instances\n2025-03-28T06:36:32.3437101Z         with ignore_warnings(\n2025-03-28T06:36:32.3437316Z             category=(FutureWarning, ConvergenceWarning, UserWarning, LinAlgWarning)\n2025-03-28T06:36:32.3437521Z         ):\n2025-03-28T06:36:32.3437708Z >           check(estimator)\n2025-03-28T06:36:32.3437793Z \n2025-03-28T06:36:32.3438019Z check      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3438293Z estimator  = LinearRegression(positive=True)\n2025-03-28T06:36:32.3438559Z request    = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n2025-03-28T06:36:32.3438707Z \n2025-03-28T06:36:32.3439155Z \u001b[1m\u001b[31m/io/sklearn/tests/test_common.py\u001b[0m:122: \n2025-03-28T06:36:32.3439405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3439768Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1570: in check_sample_weight_equivalence_on_dense_data\n2025-03-28T06:36:32.3440046Z     _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n2025-03-28T06:36:32.3440296Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3440498Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3440774Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:145: in wrapper\n2025-03-28T06:36:32.3440988Z     return fn(*args, **kwargs)\n2025-03-28T06:36:32.3441218Z         args       = ('LinearRegression', LinearRegression(positive=True))\n2025-03-28T06:36:32.3441452Z         fn         = <function _check_sample_weight_equivalence at 0xd8591de8>\n2025-03-28T06:36:32.3441744Z         kwargs     = {'sparse_container': None}\n2025-03-28T06:36:32.3441952Z         self       = _IgnoreWarnings(record=True)\n2025-03-28T06:36:32.3442307Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1566: in _check_sample_weight_equivalence\n2025-03-28T06:36:32.3442654Z     assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n2025-03-28T06:36:32.3442905Z         X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3443158Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3443423Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3443662Z         X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3443929Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3444177Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3444412Z         X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3444772Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3444987Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3445218Z         X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3445494Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3445740Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3445976Z         X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n2025-03-28T06:36:32.3446499Z         0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n2025-03-28T06:36:32.3446765Z         0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n2025-03-28T06:36:32.3447241Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3447553Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3448930Z         estimator_repeated = LinearRegression(positive=True)\n2025-03-28T06:36:32.3449323Z         estimator_weighted = LinearRegression(positive=True)\n2025-03-28T06:36:32.3449572Z         method     = 'predict'\n2025-03-28T06:36:32.3449847Z         n_samples  = 15\n2025-03-28T06:36:32.3450076Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3450329Z         rng        = RandomState(MT19937) at 0xCB6EECE8\n2025-03-28T06:36:32.3450580Z         sparse_container = None\n2025-03-28T06:36:32.3450850Z         sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n2025-03-28T06:36:32.3451124Z         y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n2025-03-28T06:36:32.3451409Z         y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n2025-03-28T06:36:32.3451823Z        1, 1, 1, 1, 1])\n2025-03-28T06:36:32.3452390Z         y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n2025-03-28T06:36:32.3467337Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:283: in assert_allclose_dense_sparse\n2025-03-28T06:36:32.3468147Z     assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n2025-03-28T06:36:32.3468478Z         atol       = 1e-09\n2025-03-28T06:36:32.3468965Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3469366Z         rtol       = 1e-07\n2025-03-28T06:36:32.3469659Z         x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3470005Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3470354Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3470648Z         y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3470947Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3471226Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3471533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3471702Z \n2025-03-28T06:36:32.3472165Z actual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3472656Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3473239Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3473561Z desired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3473863Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3474355Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3474896Z rtol = 1e-07, atol = 1e-09, equal_nan = True\n2025-03-28T06:36:32.3475293Z err_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3475668Z verbose = True\n2025-03-28T06:36:32.3475847Z \n2025-03-28T06:36:32.3476109Z     def assert_allclose(\n2025-03-28T06:36:32.3476425Z         actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n2025-03-28T06:36:32.3476851Z     ):\n2025-03-28T06:36:32.3477457Z         \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n2025-03-28T06:36:32.3477743Z     \n2025-03-28T06:36:32.3478184Z         This variant introspects the least precise floating point dtype\n2025-03-28T06:36:32.3478502Z         in the input argument and automatically sets the relative tolerance\n2025-03-28T06:36:32.3478839Z         parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n2025-03-28T06:36:32.3479134Z         in scikit-learn).\n2025-03-28T06:36:32.3479381Z     \n2025-03-28T06:36:32.3479837Z         `atol` is always left to 0. by default. It should be adjusted manually\n2025-03-28T06:36:32.3480176Z         to an assertion-specific value in case there are null values expected\n2025-03-28T06:36:32.3480467Z         in `desired`.\n2025-03-28T06:36:32.3480718Z     \n2025-03-28T06:36:32.3480995Z         The aggregate tolerance is `atol + rtol * abs(desired)`.\n2025-03-28T06:36:32.3481285Z     \n2025-03-28T06:36:32.3481704Z         Parameters\n2025-03-28T06:36:32.3481965Z         ----------\n2025-03-28T06:36:32.3482420Z         actual : array_like\n2025-03-28T06:36:32.3483027Z             Array obtained.\n2025-03-28T06:36:32.3483323Z         desired : array_like\n2025-03-28T06:36:32.3483598Z             Array desired.\n2025-03-28T06:36:32.3483881Z         rtol : float, optional, default=None\n2025-03-28T06:36:32.3484165Z             Relative tolerance.\n2025-03-28T06:36:32.3484462Z             If None, it is set based on the provided arrays' dtypes.\n2025-03-28T06:36:32.3484791Z         atol : float, optional, default=0.\n2025-03-28T06:36:32.3485072Z             Absolute tolerance.\n2025-03-28T06:36:32.3485449Z         equal_nan : bool, optional, default=True\n2025-03-28T06:36:32.3485916Z             If True, NaNs will compare equal.\n2025-03-28T06:36:32.3486379Z         err_msg : str, optional, default=''\n2025-03-28T06:36:32.3486669Z             The error message to be printed in case of failure.\n2025-03-28T06:36:32.3486959Z         verbose : bool, optional, default=True\n2025-03-28T06:36:32.3487445Z             If True, the conflicting values are appended to the error message.\n2025-03-28T06:36:32.3487721Z     \n2025-03-28T06:36:32.3487982Z         Raises\n2025-03-28T06:36:32.3488229Z         ------\n2025-03-28T06:36:32.3488486Z         AssertionError\n2025-03-28T06:36:32.3490170Z             If actual and desired are not equal up to specified precision.\n2025-03-28T06:36:32.3490710Z     \n2025-03-28T06:36:32.3491058Z         See Also\n2025-03-28T06:36:32.3491386Z         --------\n2025-03-28T06:36:32.3491708Z         numpy.testing.assert_allclose\n2025-03-28T06:36:32.3492016Z     \n2025-03-28T06:36:32.3492308Z         Examples\n2025-03-28T06:36:32.3493161Z         --------\n2025-03-28T06:36:32.3493425Z         >>> import numpy as np\n2025-03-28T06:36:32.3493712Z         >>> from sklearn.utils._testing import assert_allclose\n2025-03-28T06:36:32.3493988Z         >>> x = [1e-5, 1e-3, 1e-1]\n2025-03-28T06:36:32.3494270Z         >>> y = np.arccos(np.cos(x))\n2025-03-28T06:36:32.3494725Z         >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n2025-03-28T06:36:32.3495044Z         >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n2025-03-28T06:36:32.3495339Z         >>> assert_allclose(a, 1e-5)\n2025-03-28T06:36:32.3495595Z         \"\"\"\n2025-03-28T06:36:32.3496358Z         dtypes = []\n2025-03-28T06:36:32.3496662Z     \n2025-03-28T06:36:32.3496955Z         actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n2025-03-28T06:36:32.3497629Z         dtypes = [actual.dtype, desired.dtype]\n2025-03-28T06:36:32.3497899Z     \n2025-03-28T06:36:32.3498178Z         if rtol is None:\n2025-03-28T06:36:32.3498471Z             rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n2025-03-28T06:36:32.3498764Z             rtol = max(rtols)\n2025-03-28T06:36:32.3499011Z     \n2025-03-28T06:36:32.3604475Z >       np_assert_allclose(\n2025-03-28T06:36:32.3607071Z             actual,\n2025-03-28T06:36:32.3608138Z             desired,\n2025-03-28T06:36:32.3608886Z             rtol=rtol,\n2025-03-28T06:36:32.3609217Z             atol=atol,\n2025-03-28T06:36:32.3625173Z             equal_nan=equal_nan,\n2025-03-28T06:36:32.3639658Z             err_msg=err_msg,\n2025-03-28T06:36:32.3640151Z             verbose=verbose,\n2025-03-28T06:36:32.3640425Z         )\n2025-03-28T06:36:32.3640827Z \u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n2025-03-28T06:36:32.3641237Z \u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-07, atol=1e-09\u001b[0m\n2025-03-28T06:36:32.3641778Z \u001b[1m\u001b[31mE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\u001b[0m\n2025-03-28T06:36:32.3642255Z \u001b[1m\u001b[31mE       Mismatched elements: 6 / 15 (40%)\u001b[0m\n2025-03-28T06:36:32.3642821Z \u001b[1m\u001b[31mE       Max absolute difference among violations: 2.51014256\u001b[0m\n2025-03-28T06:36:32.3643306Z \u001b[1m\u001b[31mE       Max relative difference among violations: 2.17024526\u001b[0m\n2025-03-28T06:36:32.3643773Z \u001b[1m\u001b[31mE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\u001b[0m\n2025-03-28T06:36:32.3644416Z \u001b[1m\u001b[31mE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\u001b[0m\n2025-03-28T06:36:32.3644878Z \u001b[1m\u001b[31mE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\u001b[0m\n2025-03-28T06:36:32.3645289Z \u001b[1m\u001b[31mE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\u001b[0m\n2025-03-28T06:36:32.3645729Z \u001b[1m\u001b[31mE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\u001b[0m\n2025-03-28T06:36:32.3646166Z \u001b[1m\u001b[31mE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\u001b[0m\n2025-03-28T06:36:32.3646542Z \u001b[1m\u001b[31mE              1.709333, 1.      , 1.      ])\u001b[0m\n2025-03-28T06:36:32.3646727Z \n2025-03-28T06:36:32.3647013Z actual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3647353Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3647675Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3647970Z atol       = 1e-09\n2025-03-28T06:36:32.3648247Z desired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3648536Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3648814Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3649119Z dtypes     = [dtype('float64'), dtype('float64')]\n2025-03-28T06:36:32.3650674Z equal_nan  = True\n2025-03-28T06:36:32.3664015Z err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3664453Z rtol       = 1e-07\n2025-03-28T06:36:32.3664740Z verbose    = True\n2025-03-28T06:36:32.3664894Z \n2025-03-28T06:36:32.3665331Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:237: AssertionError\n2025-03-28T06:36:32.3665827Z \u001b[31m\u001b[1m_________________________ test_check_estimator_clones __________________________\u001b[0m\n2025-03-28T06:36:32.3666186Z \n2025-03-28T06:36:32.3666476Z     def test_check_estimator_clones():\n2025-03-28T06:36:32.3666804Z         # check that check_estimator doesn't modify the estimator it receives\n2025-03-28T06:36:32.3667086Z     \n2025-03-28T06:36:32.3667500Z         iris = load_iris()\n2025-03-28T06:36:32.3667744Z     \n2025-03-28T06:36:32.3667984Z         for Estimator in [\n2025-03-28T06:36:32.3668245Z             GaussianMixture,\n2025-03-28T06:36:32.3668775Z             LinearRegression,\n2025-03-28T06:36:32.3669031Z             SGDClassifier,\n2025-03-28T06:36:32.3669267Z             PCA,\n2025-03-28T06:36:32.3669511Z             MiniBatchKMeans,\n2025-03-28T06:36:32.3669766Z         ]:\n2025-03-28T06:36:32.3670005Z             # without fitting\n2025-03-28T06:36:32.3670276Z             with ignore_warnings(category=ConvergenceWarning):\n2025-03-28T06:36:32.3670550Z                 est = Estimator()\n2025-03-28T06:36:32.3670803Z                 set_random_state(est)\n2025-03-28T06:36:32.3671081Z                 old_hash = joblib.hash(est)\n2025-03-28T06:36:32.3671475Z >               check_estimator(\n2025-03-28T06:36:32.3671766Z                     est, expected_failed_checks=_get_expected_failed_checks(est)\n2025-03-28T06:36:32.3672037Z                 )\n2025-03-28T06:36:32.3672196Z \n2025-03-28T06:36:32.3672459Z Estimator  = <class 'sklearn.linear_model._base.LinearRegression'>\n2025-03-28T06:36:32.3672931Z est        = LinearRegression()\n2025-03-28T06:36:32.3673224Z iris       = {'data': array([[5.1, 3.5, 1.4, 0.2],\n2025-03-28T06:36:32.3673509Z        [4.9, 3. , 1.4, 0.2],\n2025-03-28T06:36:32.3673763Z        [4.7, 3.2, 1.3, 0.2],\n2025-03-28T06:36:32.3674269Z        [4.6, 3.1, 1.5,... width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n2025-03-28T06:36:32.3674627Z old_hash   = 'fdcbee8ed611695d1e19a9bdabd615ac'\n2025-03-28T06:36:32.3674814Z \n2025-03-28T06:36:32.3675204Z \u001b[1m\u001b[31m/io/sklearn/utils/tests/test_estimator_checks.py\u001b[0m:919: \n2025-03-28T06:36:32.3675540Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3675945Z \u001b[1m\u001b[31m/io/sklearn/utils/_param_validation.py\u001b[0m:218: in wrapper\n2025-03-28T06:36:32.3676248Z     return func(*args, **kwargs)\n2025-03-28T06:36:32.3676516Z         args       = (LinearRegression(),)\n2025-03-28T06:36:32.3676790Z         func       = <function check_estimator at 0xd8591668>\n2025-03-28T06:36:32.3677211Z         func_sig   = <Signature (estimator=None, generate_only=False, *, legacy: 'bool' = True, expected_failed_checks: 'dict[str, str] | N...al['warn'] | None\" = 'warn', on_fail: \"Literal['raise', 'warn'] | None\" = 'raise', callback: 'Callable | None' = None)>\n2025-03-28T06:36:32.3677607Z         global_skip_validation = False\n2025-03-28T06:36:32.3677881Z         kwargs     = {'expected_failed_checks': {}}\n2025-03-28T06:36:32.3678234Z         parameter_constraints = {'callback': [<built-in function callable>, None], 'expected_failed_checks': [<class 'dict'>, None], 'generate_only': ['boolean'], 'legacy': ['boolean'], ...}\n2025-03-28T06:36:32.3678659Z         params     = {'callback': None, 'estimator': LinearRegression(), 'expected_failed_checks': {}, 'generate_only': False, ...}\n2025-03-28T06:36:32.3678985Z         prefer_skip_nested_validation = False\n2025-03-28T06:36:32.3679252Z         to_ignore  = ['self', 'cls']\n2025-03-28T06:36:32.3679647Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:856: in check_estimator\n2025-03-28T06:36:32.3679964Z     check(estimator)\n2025-03-28T06:36:32.3680221Z         callback   = None\n2025-03-28T06:36:32.3680536Z         check      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3680953Z         check_result = {'check_name': 'check_sample_weight_equivalence_on_dense_data', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}\n2025-03-28T06:36:32.3681402Z         estimator  = LinearRegression(positive=True)\n2025-03-28T06:36:32.3681697Z         expected_failed_checks = {}\n2025-03-28T06:36:32.3681899Z         generate_only = False\n2025-03-28T06:36:32.3682090Z         legacy     = True\n2025-03-28T06:36:32.3682266Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3682442Z         on_fail    = 'raise'\n2025-03-28T06:36:32.3682732Z         on_skip    = 'warn'\n2025-03-28T06:36:32.3682922Z         reason     = 'Check is not expected to fail'\n2025-03-28T06:36:32.3683274Z         test_can_fail = False\n2025-03-28T06:36:32.3683596Z         test_results = [{'check_name': 'check_estimator_cloneable', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': F...k_no_attributes_set_in_init', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}, ...]\n2025-03-28T06:36:32.3684106Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1570: in check_sample_weight_equivalence_on_dense_data\n2025-03-28T06:36:32.3684400Z     _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n2025-03-28T06:36:32.3684731Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3684930Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3685217Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:145: in wrapper\n2025-03-28T06:36:32.3685439Z     return fn(*args, **kwargs)\n2025-03-28T06:36:32.3685650Z         args       = ('LinearRegression', LinearRegression(positive=True))\n2025-03-28T06:36:32.3685891Z         fn         = <function _check_sample_weight_equivalence at 0xd8591de8>\n2025-03-28T06:36:32.3686105Z         kwargs     = {'sparse_container': None}\n2025-03-28T06:36:32.3686316Z         self       = _IgnoreWarnings(record=True)\n2025-03-28T06:36:32.3686643Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1566: in _check_sample_weight_equivalence\n2025-03-28T06:36:32.3686902Z     assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n2025-03-28T06:36:32.3687138Z         X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3687418Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3687667Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3687906Z         X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3688173Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3688439Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3688661Z         X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3688871Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3689077Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3689323Z         X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3689588Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3689834Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3690073Z         X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n2025-03-28T06:36:32.3690355Z         0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n2025-03-28T06:36:32.3690606Z         0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n2025-03-28T06:36:32.3690894Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3691216Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691435Z         estimator_repeated = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691653Z         estimator_weighted = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691853Z         method     = 'predict'\n2025-03-28T06:36:32.3692029Z         n_samples  = 15\n2025-03-28T06:36:32.3692223Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3692420Z         rng        = RandomState(MT19937) at 0xCD5304A8\n2025-03-28T06:36:32.3692720Z         sparse_container = None\n2025-03-28T06:36:32.3692923Z         sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n2025-03-28T06:36:32.3693289Z         y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n2025-03-28T06:36:32.3693514Z         y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n2025-03-28T06:36:32.4389210Z        1, 1, 1, 1, 1])\n2025-03-28T06:36:32.4392227Z         y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n2025-03-28T06:36:32.4394205Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:283: in assert_allclose_dense_sparse\n2025-03-28T06:36:32.4395241Z     assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n2025-03-28T06:36:32.4395958Z         atol       = 1e-09\n2025-03-28T06:36:32.4396339Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4396710Z         rtol       = 1e-07\n2025-03-28T06:36:32.4397020Z         x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4397420Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4397769Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4398083Z         y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4398404Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4398711Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4399020Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.4399360Z \n2025-03-28T06:36:32.4399684Z actual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4400047Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4400383Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4400717Z desired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4401016Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4401305Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4401599Z rtol = 1e-07, atol = 1e-09, equal_nan = True\n2025-03-28T06:36:32.4401994Z err_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4402357Z verbose = True\n2025-03-28T06:36:32.4402649Z \n2025-03-28T06:36:32.4403197Z     def assert_allclose(\n2025-03-28T06:36:32.4403533Z         actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n2025-03-28T06:36:32.4403835Z     ):\n2025-03-28T06:36:32.4404125Z         \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n2025-03-28T06:36:32.4404412Z     \n2025-03-28T06:36:32.4404713Z         This variant introspects the least precise floating point dtype\n2025-03-28T06:36:32.4405098Z         in the input argument and automatically sets the relative tolerance\n2025-03-28T06:36:32.4405430Z         parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n2025-03-28T06:36:32.4405731Z         in scikit-learn).\n2025-03-28T06:36:32.4406156Z     \n2025-03-28T06:36:32.4406440Z         `atol` is always left to 0. by default. It should be adjusted manually\n2025-03-28T06:36:32.4406762Z         to an assertion-specific value in case there are null values expected\n2025-03-28T06:36:32.4407464Z         in `desired`.\n2025-03-28T06:36:32.4407766Z     \n2025-03-28T06:36:32.4408043Z         The aggregate tolerance is `atol + rtol * abs(desired)`.\n2025-03-28T06:36:32.4408313Z     \n2025-03-28T06:36:32.4408563Z         Parameters\n2025-03-28T06:36:32.4408818Z         ----------\n2025-03-28T06:36:32.4409095Z         actual : array_like\n2025-03-28T06:36:32.4409358Z             Array obtained.\n2025-03-28T06:36:32.4409820Z         desired : array_like\n2025-03-28T06:36:32.4410093Z             Array desired.\n2025-03-28T06:36:32.4410364Z         rtol : float, optional, default=None\n2025-03-28T06:36:32.4410653Z             Relative tolerance.\n2025-03-28T06:36:32.4410943Z             If None, it is set based on the provided arrays' dtypes.\n2025-03-28T06:36:32.4411236Z         atol : float, optional, default=0.\n2025-03-28T06:36:32.4411512Z             Absolute tolerance.\n2025-03-28T06:36:32.4411809Z         equal_nan : bool, optional, default=True\n2025-03-28T06:36:32.4412101Z             If True, NaNs will compare equal.\n2025-03-28T06:36:32.4412731Z         err_msg : str, optional, default=''\n2025-03-28T06:36:32.4413109Z             The error message to be printed in case of failure.\n2025-03-28T06:36:32.4413412Z         verbose : bool, optional, default=True\n2025-03-28T06:36:32.4413734Z             If True, the conflicting values are appended to the error message.\n2025-03-28T06:36:32.4414013Z     \n2025-03-28T06:36:32.4414256Z         Raises\n2025-03-28T06:36:32.4414514Z         ------\n2025-03-28T06:36:32.4414771Z         AssertionError\n2025-03-28T06:36:32.4415083Z             If actual and desired are not equal up to specified precision.\n2025-03-28T06:36:32.4415491Z     \n2025-03-28T06:36:32.4415753Z         See Also\n2025-03-28T06:36:32.4416185Z         --------\n2025-03-28T06:36:32.4416466Z         numpy.testing.assert_allclose\n2025-03-28T06:36:32.4416900Z     \n2025-03-28T06:36:32.4417154Z         Examples\n2025-03-28T06:36:32.4417419Z         --------\n2025-03-28T06:36:32.4417685Z         >>> import numpy as np\n2025-03-28T06:36:32.4418011Z         >>> from sklearn.utils._testing import assert_allclose\n2025-03-28T06:36:32.4418314Z         >>> x = [1e-5, 1e-3, 1e-1]\n2025-03-28T06:36:32.4418599Z         >>> y = np.arccos(np.cos(x))\n2025-03-28T06:36:32.4418899Z         >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n2025-03-28T06:36:32.4419379Z         >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n2025-03-28T06:36:32.4420007Z         >>> assert_allclose(a, 1e-5)\n2025-03-28T06:36:32.4420262Z         \"\"\"\n2025-03-28T06:36:32.4420502Z         dtypes = []\n2025-03-28T06:36:32.4420743Z     \n2025-03-28T06:36:32.4421010Z         actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n2025-03-28T06:36:32.4421324Z         dtypes = [actual.dtype, desired.dtype]\n2025-03-28T06:36:32.4421576Z     \n2025-03-28T06:36:32.4421816Z         if rtol is None:\n2025-03-28T06:36:32.4422102Z             rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n2025-03-28T06:36:32.4422404Z             rtol = max(rtols)\n2025-03-28T06:36:32.4422867Z     \n2025-03-28T06:36:32.4423151Z >       np_assert_allclose(\n2025-03-28T06:36:32.4423412Z             actual,\n2025-03-28T06:36:32.4423660Z             desired,\n2025-03-28T06:36:32.4423929Z             rtol=rtol,\n2025-03-28T06:36:32.4424278Z             atol=atol,\n2025-03-28T06:36:32.4424545Z             equal_nan=equal_nan,\n2025-03-28T06:36:32.4424804Z             err_msg=err_msg,\n2025-03-28T06:36:32.4425069Z             verbose=verbose,\n2025-03-28T06:36:32.4425335Z         )\n2025-03-28T06:36:32.4425695Z \u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n2025-03-28T06:36:32.4426095Z \u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-07, atol=1e-09\u001b[0m\n2025-03-28T06:36:32.4426645Z \u001b[1m\u001b[31mE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\u001b[0m\n2025-03-28T06:36:32.4427542Z \u001b[1m\u001b[31mE       Mismatched elements: 6 / 15 (40%)\u001b[0m\n2025-03-28T06:36:32.4427985Z \u001b[1m\u001b[31mE       Max absolute difference among violations: 2.51014256\u001b[0m\n2025-03-28T06:36:32.4428409Z \u001b[1m\u001b[31mE       Max relative difference among violations: 2.17024526\u001b[0m\n2025-03-28T06:36:32.4428858Z \u001b[1m\u001b[31mE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\u001b[0m\n2025-03-28T06:36:32.4429327Z \u001b[1m\u001b[31mE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\u001b[0m\n2025-03-28T06:36:32.4429765Z \u001b[1m\u001b[31mE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\u001b[0m\n2025-03-28T06:36:32.4430365Z \u001b[1m\u001b[31mE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\u001b[0m\n2025-03-28T06:36:32.4430805Z \u001b[1m\u001b[31mE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\u001b[0m\n2025-03-28T06:36:32.4431250Z \u001b[1m\u001b[31mE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\u001b[0m\n2025-03-28T06:36:32.4431622Z \u001b[1m\u001b[31mE              1.709333, 1.      , 1.      ])\u001b[0m\n2025-03-28T06:36:32.4431810Z \n2025-03-28T06:36:32.4432416Z actual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4433326Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4433865Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4434473Z atol       = 1e-09\n2025-03-28T06:36:32.4434795Z desired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4435122Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4435605Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4435932Z dtypes     = [dtype('float64'), dtype('float64')]\n2025-03-28T06:36:32.4436253Z equal_nan  = True\n2025-03-28T06:36:32.4436639Z err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4437185Z rtol       = 1e-07\n2025-03-28T06:36:32.4439658Z verbose    = True\n2025-03-28T06:36:32.4439971Z \n2025-03-28T06:36:32.4440407Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:237: AssertionError\n```\n\n</details>\n\nLooking at the software runtime info of each I only see two differences:\n\n- the pip version;\n- the CPU model.\n\nAll other dependencies seem to match, including the openblas version inspected by threadpoolctl.\n\nEDIT: this is wrong, the scipy version is not the same and I missed it.",
  "pr_number": 31101,
  "pr_title": "MAINT XFAIL check_sample_weight_equivalence for LinearRegression on 32 bit CI",
  "gold_patch": "diff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex 0e2151220f396..e619deab1c93e 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -176,7 +176,7 @@\n from sklearn.utils import all_estimators\n from sklearn.utils._tags import get_tags\n from sklearn.utils._testing import SkipTest\n-from sklearn.utils.fixes import parse_version, sp_base_version\n+from sklearn.utils.fixes import _IS_32BIT, parse_version, sp_base_version\n \n CROSS_DECOMPOSITION = [\"PLSCanonical\", \"PLSRegression\", \"CCA\", \"PLSSVD\"]\n \n@@ -1283,5 +1283,17 @@ def _get_expected_failed_checks(estimator):\n                     \"check_dataframe_column_names_consistency\": \"FIXME\",\n                 }\n             )\n+    if type(estimator) == LinearRegression:\n+        if _IS_32BIT:\n+            failed_checks.update(\n+                {\n+                    \"check_sample_weight_equivalence_on_dense_data\": (\n+                        \"Issue #31098. Fails on 32-bit platforms with recent scipy.\"\n+                    ),\n+                    \"check_sample_weight_equivalence_on_sparse_data\": (\n+                        \"Issue #31098. Fails on 32-bit platforms with recent scipy.\"\n+                    ),\n+                }\n+            )\n \n     return failed_checks\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/utils/_test_common/instance_generator.py"
  ],
  "difficulty": "easy",
  "created_at": "2025-03-28T15:41:44Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31101",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/31098"
}