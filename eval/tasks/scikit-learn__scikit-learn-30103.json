{
  "id": "scikit-learn__scikit-learn-30103",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "ed202901cde3f8eb9cc68d845a58fc4cd0f8c239",
  "issue_number": 30079,
  "issue_title": "`roc_auc_score`: incorrect result after merging #27412",
  "issue_body": "### Describe the bug\r\n\r\nWhen all data instances come from the same class, #27412 changed the behaviour of `roc_auc_score` to return `0.0` instead of raising an exception. The argument for the change was the consistency with PR curves. I believe that this result is incorrect, or, at least, not correct under all interpretations. Even if only the latter: it is not worth breaking backwards compatibility for a change that is a matter of discussion - in particular if the change is masking an error by returning a (dubious) \"default\".\r\n\r\n### Arguments\r\n\r\nThe issue arises when all data instances belong to the same class. While AUC is, literally, the area under the ROC curve, we interpret it as the score reflecting the quality of ranking, which is also related to the Gini index and Mann-Whitney U-statistics, as also described in sklearn documentation.\r\n\r\n- Under geometric interpretation, if all data comes from the same class, the curve may go either straight right or straight up, depending upon the class, so it can be either 0 or 1 (or 0.5), not (necessarily) 0.0.\r\n- Under statistical interpretation, the AUC is undefined. AUC is *the probability that for a random pair of instances from different classes, the score assigned to the instance from the positive class is higher than the score assigned to the instance from the negative class*. This measure cannot be computed for data from a single class and is thus undefined. The function should return `np.nan` or raise an exception (as it used to).\r\n- Furthermore (and related to the previous point), for any `y_true` and `y_score`, it holds that\r\n\r\n```python\r\n    auc(y_true, y_score) \\\r\n    == auc(1 - y_true, 1 - y_score) \\\r\n    == 1 - auc(y_true, 1 - y_score) \\\r\n    == 1 - auc(1 - y_true, y_score)\r\n```\r\n\r\nFlipping either labels or scores reverses the curve and the AUC, and flipping both keeps AUC the same. Before #27412, `auc_roc_score` returned an exception when the result cannot be computed. Now it returns 0.0, which leads to inconsistency when flipping classes or scores (or both).\r\n\r\n### Suggestion\r\n\r\nI suggest reverting the change at https://github.com/scikit-learn/scikit-learn/pull/27412/files#diff-4eb3c023f8a3f088d62208f6adbd02b6df5196de2257ccd228dffc972c964634R375, that is, raising an exception instead of returning an (arbitrary, in some contexts) number. Alternatively, the function could return `np.nan`, but it is better to have an explicit exception and, above all, to keep the backward compatibility with behaviour that was not wrong.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.metrics import roc_auc_score\r\nimport numpy as np\r\n\r\ny_true = np.array([1, 1, 1, 1, 1])\r\ny_score = np.array([0.8, 0.6, 0.5, 0.3, 0.2])\r\nprint(roc_auc_score(y_true, y_score))\r\n```\r\n\r\n### Expected Results\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/janez/miniforge3/envs/o3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/janez/miniforge3/envs/o3/lib/python3.11/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\r\n    return _average_binary_score(\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/janez/miniforge3/envs/o3/lib/python3.11/site-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\r\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/janez/miniforge3/envs/o3/lib/python3.11/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\r\n    raise ValueError(\r\nValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\r\n```\r\n\r\n### Actual Results\r\n\r\n0.0\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.10 | packaged by conda-forge | (main, Sep 10 2024, 10:57:35) [Clang 17.0.6 ]\r\nexecutable: /Users/janez/miniforge3/envs/o3edge/bin/python\r\n   machine: macOS-14.6.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 73.0.1\r\n        numpy: 1.26.4\r\n        scipy: 1.15.0.dev0\r\n       Cython: 3.0.11\r\n       pandas: 3.0.0.dev0+1524.g23c497bb2f\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: /Users/janez/miniforge3/envs/o3edge/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: /Users/janez/miniforge3/envs/o3edge/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib\r\n        version: None\r\n```\r\n",
  "pr_number": 30103,
  "pr_title": "FIX roc_auc_curve: Return np.nan instead of 0.0 for single class",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/27412.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/27412.fix.rst\nindex b62c1c2b91790..350bd92a19478 100644\n--- a/doc/whats_new/upcoming_changes/sklearn.metrics/27412.fix.rst\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/27412.fix.rst\n@@ -1,3 +1,3 @@\n-- :func:`metrics.roc_auc_score` will now correctly return 0.0 and\n+- :func:`metrics.roc_auc_score` will now correctly return np.nan and\n   warn user if only one class is present in the labels.\n-  By :user:`Gleb Levitski <glevv>`\n+  By :user:`Gleb Levitski <glevv>` and :user:`Janez Dem\u0161ar <janezd>`\ndiff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/30013.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/30013.fix.rst\nnew file mode 100644\nindex 0000000000000..4cee2ec523fb8\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/30013.fix.rst\n@@ -0,0 +1,3 @@\n+- :func:`metrics.roc_auc_score` will now correctly return np.nan and\n+  warn user if only one class is present in the labels.\n+  By :user:`Gleb Levitski <glevv>` and :user:`Janez Dem\u0161ar <janezd>`\n\\ No newline at end of file\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0d24a68bf464b..958ab3be9cc0d 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -375,12 +375,11 @@ def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=None):\n         warnings.warn(\n             (\n                 \"Only one class is present in y_true. ROC AUC score \"\n-                \"is not defined in that case. The score is set to \"\n-                \"0.0.\"\n+                \"is not defined in that case.\"\n             ),\n             UndefinedMetricWarning,\n         )\n-        return 0.0\n+        return np.nan\n \n     fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight)\n     if max_fpr is None or max_fpr == 1:\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex f70f0bfa50137..e6abc8c433013 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -1,3 +1,4 @@\n+import math\n from functools import partial\n from inspect import signature\n from itertools import chain, permutations, product\n@@ -843,9 +844,9 @@ def test_format_invariance_with_1d_vectors(name):\n         ):\n             if \"roc_auc\" in name:\n                 # for consistency between the `roc_cuve` and `roc_auc_score`\n-                # 0.0 is returned and an `UndefinedMetricWarning` is raised\n+                # np.nan is returned and an `UndefinedMetricWarning` is raised\n                 with pytest.warns(UndefinedMetricWarning):\n-                    assert metric(y1_row, y2_row) == pytest.approx(0.0)\n+                    assert math.isnan(metric(y1_row, y2_row))\n             else:\n                 with pytest.raises(ValueError):\n                     metric(y1_row, y2_row)\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex 7e7d784522524..c92fee002595f 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -1,3 +1,4 @@\n+import math\n import re\n \n import numpy as np\n@@ -370,7 +371,8 @@ def test_roc_curve_toydata():\n         \"ROC AUC score is not defined in that case.\"\n     )\n     with pytest.warns(UndefinedMetricWarning, match=expected_message):\n-        roc_auc_score(y_true, y_score)\n+        auc = roc_auc_score(y_true, y_score)\n+    assert math.isnan(auc)\n \n     # case with no negative samples\n     y_true = [1, 1]\n@@ -388,7 +390,8 @@ def test_roc_curve_toydata():\n         \"ROC AUC score is not defined in that case.\"\n     )\n     with pytest.warns(UndefinedMetricWarning, match=expected_message):\n-        roc_auc_score(y_true, y_score)\n+        auc = roc_auc_score(y_true, y_score)\n+    assert math.isnan(auc)\n \n     # Multi-label classification task\n     y_true = np.array([[0, 1], [0, 1]])\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_ranking.py",
    "sklearn/metrics/tests/test_common.py",
    "sklearn/metrics/tests/test_ranking.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-10-18T18:12:53Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30103",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30079"
}