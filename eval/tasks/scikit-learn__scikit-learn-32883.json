{
  "id": "scikit-learn__scikit-learn-32883",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "7f0900c265936eac9a89bba37eb19ee66208d46a",
  "issue_number": 30134,
  "issue_title": "FEA add `confusion_matrix_at_thresholds`",
  "issue_body": "<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https://github.com/blog/1506-closing-issues-via-pull-requests\r\n-->\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nFixes #16470 \r\n\r\n#### Any other comments?\r\n* In `sklearn/metrics/_ranking.py`, changed the name of the function `_binary_clf_curve` to `binary_classifcation_curve` without changing the body. I also changed test functions like `test_binary_clf_curve_multiclass_error` without changing the body\r\n* `det_curve`, `roc_curve` and `precision_recall_curve` call this function, so I updated the name of the function in the body\r\n* I added examples in the docstring of the function\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttps://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n",
  "pr_number": 32883,
  "pr_title": "TST Add `confusion_matrix_at_thresholds` to common tests",
  "gold_patch": "diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 34bfbc8b26252..43423a05702bc 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -18,6 +18,7 @@\n     classification_report,\n     cohen_kappa_score,\n     confusion_matrix,\n+    confusion_matrix_at_thresholds,\n     coverage_error,\n     d2_absolute_error_score,\n     d2_brier_score,\n@@ -156,17 +157,13 @@\n     \"balanced_accuracy_score\": balanced_accuracy_score,\n     \"adjusted_balanced_accuracy_score\": partial(balanced_accuracy_score, adjusted=True),\n     \"unnormalized_accuracy_score\": partial(accuracy_score, normalize=False),\n-    # `confusion_matrix` returns absolute values and hence behaves unnormalized\n-    # . Naming it with an unnormalized_ prefix is necessary for this module to\n-    # skip sample_weight scaling checks which will fail for unnormalized\n-    # metrics.\n-    \"unnormalized_confusion_matrix\": confusion_matrix,\n+    \"confusion_matrix\": confusion_matrix,\n     \"normalized_confusion_matrix\": lambda *args, **kwargs: (\n         confusion_matrix(*args, **kwargs).astype(\"float\")\n         / confusion_matrix(*args, **kwargs).sum(axis=1)[:, np.newaxis]\n     ),\n-    \"unnormalized_multilabel_confusion_matrix\": multilabel_confusion_matrix,\n-    \"unnormalized_multilabel_confusion_matrix_sample\": partial(\n+    \"multilabel_confusion_matrix\": multilabel_confusion_matrix,\n+    \"multilabel_confusion_matrix_sample\": partial(\n         multilabel_confusion_matrix, samplewise=True\n     ),\n     \"hamming_loss\": hamming_loss,\n@@ -240,6 +237,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n \n \n CURVE_METRICS = {\n+    \"confusion_matrix_at_thresholds\": confusion_matrix_at_thresholds,\n     \"roc_curve\": roc_curve,\n     \"precision_recall_curve\": precision_recall_curve_padded_thresholds,\n     \"det_curve\": det_curve,\n@@ -305,7 +303,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"samples_recall_score\",\n     \"samples_jaccard_score\",\n     \"coverage_error\",\n-    \"unnormalized_multilabel_confusion_matrix_sample\",\n+    \"multilabel_confusion_matrix_sample\",\n     \"label_ranking_loss\",\n     \"label_ranking_average_precision_score\",\n     \"dcg_score\",\n@@ -327,6 +325,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"f2_score\",\n     \"f0.5_score\",\n     # curves\n+    \"confusion_matrix_at_thresholds\",\n     \"roc_curve\",\n     \"precision_recall_curve\",\n     \"det_curve\",\n@@ -356,6 +355,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n \n # Metrics with a \"pos_label\" argument\n METRICS_WITH_POS_LABEL = {\n+    \"confusion_matrix_at_thresholds\",\n     \"roc_curve\",\n     \"precision_recall_curve\",\n     \"det_curve\",\n@@ -377,7 +377,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n # TODO: Handle multi_class metrics that has a labels argument as well as a\n # decision function argument. e.g hinge_loss\n METRICS_WITH_LABELS = {\n-    \"unnormalized_confusion_matrix\",\n+    \"confusion_matrix\",\n     \"normalized_confusion_matrix\",\n     \"roc_curve\",\n     \"precision_recall_curve\",\n@@ -406,8 +406,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"macro_precision_score\",\n     \"macro_recall_score\",\n     \"macro_jaccard_score\",\n-    \"unnormalized_multilabel_confusion_matrix\",\n-    \"unnormalized_multilabel_confusion_matrix_sample\",\n+    \"multilabel_confusion_matrix\",\n+    \"multilabel_confusion_matrix_sample\",\n     \"cohen_kappa_score\",\n     \"log_loss\",\n     \"d2_log_loss_score\",\n@@ -470,7 +470,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"micro_precision_score\",\n     \"micro_recall_score\",\n     \"micro_jaccard_score\",\n-    \"unnormalized_multilabel_confusion_matrix\",\n+    \"multilabel_confusion_matrix\",\n     \"samples_f0.5_score\",\n     \"samples_f1_score\",\n     \"samples_f2_score\",\n@@ -538,8 +538,9 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"adjusted_balanced_accuracy_score\",\n     \"explained_variance_score\",\n     \"r2_score\",\n-    \"unnormalized_confusion_matrix\",\n+    \"confusion_matrix\",\n     \"normalized_confusion_matrix\",\n+    \"confusion_matrix_at_thresholds\",\n     \"roc_curve\",\n     \"precision_recall_curve\",\n     \"det_curve\",\n@@ -552,7 +553,7 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"weighted_f2_score\",\n     \"weighted_precision_score\",\n     \"weighted_jaccard_score\",\n-    \"unnormalized_multilabel_confusion_matrix\",\n+    \"multilabel_confusion_matrix\",\n     \"macro_f0.5_score\",\n     \"macro_f2_score\",\n     \"macro_precision_score\",\n@@ -575,6 +576,19 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"weighted_ovo_roc_auc\",\n }\n \n+WEIGHT_SCALE_DEPENDENT_METRICS = {\n+    # 'confusion_matrix' metrics returns absolute `tps`, `fps` etc values, which\n+    # are scaled by weights, so will vary e.g., scaling by 3 will result in 3 * `tps`\n+    \"confusion_matrix\",\n+    \"confusion_matrix_at_thresholds\",\n+    \"multilabel_confusion_matrix\",\n+    \"multilabel_confusion_matrix_sample\",\n+    # Metrics where we set `normalize=False`\n+    \"unnormalized_accuracy_score\",\n+    \"unnormalized_zero_one_loss\",\n+    \"unnormalized_log_loss\",\n+}\n+\n METRICS_REQUIRE_POSITIVE_Y = {\n     \"mean_poisson_deviance\",\n     \"mean_gamma_deviance\",\n@@ -1612,12 +1626,15 @@ def check_sample_weight_invariance(name, metric, y1, y2, sample_weight=None):\n         % (weighted_score_zeroed, weighted_score_subset, name),\n     )\n \n-    if not name.startswith(\"unnormalized\"):\n-        # check that the score is invariant under scaling of the weights by a\n-        # common factor\n-        # Due to numerical instability of floating points in `cumulative_sum` in\n-        # `median_absolute_error`, it is not always equivalent when scaling by a float.\n-        scaling_values = [2] if name == \"median_absolute_error\" else [2, 0.3]\n+    # Check the score is invariant under scaling of weights by a constant factor\n+    if name not in WEIGHT_SCALE_DEPENDENT_METRICS:\n+        # Numerical instability of floating points in `cumulative_sum` in\n+        # `median_absolute_error`, and in `diff` when in calculating collinear points\n+        # and points in between to drop `roc_curve` means they are not always\n+        # equivalent when scaling by a float.\n+        scaling_values = (\n+            [2] if name in {\"median_absolute_error\", \"roc_curve\"} else [2, 0.3]\n+        )\n         for scaling in scaling_values:\n             assert_allclose(\n                 weighted_score,\n@@ -1715,7 +1732,7 @@ def test_binary_sample_weight_invariance(name):\n     y_pred = random_state.randint(0, 2, size=(n_samples,))\n     y_score = random_state.random_sample(size=(n_samples,))\n     metric = ALL_METRICS[name]\n-    if name in CONTINUOUS_CLASSIFICATION_METRICS:\n+    if name in (CONTINUOUS_CLASSIFICATION_METRICS | CURVE_METRICS.keys()):\n         check_sample_weight_invariance(name, metric, y_true, y_score)\n     else:\n         check_sample_weight_invariance(name, metric, y_true, y_pred)\n@@ -1816,7 +1833,7 @@ def test_no_averaging_labels():\n \n \n @pytest.mark.parametrize(\n-    \"name\", sorted(MULTILABELS_METRICS - {\"unnormalized_multilabel_confusion_matrix\"})\n+    \"name\", sorted(MULTILABELS_METRICS - {\"multilabel_confusion_matrix\"})\n )\n def test_multilabel_label_permutations_invariance(name):\n     random_state = check_random_state(0)\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/tests/test_common.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-12-11T02:21:30Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32883",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/30134"
}