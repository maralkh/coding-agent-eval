{
  "id": "scikit-learn__scikit-learn-30380",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "2707099b23a0a8580731553629566c1182d26f48",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30380,
  "pr_title": "Propagate warnings to all workers in joblib",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.utils/30380.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.utils/30380.enhancement.rst\nnew file mode 100644\nindex 0000000000000..bd1eaf9213257\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.utils/30380.enhancement.rst\n@@ -0,0 +1,2 @@\n+- Warning filters from the main process are propagated to joblib workers.\n+  By `Thomas Fan`_\ndiff --git a/sklearn/utils/parallel.py b/sklearn/utils/parallel.py\nindex da7ad69ffc3bf..8cfc78ebcd34a 100644\n--- a/sklearn/utils/parallel.py\n+++ b/sklearn/utils/parallel.py\n@@ -21,10 +21,10 @@\n _threadpool_controller = None\n \n \n-def _with_config(delayed_func, config):\n+def _with_config_and_warning_filters(delayed_func, config, warning_filters):\n     \"\"\"Helper function that intends to attach a config to a delayed function.\"\"\"\n-    if hasattr(delayed_func, \"with_config\"):\n-        return delayed_func.with_config(config)\n+    if hasattr(delayed_func, \"with_config_and_warning_filters\"):\n+        return delayed_func.with_config_and_warning_filters(config, warning_filters)\n     else:\n         warnings.warn(\n             (\n@@ -70,11 +70,16 @@ def __call__(self, iterable):\n         # in a different thread depending on the backend and on the value of\n         # pre_dispatch and n_jobs.\n         config = get_config()\n-        iterable_with_config = (\n-            (_with_config(delayed_func, config), args, kwargs)\n+        warning_filters = warnings.filters\n+        iterable_with_config_and_warning_filters = (\n+            (\n+                _with_config_and_warning_filters(delayed_func, config, warning_filters),\n+                args,\n+                kwargs,\n+            )\n             for delayed_func, args, kwargs in iterable\n         )\n-        return super().__call__(iterable_with_config)\n+        return super().__call__(iterable_with_config_and_warning_filters)\n \n \n # remove when https://github.com/joblib/joblib/issues/1071 is fixed\n@@ -118,13 +123,15 @@ def __init__(self, function):\n         self.function = function\n         update_wrapper(self, self.function)\n \n-    def with_config(self, config):\n+    def with_config_and_warning_filters(self, config, warning_filters):\n         self.config = config\n+        self.warning_filters = warning_filters\n         return self\n \n     def __call__(self, *args, **kwargs):\n-        config = getattr(self, \"config\", None)\n-        if config is None:\n+        config = getattr(self, \"config\", {})\n+        warning_filters = getattr(self, \"warning_filters\", [])\n+        if not config or not warning_filters:\n             warnings.warn(\n                 (\n                     \"`sklearn.utils.parallel.delayed` should be used with\"\n@@ -134,8 +141,9 @@ def __call__(self, *args, **kwargs):\n                 ),\n                 UserWarning,\n             )\n-            config = {}\n-        with config_context(**config):\n+\n+        with config_context(**config), warnings.catch_warnings():\n+            warnings.filters = warning_filters\n             return self.function(*args, **kwargs)\n \n \ndiff --git a/sklearn/utils/tests/test_parallel.py b/sklearn/utils/tests/test_parallel.py\nindex 3a359ef8690e5..2f5025afe0662 100644\n--- a/sklearn/utils/tests/test_parallel.py\n+++ b/sklearn/utils/tests/test_parallel.py\n@@ -1,4 +1,5 @@\n import time\n+import warnings\n \n import joblib\n import numpy as np\n@@ -9,6 +10,7 @@\n from sklearn.compose import make_column_transformer\n from sklearn.datasets import load_iris\n from sklearn.ensemble import RandomForestClassifier\n+from sklearn.exceptions import ConvergenceWarning\n from sklearn.model_selection import GridSearchCV\n from sklearn.pipeline import make_pipeline\n from sklearn.preprocessing import StandardScaler\n@@ -98,3 +100,54 @@ def transform(self, X, y=None):\n         search_cv.fit(iris.data, iris.target)\n \n     assert not np.isnan(search_cv.cv_results_[\"mean_test_score\"]).any()\n+\n+\n+def raise_warning():\n+    warnings.warn(\"Convergence warning\", ConvergenceWarning)\n+\n+\n+@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n+@pytest.mark.parametrize(\"backend\", [\"loky\", \"threading\", \"multiprocessing\"])\n+def test_filter_warning_propagates(n_jobs, backend):\n+    \"\"\"Check warning propagates to the job.\"\"\"\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"error\", category=ConvergenceWarning)\n+\n+        with pytest.raises(ConvergenceWarning):\n+            Parallel(n_jobs=n_jobs, backend=backend)(\n+                delayed(raise_warning)() for _ in range(2)\n+            )\n+\n+\n+def get_warnings():\n+    return warnings.filters\n+\n+\n+def test_check_warnings_threading():\n+    \"\"\"Check that warnings filters are set correctly in the threading backend.\"\"\"\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"error\", category=ConvergenceWarning)\n+\n+        filters = warnings.filters\n+        assert (\"error\", None, ConvergenceWarning, None, 0) in filters\n+\n+        all_warnings = Parallel(n_jobs=2, backend=\"threading\")(\n+            delayed(get_warnings)() for _ in range(2)\n+        )\n+\n+        assert all(w == filters for w in all_warnings)\n+\n+\n+def test_filter_warning_propagates_no_side_effect_with_loky_backend():\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"error\", category=ConvergenceWarning)\n+\n+        Parallel(n_jobs=2, backend=\"loky\")(delayed(time.sleep)(0) for _ in range(10))\n+\n+        # Since loky workers are reused, make sure that inside the loky workers,\n+        # warnings filters have been reset to their original value. Using joblib\n+        # directly should not turn ConvergenceWarning into an error.\n+        joblib.Parallel(n_jobs=2, backend=\"loky\")(\n+            joblib.delayed(warnings.warn)(\"Convergence warning\", ConvergenceWarning)\n+            for _ in range(10)\n+        )\n",
  "fail_to_pass": [
    "test_filter_warning_propagates",
    "test_check_warnings_threading",
    "test_filter_warning_propagates_no_side_effect_with_loky_backend"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/utils/parallel.py",
    "sklearn/utils/tests/test_parallel.py"
  ],
  "difficulty": "medium",
  "created_at": "2024-12-01T15:46:47Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30380",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}