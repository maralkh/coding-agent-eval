{
  "id": "scikit-learn__scikit-learn-30395",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "5035b6df2b99924ae753b7f10c894b5bd0214726",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30395,
  "pr_title": "ENH Add array api support for precision, recall and fbeta_score",
  "gold_patch": "diff --git a/doc/modules/array_api.rst b/doc/modules/array_api.rst\nindex 82d77f60afc9a..a1aae54771ef1 100644\n--- a/doc/modules/array_api.rst\n+++ b/doc/modules/array_api.rst\n@@ -135,6 +135,7 @@ Metrics\n - :func:`sklearn.metrics.d2_tweedie_score`\n - :func:`sklearn.metrics.explained_variance_score`\n - :func:`sklearn.metrics.f1_score`\n+- :func:`sklearn.metrics.fbeta_score`\n - :func:`sklearn.metrics.max_error`\n - :func:`sklearn.metrics.mean_absolute_error`\n - :func:`sklearn.metrics.mean_absolute_percentage_error`\n@@ -156,8 +157,10 @@ Metrics\n - :func:`sklearn.metrics.pairwise.polynomial_kernel`\n - :func:`sklearn.metrics.pairwise.rbf_kernel` (see :ref:`device_support_for_float64`)\n - :func:`sklearn.metrics.pairwise.sigmoid_kernel`\n+- :func:`sklearn.metrics.precision_score`\n - :func:`sklearn.metrics.precision_recall_fscore_support`\n - :func:`sklearn.metrics.r2_score`\n+- :func:`sklearn.metrics.recall_score`\n - :func:`sklearn.metrics.root_mean_squared_error`\n - :func:`sklearn.metrics.root_mean_squared_log_error`\n - :func:`sklearn.metrics.zero_one_loss`\ndiff --git a/doc/whats_new/upcoming_changes/array-api/30395.feature.rst b/doc/whats_new/upcoming_changes/array-api/30395.feature.rst\nnew file mode 100644\nindex 0000000000000..739ea20071dfc\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/array-api/30395.feature.rst\n@@ -0,0 +1,4 @@\n+- :func:`sklearn.metrics.fbeta_score`,\n+  :func:`sklearn.metrics.precision_score` and\n+  :func:`sklearn.metrics.recall_score` now support Array API compatible inputs.\n+  By :user:`Omar Salman <OmarManzoor>`\ndiff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex dc9252c2c9fda..f0035c4e73e9c 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -32,6 +32,7 @@\n     _count_nonzero,\n     _find_matching_floating_dtype,\n     _is_numpy_namespace,\n+    _max_precision_float_dtype,\n     _searchsorted,\n     _setdiff1d,\n     _tolist,\n@@ -1562,7 +1563,7 @@ def _prf_divide(\n \n     # build appropriate warning\n     if metric in warn_for:\n-        _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n+        _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n \n     return result\n \n@@ -1842,7 +1843,7 @@ def precision_recall_fscore_support(\n     pred_sum = tp_sum + MCM[:, 0, 1]\n     true_sum = tp_sum + MCM[:, 1, 0]\n \n-    xp, _ = get_namespace(y_true, y_pred)\n+    xp, _, device_ = get_namespace_and_device(y_true, y_pred)\n     if average == \"micro\":\n         tp_sum = xp.reshape(xp.sum(tp_sum), (1,))\n         pred_sum = xp.reshape(xp.sum(pred_sum), (1,))\n@@ -1869,9 +1870,16 @@ def precision_recall_fscore_support(\n         # score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n         # Therefore, we can express the score in terms of confusion matrix entries as:\n         # score = (1 + beta**2) * tp / ((1 + beta**2) * tp + beta**2 * fn + fp)\n-        denom = beta2 * true_sum + pred_sum\n+\n+        # Array api strict requires all arrays to be of the same type so we\n+        # need to convert true_sum, pred_sum and tp_sum to the max supported\n+        # float dtype because beta2 is a float\n+        max_float_type = _max_precision_float_dtype(xp=xp, device=device_)\n+        denom = beta2 * xp.astype(true_sum, max_float_type) + xp.astype(\n+            pred_sum, max_float_type\n+        )\n         f_score = _prf_divide(\n-            (1 + beta2) * tp_sum,\n+            (1 + beta2) * xp.astype(tp_sum, max_float_type),\n             denom,\n             \"f-score\",\n             \"true nor predicted\",\n@@ -1889,7 +1897,6 @@ def precision_recall_fscore_support(\n         weights = None\n \n     if average is not None:\n-        assert average != \"binary\" or precision.shape[0] == 1\n         precision = float(_nanaverage(precision, weights=weights))\n         recall = float(_nanaverage(recall, weights=weights))\n         f_score = float(_nanaverage(f_score, weights=weights))\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex ef8e6ebb2ac2a..7e3758cd76654 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -1898,6 +1898,7 @@ def check_array_api_multiclass_classification_metric(\n \n     additional_params = {\n         \"average\": (\"micro\", \"macro\", \"weighted\"),\n+        \"beta\": (0.2, 0.5, 0.8),\n     }\n     metric_kwargs_combinations = _get_metric_kwargs_for_array_api_testing(\n         metric=metric,\n@@ -1937,6 +1938,7 @@ def check_array_api_multilabel_classification_metric(\n \n     additional_params = {\n         \"average\": (\"micro\", \"macro\", \"weighted\"),\n+        \"beta\": (0.2, 0.5, 0.8),\n     }\n     metric_kwargs_combinations = _get_metric_kwargs_for_array_api_testing(\n         metric=metric,\n@@ -2100,11 +2102,25 @@ def check_array_api_metric_pairwise(metric, array_namespace, device, dtype_name)\n         check_array_api_multiclass_classification_metric,\n         check_array_api_multilabel_classification_metric,\n     ],\n+    fbeta_score: [\n+        check_array_api_multiclass_classification_metric,\n+        check_array_api_multilabel_classification_metric,\n+    ],\n     multilabel_confusion_matrix: [\n         check_array_api_binary_classification_metric,\n         check_array_api_multiclass_classification_metric,\n         check_array_api_multilabel_classification_metric,\n     ],\n+    precision_score: [\n+        check_array_api_binary_classification_metric,\n+        check_array_api_multiclass_classification_metric,\n+        check_array_api_multilabel_classification_metric,\n+    ],\n+    recall_score: [\n+        check_array_api_binary_classification_metric,\n+        check_array_api_multiclass_classification_metric,\n+        check_array_api_multilabel_classification_metric,\n+    ],\n     zero_one_loss: [\n         check_array_api_binary_classification_metric,\n         check_array_api_multiclass_classification_metric,\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/tests/test_common.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-12-03T05:45:18Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30395",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}