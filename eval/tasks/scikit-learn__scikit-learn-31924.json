{
  "id": "scikit-learn__scikit-learn-31924",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "eb6dd0a9f8bf446b8adeb968d19ff978271df0e8",
  "issue_number": 31872,
  "issue_title": "Strange normalization of semi-supervised label propagation in `_build_graph`",
  "issue_body": "The method `_build_graph` on the `LabelPropagation` class in `sklearn/semi_supervised/_label_propagation.py` [(line 455)](https://github.com/scikit-learn/scikit-learn/blob/7d1d96819172e2a7c826f04c68b9d93188cf6a92/sklearn/semi_supervised/_label_propagation.py#L455) treats normalization differently for sparse and dense kernels. I have questions about both of them.\n\n** (Edited) Summary **\nTroubles with the current code normalization:\n- In the dense affinity_matrix case, the current code sums axis=0 and then divides the rows by these sums. Other normalizations in semi_supervised use axis=1 (as this case should). This does not cause incorrect result so long as we have symmetric affinity_matrices. The dense case arises for kernel \"rbf\" which provides symmetric matrices. But if someone provides their own kernel the normalization could be incorrect.\n- In the sparse affinity_matrix case, the current code divides all rows by the sum of the first row. This is not standard normalization, but does not cause errors so long as the row sums are all the same. The sparse case arises for kernel \"knn\" which has all rows sum to k. But if someone provides their own kernel the normalization could be incorrect.\n- The normalization is different for the dense and sparse cases, which could be confusing to someone writing their own kernel.\n\nThe fix involves changing `axis=0` to `axis=1` and correcting the sparse case to divide each row by its sum when the row sums are not all equal.\n\n<details>\n\n<summary> original somewhat rambling description </summary>\n\n** Summary **\nThe method returns a different `affinity_matrix` for sparse and for dense versions of the same kernel matrix. Neither sparse nor dense versions normalize the usual way (columns sum to 1). The dense case is correct for symmetric input kernels. The sparse case scales all values by a constant instead of by column sums.\n\nI suspect the results still converge in most non-symmetric cases. That's probably why this hasn't caused any issues. But some `label_propagation` issues like #8008, #11784, or #9722 could possibly be related.\n\n**sparse treatment**\n\nThe sparse treatment is not really normalization: scaling the entire matrix by a scalar instead of normalizing by the rows or columns. I guess it is harmless to divide the entire matrix by a scalar, but there is enough code effort involved that it seems the intent was not what actually happens. The normalization is done via:\n```python\n            affinity_matrix.data /= np.diag(np.array(normalizer))\n```\nwhere `normalizer = affinity_matrix.sum(axis=0)`.  So `normalizer` is of type `np.matrix` with shape `(1, n)` for `n` columns.  The `np.array` turns that into an `ndarray` -- but it leaves the shape as 2D. Then `np.diag` takes the main diagonal of a 2D row matrix. Note that np.diag takes 1D->2D and 2D->1D. So it is easy to get something different from what you expect if the dimension is 2D instead of 1D. The result here is just the first entry in the row.  We go through the trouble of summing across axis `0`, but then we divide the whole matrix by the first column's sum. \n\nI think this is a bug stemming from assuming `normalizer` to be a 1D object. Then `np.diag` would construct a diagonal matrix with the sums on the main diagonal. Dividing the whole affinity matrix by that diagonal matrix would result in each column being divided by its sum, which is a typical normalization. A second problem here is that we do not divide `affinity_matrix`. Instead we divide `affinity_matrix.data`. So there is no concept of a column in this division.\n\nIf I am correct, one fix would be to avoid trying to divide by a diagonal matrix. Instead divide each data value by the sum corresponding to the column for that data value. We could use `np.ravel` to avoid the 2D indexing issues.  Something like:\n```python\n            affinity_matrix.data /= np.ravel(normalizer)[affinity_matrix.indices]\n```\n\n**dense treatment**\n\nThe dense treatment scales each row by the column sums (instead of each column by the column sum). Perhaps this is what it should be, but it is not the standard normaliztion of either the columns or the rows. The relevant code is:\n```python\n            affinity_matrix /= normalizer[:, np.newaxis]\n```\nIn this case, normalizer is 1D (`affinity_matrix` is `ndarray`). So the index `[:, np.newaxis]` results in broadcasting the column sums along the rows, e.g. row 3 is a repetition of the sum of column 3.  The more natural code would be\n```python\n            affinity_matrix /= normalizer\n```\nand results in scaling each column by the column sums so the resulting columns add to 1.  Note that for square symmetric `affinity_matrix` row sums and column sums are the same. So, it is possible we are only supporting symmetric kernels. But there are tests without symmetric `affinity_matrix`. I haven't figured out why the method still seems to \"work\" even though the scaling is very strange. \n\n**Speculation**\n\n**dense** I suspect the original code used `ndarray` only, and tested symmetric `affinity_matrix`.  It looks like the code mixes up rows and columns when indexing to prepare for broadcasting. \n**sparse** I suspect this code was an attempt to normalize using matrix division by a diagonal matrix. But the code lost track of `np.diag` switching from expanding 1D->2D to reducing 2D->1D when the input is 2D instead of 1D. The result is shape `(1, 1)`, so it still broadcasts to any size input, thus no exception is raised.\n\n**doc_string** The doc_string mentions that this method may not result in a normalized matrix. But it justifies that only briefly. It is true that the doc_string is not public (the method is private). But it should be revised if/when this gets fixed.\n\nNotes: This method needs to be updated when shifting from `spmatrix` to `sparray` because the sparse sum is no longer 2D `np.matrix`. It is 1D `np.ndarray`. That's how I discovered this rabbit hole. :) \n\nI can make a PR for this, but I wanted to make sure my perspective is correct first. Do we want the affinity matrix to be stochastic (with columns summing to 1)? Or is there another approach this code uses that I'm not seeing. Thanks...\n\n</details>",
  "pr_number": 31924,
  "pr_title": "FIX normalization in semi_supervised label_propagation",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.semi_supervised/31924.fix.rst b/doc/whats_new/upcoming_changes/sklearn.semi_supervised/31924.fix.rst\nnew file mode 100644\nindex 0000000000000..fe21593d99680\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.semi_supervised/31924.fix.rst\n@@ -0,0 +1,4 @@\n+- User written kernel results are now normalized in\n+  :class:`semi_supervised.LabelPropagation`\n+  so all row sums equal 1 even if kernel gives asymmetric or non-uniform row sums.\n+  By :user:`Dan Schult <dschult>`.\ndiff --git a/sklearn/semi_supervised/_label_propagation.py b/sklearn/semi_supervised/_label_propagation.py\nindex 7ff1460b0d8be..95dffd212dee0 100644\n--- a/sklearn/semi_supervised/_label_propagation.py\n+++ b/sklearn/semi_supervised/_label_propagation.py\n@@ -453,19 +453,22 @@ def __init__(\n         )\n \n     def _build_graph(self):\n-        \"\"\"Matrix representing a fully connected graph between each sample\n-\n-        This basic implementation creates a non-stochastic affinity matrix, so\n-        class distributions will exceed 1 (normalization may be desired).\n-        \"\"\"\n+        \"\"\"Matrix representing a fully connected graph between each sample.\"\"\"\n         if self.kernel == \"knn\":\n             self.nn_fit = None\n         affinity_matrix = self._get_kernel(self.X_)\n-        normalizer = affinity_matrix.sum(axis=0)\n+        normalizer = affinity_matrix.sum(axis=1)\n+        # handle spmatrix (make normalizer 1D)\n+        if sparse.isspmatrix(affinity_matrix):\n+            normalizer = np.ravel(normalizer)\n+        # TODO: when SciPy 1.12+ is min dependence, replace up to ---- with:\n+        # affinity_matrix /= normalizer[:, np.newaxis]\n         if sparse.issparse(affinity_matrix):\n-            affinity_matrix.data /= np.diag(np.array(normalizer))\n-        else:\n+            inv_normalizer = sparse.diags(1.0 / normalizer)\n+            affinity_matrix = inv_normalizer @ affinity_matrix\n+        else:  # Dense affinity_matrix\n             affinity_matrix /= normalizer[:, np.newaxis]\n+        # ----\n         return affinity_matrix\n \n     def fit(self, X, y):\ndiff --git a/sklearn/semi_supervised/tests/test_label_propagation.py b/sklearn/semi_supervised/tests/test_label_propagation.py\nindex 4b046aa111250..410e0db6cd675 100644\n--- a/sklearn/semi_supervised/tests/test_label_propagation.py\n+++ b/sklearn/semi_supervised/tests/test_label_propagation.py\n@@ -18,7 +18,8 @@\n     assert_array_equal,\n )\n \n-CONSTRUCTOR_TYPES = (\"array\", \"sparse_csr\", \"sparse_csc\")\n+SPARSE_TYPES = (\"sparse_csr\", \"sparse_csc\", \"sparse_csr_array\", \"sparse_csc_array\")\n+CONSTRUCTOR_TYPES = (\"array\",) + SPARSE_TYPES\n \n ESTIMATORS = [\n     (label_propagation.LabelPropagation, {\"kernel\": \"rbf\"}),\n@@ -35,6 +36,12 @@\n     ),\n ]\n \n+LP_ESTIMATORS = [\n+    (klass, params)\n+    for (klass, params) in ESTIMATORS\n+    if klass == label_propagation.LabelPropagation\n+]\n+\n \n @pytest.mark.parametrize(\"Estimator, parameters\", ESTIMATORS)\n def test_fit_transduction(global_dtype, Estimator, parameters):\n@@ -126,7 +133,7 @@ def test_label_propagation_closed_form(global_dtype):\n     assert_allclose(expected, clf.label_distributions_, atol=1e-4)\n \n \n-@pytest.mark.parametrize(\"accepted_sparse_type\", [\"sparse_csr\", \"sparse_csc\"])\n+@pytest.mark.parametrize(\"accepted_sparse_type\", SPARSE_TYPES)\n @pytest.mark.parametrize(\"index_dtype\", [np.int32, np.int64])\n @pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n @pytest.mark.parametrize(\"Estimator, parameters\", ESTIMATORS)\n@@ -143,6 +150,29 @@ def test_sparse_input_types(\n     assert_array_equal(clf.predict([[0.5, 2.5]]), np.array([1]))\n \n \n+@pytest.mark.parametrize(\"constructor\", CONSTRUCTOR_TYPES)\n+@pytest.mark.parametrize(\"Estimator, parameters\", LP_ESTIMATORS)\n+def test_label_propagation_build_graph_normalized(constructor, Estimator, parameters):\n+    # required but unused X and labels values\n+    X = np.array([[1.0, 0.0], [1.0, 1.0], [1.0, 3.0]])\n+    labels = [0, 1, -1]\n+\n+    # test normalization of an affinity_matrix\n+    aff_matrix = np.array([[1.0, 1.0, 0.0], [2.0, 1.0, 1.0], [0.0, 1.0, 3.0]])\n+    expected = np.array([[0.5, 0.5, 0.0], [0.5, 0.25, 0.25], [0.0, 0.25, 0.75]])\n+\n+    def kernel_affinity_matrix(x, y=None):\n+        return _convert_container(aff_matrix, constructor)\n+\n+    clf = Estimator(kernel=kernel_affinity_matrix).fit(X, labels)\n+    graph = clf._build_graph()\n+    assert_allclose(graph.sum(axis=1), 1)  # normalized rows\n+\n+    if issparse(graph):\n+        graph = graph.toarray()\n+    assert_allclose(graph, expected)\n+\n+\n @pytest.mark.parametrize(\"constructor_type\", CONSTRUCTOR_TYPES)\n def test_convergence_speed(constructor_type):\n     # This is a non-regression test for #5774\n",
  "fail_to_pass": [
    "test_label_propagation_build_graph_normalized"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/semi_supervised/_label_propagation.py",
    "sklearn/semi_supervised/tests/test_label_propagation.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-08-11T12:50:37Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31924",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/31872"
}