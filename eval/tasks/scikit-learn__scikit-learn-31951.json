{
  "id": "scikit-learn__scikit-learn-31951",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "726ed184ed80b0191732baaaf5825b86b41db4d2",
  "issue_number": 31931,
  "issue_title": "Allow common estimator checks to use `xfail_strict=True`",
  "issue_body": "### Describe the workflow you want to enable\n\nI'd like to be able to use [`parametrize_with_checks`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.parametrize_with_checks.html) and use \"strict mode\" to notice when checks that are marked as xfail start passing. But I don't want to turn on strict mode for my whole test suite (`xfail_strict = true` in `pytest.ini`)\n\n### Describe your proposed solution\n\nWe use `pytest.mark.xfail` internally when generating all the estimator + check combinations. I think we could pass `strict=True` there to make it a failure for a test, that is marked as xfail, to pass.\n\nhttps://github.com/scikit-learn/scikit-learn/blob/c5497b7f7eacfaff061cf68e09bcd48aa93d4d6b/sklearn/utils/estimator_checks.py#L456\n\nI think we want to make this behaviour configurable, so we need a new parameter for `parametrize_with_checks`, something like `strict=None` with the option to set it to `True`/`False`.\n\nI'd set the default to `None` so that not setting it does not override the setting in `pytest.ini` (to be checked if this actually works). If you are using `pytest.ini` to control strict mode then not passing `strict` to `parametrize_with_checks` should not change anything.\n\n### Describe alternatives you've considered, if relevant\n\nI tried layering `@pytest.mark.xfail(strict=True)` on top of `@parametrize_with_checks` but that doesn't seem to work.\n\n```python\n@pytest.mark.xfail(strict=True)\n@parametrize_with_checks(...)\ndef test_sklearn_compat(estimator, check):\n   ...\n```\n\n### Additional context\n\n_No response_",
  "pr_number": 31951,
  "pr_title": "Add option to use strict xfail mode in `parametrize_with_checks`",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.utils/31951.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.utils/31951.enhancement.rst\nnew file mode 100644\nindex 0000000000000..78df7fff40743\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.utils/31951.enhancement.rst\n@@ -0,0 +1,4 @@\n+- ``sklearn.utils.estimator_checks.parametrize_with_checks`` now lets you configure\n+  strict mode for xfailing checks. Tests that unexpectedly pass will lead to a test\n+  failure. The default behaviour is unchanged.\n+  By :user:`Tim Head <betatim>`.\ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\nindex 0841f9dd01d4d..d8cd13848a09d 100644\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -424,6 +424,7 @@ def _maybe_mark(\n     expected_failed_checks: dict[str, str] | None = None,\n     mark: Literal[\"xfail\", \"skip\", None] = None,\n     pytest=None,\n+    xfail_strict: bool | None = None,\n ):\n     \"\"\"Mark the test as xfail or skip if needed.\n \n@@ -442,6 +443,13 @@ def _maybe_mark(\n         Pytest module to use to mark the check. This is only needed if ``mark`` is\n         `\"xfail\"`. Note that one can run `check_estimator` without having `pytest`\n         installed. This is used in combination with `parametrize_with_checks` only.\n+    xfail_strict : bool, default=None\n+        Whether to run checks in xfail strict mode. This option is ignored unless\n+        `mark=\"xfail\"`. If True, checks that are expected to fail but actually\n+        pass will lead to a test failure. If False, unexpectedly passing tests\n+        will be marked as xpass. If None, the default pytest behavior is used.\n+\n+        .. versionadded:: 1.8\n     \"\"\"\n     should_be_marked, reason = _should_be_skipped_or_marked(\n         estimator, check, expected_failed_checks\n@@ -451,7 +459,14 @@ def _maybe_mark(\n \n     estimator_name = estimator.__class__.__name__\n     if mark == \"xfail\":\n-        return pytest.param(estimator, check, marks=pytest.mark.xfail(reason=reason))\n+        # With xfail_strict=None we want the value from the pytest config to\n+        # take precedence and that means not passing strict to the xfail\n+        # mark at all.\n+        if xfail_strict is None:\n+            mark = pytest.mark.xfail(reason=reason)\n+        else:\n+            mark = pytest.mark.xfail(reason=reason, strict=xfail_strict)\n+        return pytest.param(estimator, check, marks=mark)\n     else:\n \n         @wraps(check)\n@@ -501,6 +516,7 @@ def estimator_checks_generator(\n     legacy: bool = True,\n     expected_failed_checks: dict[str, str] | None = None,\n     mark: Literal[\"xfail\", \"skip\", None] = None,\n+    xfail_strict: bool | None = None,\n ):\n     \"\"\"Iteratively yield all check callables for an estimator.\n \n@@ -528,6 +544,13 @@ def estimator_checks_generator(\n         xfail(`pytest.mark.xfail`) or skip. Marking a test as \"skip\" is done via\n         wrapping the check in a function that raises a\n         :class:`~sklearn.exceptions.SkipTest` exception.\n+    xfail_strict : bool, default=None\n+        Whether to run checks in xfail strict mode. This option is ignored unless\n+        `mark=\"xfail\"`. If True, checks that are expected to fail but actually\n+        pass will lead to a test failure. If False, unexpectedly passing tests\n+        will be marked as xpass. If None, the default pytest behavior is used.\n+\n+        .. versionadded:: 1.8\n \n     Returns\n     -------\n@@ -552,6 +575,7 @@ def estimator_checks_generator(\n                 expected_failed_checks=expected_failed_checks,\n                 mark=mark,\n                 pytest=pytest,\n+                xfail_strict=xfail_strict,\n             )\n \n \n@@ -560,6 +584,7 @@ def parametrize_with_checks(\n     *,\n     legacy: bool = True,\n     expected_failed_checks: Callable | None = None,\n+    xfail_strict: bool | None = None,\n ):\n     \"\"\"Pytest specific decorator for parametrizing estimator checks.\n \n@@ -605,9 +630,16 @@ def parametrize_with_checks(\n         Where `\"check_name\"` is the name of the check, and `\"my reason\"` is why\n         the check fails. These tests will be marked as xfail if the check fails.\n \n-\n         .. versionadded:: 1.6\n \n+    xfail_strict : bool, default=None\n+        Whether to run checks in xfail strict mode. If True, checks that are\n+        expected to fail but actually pass will lead to a test failure. If\n+        False, unexpectedly passing tests will be marked as xpass. If None,\n+        the default pytest behavior is used.\n+\n+        .. versionadded:: 1.8\n+\n     Returns\n     -------\n     decorator : `pytest.mark.parametrize`\n@@ -640,7 +672,12 @@ def parametrize_with_checks(\n \n     def _checks_generator(estimators, legacy, expected_failed_checks):\n         for estimator in estimators:\n-            args = {\"estimator\": estimator, \"legacy\": legacy, \"mark\": \"xfail\"}\n+            args = {\n+                \"estimator\": estimator,\n+                \"legacy\": legacy,\n+                \"mark\": \"xfail\",\n+                \"xfail_strict\": xfail_strict,\n+            }\n             if callable(expected_failed_checks):\n                 args[\"expected_failed_checks\"] = expected_failed_checks(estimator)\n             yield from estimator_checks_generator(**args)\ndiff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py\nindex 2abe8caefd915..8048979640509 100644\n--- a/sklearn/utils/tests/test_estimator_checks.py\n+++ b/sklearn/utils/tests/test_estimator_checks.py\n@@ -1324,6 +1324,61 @@ def test_all_estimators_all_public():\n     run_tests_without_pytest()\n \n \n+def test_estimator_checks_generator_strict_none():\n+    # Check that no \"strict\" mark is included in the generated checks\n+    est = next(_construct_instances(NuSVC))\n+    expected_to_fail = _get_expected_failed_checks(est)\n+    # If we don't pass strict, it should not appear in the xfail mark either\n+    # This way the behaviour configured in pytest.ini takes precedence.\n+    checks = estimator_checks_generator(\n+        est,\n+        legacy=True,\n+        expected_failed_checks=expected_to_fail,\n+        mark=\"xfail\",\n+    )\n+    # make sure we use a class that has expected failures\n+    assert len(expected_to_fail) > 0\n+    marked_checks = [c for c in checks if hasattr(c, \"marks\")]\n+    # make sure we have some checks with marks\n+    assert len(marked_checks) > 0\n+\n+    for parameter_set in marked_checks:\n+        first_mark = parameter_set.marks[0]\n+        assert \"strict\" not in first_mark.kwargs\n+\n+\n+def test_estimator_checks_generator_strict_xfail_tests():\n+    # Make sure that the checks generator marks tests that are expected to fail\n+    # as strict xfail\n+    est = next(_construct_instances(NuSVC))\n+    expected_to_fail = _get_expected_failed_checks(est)\n+    checks = estimator_checks_generator(\n+        est,\n+        legacy=True,\n+        expected_failed_checks=expected_to_fail,\n+        mark=\"xfail\",\n+        xfail_strict=True,\n+    )\n+    # make sure we use a class that has expected failures\n+    assert len(expected_to_fail) > 0\n+    strict_xfailed_checks = []\n+\n+    # xfail'ed checks are wrapped in a ParameterSet, so below we extract\n+    # the things we need via a bit of a crutch: len()\n+    marked_checks = [c for c in checks if hasattr(c, \"marks\")]\n+    # make sure we use a class that has expected failures\n+    assert len(expected_to_fail) > 0\n+\n+    for parameter_set in marked_checks:\n+        _, check = parameter_set.values\n+        first_mark = parameter_set.marks[0]\n+        if first_mark.kwargs[\"strict\"]:\n+            strict_xfailed_checks.append(_check_name(check))\n+\n+    # all checks expected to fail are marked as strict xfail\n+    assert set(expected_to_fail.keys()) == set(strict_xfailed_checks)\n+\n+\n @_mark_thread_unsafe_if_pytest_imported  # Some checks use warnings.\n def test_estimator_checks_generator_skipping_tests():\n     # Make sure the checks generator skips tests that are expected to fail\n",
  "fail_to_pass": [
    "test_estimator_checks_generator_strict_none",
    "test_estimator_checks_generator_strict_xfail_tests"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/utils/estimator_checks.py",
    "sklearn/utils/tests/test_estimator_checks.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-08-15T09:56:38Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31951",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/31931"
}