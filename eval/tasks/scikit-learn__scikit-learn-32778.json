{
  "id": "scikit-learn__scikit-learn-32778",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "0e366004a64444c2ee7124bfc28d49d87faabae2",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 32778,
  "pr_title": "FIX Correct the formulation of `alpha` in `SGDOneClassSVM`",
  "gold_patch": "diff --git a/doc/modules/sgd.rst b/doc/modules/sgd.rst\nindex 360ba2f11c994..8f6043521b82e 100644\n--- a/doc/modules/sgd.rst\n+++ b/doc/modules/sgd.rst\n@@ -283,7 +283,7 @@ variant can be several orders of magnitude faster.\n \n   This is similar to the optimization problems studied in section\n   :ref:`sgd_mathematical_formulation` with :math:`y_i = 1, 1 \\leq i \\leq n` and\n-  :math:`\\alpha = \\nu/2`, :math:`L` being the hinge loss function and :math:`R`\n+  :math:`\\alpha = \\nu`, :math:`L` being the hinge loss function and :math:`R`\n   being the :math:`L_2` norm. We just need to add the term :math:`b\\nu` in the\n   optimization loop.\n \n@@ -457,7 +457,7 @@ misclassification error (Zero-one loss) as shown in the Figure below.\n Popular choices for the regularization term :math:`R` (the `penalty`\n parameter) include:\n \n-- :math:`L_2` norm: :math:`R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2`,\n+- :math:`L_2` norm: :math:`R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = \\frac{1}{2} ||w||_2^2`,\n - :math:`L_1` norm: :math:`R(w) := \\sum_{j=1}^{m} |w_j|`, which leads to sparse\n   solutions.\n - Elastic Net: :math:`R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 +\ndiff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/32778.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/32778.fix.rst\nnew file mode 100644\nindex 0000000000000..5dedb5f37e6e2\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/32778.fix.rst\n@@ -0,0 +1,5 @@\n+- Correct the formulation of `alpha` within :class:`linear_model.SGDOneClassSVM`.\n+  The corrected value is `alpha = nu` instead of `alpha = nu / 2`.\n+  Note: This might result in changed values for the fitted attributes like\n+  `coef_` and `offset_` as well as the predictions made using this class.\n+  By :user:`Omar Salman <OmarManzoor>`.\ndiff --git a/sklearn/linear_model/_sgd_fast.pyx.tp b/sklearn/linear_model/_sgd_fast.pyx.tp\nindex 79699247f7a07..6170444aefe2b 100644\n--- a/sklearn/linear_model/_sgd_fast.pyx.tp\n+++ b/sklearn/linear_model/_sgd_fast.pyx.tp\n@@ -493,7 +493,7 @@ def _plain_sgd{{name_suffix}}(\n                     objective_sum += cur_loss_val\n                     # for PA1/PA2 (passive/aggressive model, online algorithm) use only the loss\n                     if learning_rate != PA1 and learning_rate != PA2:\n-                        # sum up all the terms in the optimization objective function \n+                        # sum up all the terms in the optimization objective function\n                         # (i.e. also include regularization in addition to the loss)\n                         # Note: for the L2 term SGD optimizes 0.5 * L2**2, due to using\n                         # weight decay that's why the 0.5 coefficient is required\n@@ -503,8 +503,8 @@ def _plain_sgd{{name_suffix}}(\n                                 l1_ratio * w.l1norm()\n                             )\n                         if one_class:  # specific to One-Class SVM\n-                            # nu is alpha * 2 (alpha is set as nu / 2 by the caller)\n-                            objective_sum += intercept * (alpha * 2)\n+                            # nu is alpha\n+                            objective_sum += intercept * alpha\n \n                 if y > 0.0:\n                     class_weight = weight_pos\n@@ -549,7 +549,7 @@ def _plain_sgd{{name_suffix}}(\n                 if fit_intercept == 1:\n                     intercept_update = update\n                     if one_class:  # specific for One-Class SVM\n-                        intercept_update -= 2. * eta * alpha\n+                        intercept_update -= eta * alpha\n                     if intercept_update != 0:\n                         intercept += intercept_update * intercept_decay\n \ndiff --git a/sklearn/linear_model/_stochastic_gradient.py b/sklearn/linear_model/_stochastic_gradient.py\nindex c65cdbdcf51ce..1c969dc1a141a 100644\n--- a/sklearn/linear_model/_stochastic_gradient.py\n+++ b/sklearn/linear_model/_stochastic_gradient.py\n@@ -2492,7 +2492,7 @@ def partial_fit(self, X, y=None, sample_weight=None):\n         if not hasattr(self, \"coef_\"):\n             self._more_validate_params(for_partial_fit=True)\n \n-        alpha = self.nu / 2\n+        alpha = self.nu\n         return self._partial_fit(\n             X,\n             alpha,\n@@ -2596,7 +2596,7 @@ def fit(self, X, y=None, coef_init=None, offset_init=None, sample_weight=None):\n         \"\"\"\n         self._more_validate_params()\n \n-        alpha = self.nu / 2\n+        alpha = self.nu\n         self._fit(\n             X,\n             alpha=alpha,\ndiff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py\nindex ad48cfec3938c..23cb2441143f7 100644\n--- a/sklearn/linear_model/tests/test_sgd.py\n+++ b/sklearn/linear_model/tests/test_sgd.py\n@@ -6,9 +6,11 @@\n import numpy as np\n import pytest\n import scipy.sparse as sp\n+from scipy.optimize import minimize\n \n from sklearn import datasets, linear_model, metrics\n from sklearn.base import clone, is_classifier\n+from sklearn.datasets import make_blobs\n from sklearn.exceptions import ConvergenceWarning\n from sklearn.kernel_approximation import Nystroem\n from sklearn.linear_model import _sgd_fast as sgd_fast\n@@ -1496,7 +1498,7 @@ def asgd_oneclass(klass, X, eta, nu, coef_init=None, offset_init=0.0):\n             gradient = -1\n         else:\n             gradient = 0\n-        coef *= max(0, 1.0 - (eta * nu / 2))\n+        coef *= max(0, 1.0 - eta * nu)\n         coef += -(eta * gradient * entry)\n         intercept += -(eta * (nu + gradient)) * decay\n \n@@ -1708,28 +1710,6 @@ def test_average_sparse_oneclass(klass):\n     assert_allclose(clf.offset_, average_offset)\n \n \n-def test_sgd_oneclass():\n-    # Test fit, decision_function, predict and score_samples on a toy\n-    # dataset\n-    X_train = np.array([[-2, -1], [-1, -1], [1, 1]])\n-    X_test = np.array([[0.5, -2], [2, 2]])\n-    clf = SGDOneClassSVM(\n-        nu=0.5, eta0=1, learning_rate=\"constant\", shuffle=False, max_iter=1\n-    )\n-    clf.fit(X_train)\n-    assert_allclose(clf.coef_, np.array([-0.125, 0.4375]))\n-    assert clf.offset_[0] == -0.5\n-\n-    scores = clf.score_samples(X_test)\n-    assert_allclose(scores, np.array([-0.9375, 0.625]))\n-\n-    dec = clf.score_samples(X_test) - clf.offset_\n-    assert_allclose(clf.decision_function(X_test), dec)\n-\n-    pred = clf.predict(X_test)\n-    assert_array_equal(pred, np.array([-1, 1]))\n-\n-\n def test_ocsvm_vs_sgdocsvm():\n     # Checks SGDOneClass SVM gives a good approximation of kernelized\n     # One-Class SVM\n@@ -1785,12 +1765,13 @@ def test_sgd_oneclass_convergence():\n         assert model.n_iter_ > 6\n \n \n-def test_sgd_oneclass_vs_linear_oneclass():\n+@pytest.mark.parametrize(\"eta0, max_iter\", [(1e-3, 10000), (3e-4, 20000)])\n+def test_sgd_oneclass_vs_linear_oneclass(eta0, max_iter):\n     # Test convergence vs. liblinear `OneClassSVM` with kernel=\"linear\"\n     for nu in [0.1, 0.5, 0.9]:\n         # allow enough iterations, small dataset\n         model = SGDOneClassSVM(\n-            nu=nu, max_iter=20000, tol=None, learning_rate=\"constant\", eta0=1e-3\n+            nu=nu, max_iter=max_iter, tol=None, learning_rate=\"constant\", eta0=eta0\n         )\n         model_ref = OneClassSVM(kernel=\"linear\", nu=nu, tol=1e-6)  # reference model\n         model.fit(iris.data)\n@@ -1815,7 +1796,30 @@ def test_sgd_oneclass_vs_linear_oneclass():\n         assert dec_fn_corr > 0.99\n         assert preds_corr > 0.95\n         assert coef_corr > 0.99\n-        assert_allclose(1 - share_ones, nu)\n+        assert_allclose(1 - share_ones, nu, atol=1e-2)\n+\n+\n+@pytest.mark.parametrize(\"nu\", [0.1, 0.9])\n+def test_sgd_oneclass_vs_linear_oneclass_offsets_match(nu):\n+    \"\"\"Test that the `offset_` of `SGDOneClassSVM` is close to the `offset_`\n+    of `OneClassSVM` with `kernel=\"linear\"`, given enough iterations and a\n+    suitable value for the `eta0` parameter, while also ensuring that the\n+    dataset is scaled.\n+    \"\"\"\n+    X = iris.data\n+    X_scaled = StandardScaler().fit_transform(X)\n+    model = SGDOneClassSVM(\n+        nu=nu,\n+        max_iter=40000,\n+        tol=None,\n+        learning_rate=\"optimal\",\n+        eta0=1e-6,\n+        random_state=42,\n+    )\n+    model_ref = OneClassSVM(kernel=\"linear\", nu=nu, tol=5e-6)\n+    model.fit(X_scaled)\n+    model_ref.fit(X_scaled)\n+    assert_allclose(model.offset_, model_ref.offset_, atol=1.3e-6)\n \n \n def test_l1_ratio():\n@@ -2265,10 +2269,10 @@ def test_sgd_numerical_consistency(SGDEstimator):\n     X_32 = X.astype(dtype=np.float32)\n     Y_32 = np.array(Y, dtype=np.float32)\n \n-    sgd_64 = SGDEstimator(max_iter=20)\n+    sgd_64 = SGDEstimator(max_iter=22, shuffle=False)\n     sgd_64.fit(X_64, Y_64)\n \n-    sgd_32 = SGDEstimator(max_iter=20)\n+    sgd_32 = SGDEstimator(max_iter=22, shuffle=False)\n     sgd_32.fit(X_32, Y_32)\n \n     assert_allclose(sgd_64.coef_, sgd_32.coef_)\n@@ -2281,3 +2285,52 @@ def test_sgd_one_class_svm_estimator_type():\n     \"\"\"\n     sgd_ocsvm = SGDOneClassSVM()\n     assert get_tags(sgd_ocsvm).estimator_type == \"outlier_detector\"\n+\n+\n+def test_sgd_one_class_svm_formulation_with_scipy_minimize():\n+    \"\"\"Test that SGDOneClassSVM minimizes the correct objective function.\"\"\"\n+    nu = 0.5\n+    hinge_threshold = 1.0\n+    n_samples, n_features = 300, 3\n+    random_seed = 42\n+\n+    def objective(w, X, y, alpha):\n+        weights = w[:-1]\n+        intercept = w[-1]\n+        p = X @ weights + intercept\n+        z = p * y\n+        avg_loss = np.mean(np.maximum(hinge_threshold - z, 0.0))\n+        reg = 0.5 * alpha * weights @ weights\n+        obj = avg_loss + reg + intercept * alpha\n+        return obj\n+\n+    X, _ = make_blobs(\n+        n_samples=n_samples,\n+        n_features=n_features,\n+        random_state=random_seed,\n+    )\n+    y = np.ones(n_samples, dtype=X.dtype)\n+    w0 = np.zeros(n_features + 1)\n+    scipy_output = minimize(\n+        objective,\n+        w0,\n+        method=\"Nelder-Mead\",\n+        args=(X, y, nu),\n+        options={\"maxiter\": 1000},\n+    )\n+    w_out = scipy_output.x\n+    expected_coef = w_out[:-1]\n+    expected_offset = 1 - w_out[-1]\n+\n+    model = SGDOneClassSVM(\n+        nu=nu,\n+        learning_rate=\"constant\",\n+        max_iter=4000,\n+        tol=None,\n+        eta0=1e-4,\n+        random_state=random_seed,\n+    )\n+    model.fit(X, y)\n+\n+    assert_allclose(model.coef_, expected_coef, rtol=5e-3)\n+    assert_allclose(model.offset_, expected_offset, rtol=1e-2)\n",
  "fail_to_pass": [
    "test_sgd_oneclass_vs_linear_oneclass",
    "test_sgd_oneclass_vs_linear_oneclass_offsets_match",
    "test_sgd_one_class_svm_formulation_with_scipy_minimize"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_stochastic_gradient.py",
    "sklearn/linear_model/tests/test_sgd.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-11-24T16:24:29Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32778",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}