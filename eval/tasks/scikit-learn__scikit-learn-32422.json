{
  "id": "scikit-learn__scikit-learn-32422",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "ea9e824705cc6313aa65413e9ee245fa974f8dd6",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 32422,
  "pr_title": "FEA Add array API support for brier_score_loss, log_loss, d2_brier_score and d2_log_loss_score",
  "gold_patch": "diff --git a/doc/modules/array_api.rst b/doc/modules/array_api.rst\nindex 745c6ffb3ff4b..722537122d9a5 100644\n--- a/doc/modules/array_api.rst\n+++ b/doc/modules/array_api.rst\n@@ -142,13 +142,17 @@ Metrics\n -------\n \n - :func:`sklearn.metrics.accuracy_score`\n+- :func:`sklearn.metrics.brier_score_loss`\n - :func:`sklearn.metrics.confusion_matrix`\n+- :func:`sklearn.metrics.d2_brier_score`\n+- :func:`sklearn.metrics.d2_log_loss_score`\n - :func:`sklearn.metrics.d2_tweedie_score`\n - :func:`sklearn.metrics.explained_variance_score`\n - :func:`sklearn.metrics.f1_score`\n - :func:`sklearn.metrics.fbeta_score`\n - :func:`sklearn.metrics.hamming_loss`\n - :func:`sklearn.metrics.jaccard_score`\n+- :func:`sklearn.metrics.log_loss`\n - :func:`sklearn.metrics.max_error`\n - :func:`sklearn.metrics.mean_absolute_error`\n - :func:`sklearn.metrics.mean_absolute_percentage_error`\ndiff --git a/doc/whats_new/upcoming_changes/array-api/32422.feature.rst b/doc/whats_new/upcoming_changes/array-api/32422.feature.rst\nnew file mode 100644\nindex 0000000000000..fa0cfe503d7f7\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/array-api/32422.feature.rst\n@@ -0,0 +1,4 @@\n+- :func:`sklearn.metrics.brier_score_loss`, :func:`sklearn.metrics.log_loss`,\n+  :func:`sklearn.metrics.d2_brier_score` and :func:`sklearn.metrics.d2_log_loss_score`\n+  now support array API compatible inputs.\n+  By :user:`Omar Salman <OmarManzoor>`\ndiff --git a/sklearn/metrics/_classification.py b/sklearn/metrics/_classification.py\nindex 52154332bf82c..be2d5cb585fd0 100644\n--- a/sklearn/metrics/_classification.py\n+++ b/sklearn/metrics/_classification.py\n@@ -11,6 +11,7 @@\n # SPDX-License-Identifier: BSD-3-Clause\n \n import warnings\n+from math import sqrt\n from numbers import Integral, Real\n \n import numpy as np\n@@ -37,8 +38,10 @@\n     _searchsorted,\n     _tolist,\n     _union1d,\n+    ensure_common_namespace_device,\n     get_namespace,\n     get_namespace_and_device,\n+    supported_float_dtypes,\n     xpx,\n )\n from sklearn.utils._param_validation import (\n@@ -169,6 +172,69 @@ def _check_targets(y_true, y_pred, sample_weight=None):\n     return y_type, y_true, y_pred, sample_weight\n \n \n+def _one_hot_encoding_multiclass_target(y_true, labels, target_xp, target_device):\n+    \"\"\"Convert multi-class `y_true` into a one-hot encoded array and also ensure\n+    that the encoded array is placed on the target API namespace and device.\n+    Also return the classes provided by `LabelBinarizer` in additional to the\n+    integer encoded array.\n+    \"\"\"\n+    xp_y_true, is_y_true_array_api = get_namespace(y_true)\n+\n+    # For classification metrics both array API compatible and non array API\n+    # compatible inputs are allowed for `y_true`. This is because arrays that\n+    # store class labels as strings cannot be represented in namespaces other\n+    # than Numpy. Thus to avoid unnecessary complexity, we always convert\n+    # `y_true` to a Numpy array so that it can be processed appropriately by\n+    # `LabelBinarizer` and then transfer the integer encoded output back to the\n+    # target namespace and device.\n+    if is_y_true_array_api:\n+        y_true = _convert_to_numpy(y_true, xp=xp_y_true)\n+\n+    lb = LabelBinarizer()\n+    if labels is not None:\n+        lb = lb.fit(labels)\n+        # LabelBinarizer does not respect the order implied by labels, which\n+        # can be misleading.\n+        if not np.all(lb.classes_ == labels):\n+            warnings.warn(\n+                f\"Labels passed were {labels}. But this function \"\n+                \"assumes labels are ordered lexicographically. \"\n+                f\"Pass the ordered labels={lb.classes_.tolist()} and ensure that \"\n+                \"the columns of y_prob correspond to this ordering.\",\n+                UserWarning,\n+            )\n+        if not np.isin(y_true, labels).all():\n+            undeclared_labels = set(y_true) - set(labels)\n+            raise ValueError(\n+                f\"y_true contains values {undeclared_labels} not belonging \"\n+                f\"to the passed labels {labels}.\"\n+            )\n+\n+    else:\n+        lb = lb.fit(y_true)\n+\n+    if len(lb.classes_) == 1:\n+        if labels is None:\n+            raise ValueError(\n+                \"y_true contains only one label ({0}). Please \"\n+                \"provide the list of all expected class labels explicitly through the \"\n+                \"labels argument.\".format(lb.classes_[0])\n+            )\n+        else:\n+            raise ValueError(\n+                \"The labels array needs to contain at least two \"\n+                \"labels, got {0}.\".format(lb.classes_)\n+            )\n+\n+    transformed_labels = lb.transform(y_true)\n+    transformed_labels = target_xp.asarray(transformed_labels, device=target_device)\n+    if transformed_labels.shape[1] == 1:\n+        transformed_labels = target_xp.concat(\n+            (1 - transformed_labels, transformed_labels), axis=1\n+        )\n+    return transformed_labels, lb.classes_\n+\n+\n def _validate_multiclass_probabilistic_prediction(\n     y_true, y_prob, sample_weight, labels\n ):\n@@ -208,75 +274,44 @@ def _validate_multiclass_probabilistic_prediction(\n \n     y_prob : array of shape (n_samples, n_classes)\n     \"\"\"\n+    xp, _, device_ = get_namespace_and_device(y_prob)\n+\n     y_prob = check_array(\n-        y_prob, ensure_2d=False, dtype=[np.float64, np.float32, np.float16]\n+        y_prob, ensure_2d=False, dtype=supported_float_dtypes(xp, device=device_)\n     )\n \n-    if y_prob.max() > 1:\n-        raise ValueError(f\"y_prob contains values greater than 1: {y_prob.max()}\")\n-    if y_prob.min() < 0:\n-        raise ValueError(f\"y_prob contains values lower than 0: {y_prob.min()}\")\n+    if xp.max(y_prob) > 1:\n+        raise ValueError(f\"y_prob contains values greater than 1: {xp.max(y_prob)}\")\n+    if xp.min(y_prob) < 0:\n+        raise ValueError(f\"y_prob contains values lower than 0: {xp.min(y_prob)}\")\n \n     check_consistent_length(y_prob, y_true, sample_weight)\n     if sample_weight is not None:\n-        _check_sample_weight(sample_weight, y_true, force_float_dtype=False)\n-\n-    lb = LabelBinarizer()\n-\n-    if labels is not None:\n-        lb = lb.fit(labels)\n-        # LabelBinarizer does not respect the order implied by labels, which\n-        # can be misleading.\n-        if not np.all(lb.classes_ == labels):\n-            warnings.warn(\n-                f\"Labels passed were {labels}. But this function \"\n-                \"assumes labels are ordered lexicographically. \"\n-                f\"Pass the ordered labels={lb.classes_.tolist()} and ensure that \"\n-                \"the columns of y_prob correspond to this ordering.\",\n-                UserWarning,\n-            )\n-        if not np.isin(y_true, labels).all():\n-            undeclared_labels = set(y_true) - set(labels)\n-            raise ValueError(\n-                f\"y_true contains values {undeclared_labels} not belonging \"\n-                f\"to the passed labels {labels}.\"\n-            )\n-\n-    else:\n-        lb = lb.fit(y_true)\n-\n-    if len(lb.classes_) == 1:\n-        if labels is None:\n-            raise ValueError(\n-                \"y_true contains only one label ({0}). Please \"\n-                \"provide the list of all expected class labels explicitly through the \"\n-                \"labels argument.\".format(lb.classes_[0])\n-            )\n-        else:\n-            raise ValueError(\n-                \"The labels array needs to contain at least two \"\n-                \"labels, got {0}.\".format(lb.classes_)\n-            )\n+        _check_sample_weight(sample_weight, y_prob, force_float_dtype=False)\n \n-    transformed_labels = lb.transform(y_true)\n-\n-    if transformed_labels.shape[1] == 1:\n-        transformed_labels = np.append(\n-            1 - transformed_labels, transformed_labels, axis=1\n-        )\n+    transformed_labels, lb_classes = _one_hot_encoding_multiclass_target(\n+        y_true=y_true, labels=labels, target_xp=xp, target_device=device_\n+    )\n \n     # If y_prob is of single dimension, assume y_true to be binary\n     # and then check.\n     if y_prob.ndim == 1:\n-        y_prob = y_prob[:, np.newaxis]\n+        y_prob = y_prob[:, xp.newaxis]\n     if y_prob.shape[1] == 1:\n-        y_prob = np.append(1 - y_prob, y_prob, axis=1)\n+        y_prob = xp.concat([1 - y_prob, y_prob], axis=1)\n \n-    eps = np.finfo(y_prob.dtype).eps\n+    eps = xp.finfo(y_prob.dtype).eps\n \n     # Make sure y_prob is normalized\n-    y_prob_sum = y_prob.sum(axis=1)\n-    if not np.allclose(y_prob_sum, 1, rtol=np.sqrt(eps)):\n+    y_prob_sum = xp.sum(y_prob, axis=1)\n+\n+    if not xp.all(\n+        xpx.isclose(\n+            y_prob_sum,\n+            xp.asarray(1, dtype=y_prob_sum.dtype, device=device_),\n+            rtol=sqrt(eps),\n+        )\n+    ):\n         warnings.warn(\n             \"The y_prob values do not sum to one. Make sure to pass probabilities.\",\n             UserWarning,\n@@ -284,7 +319,7 @@ def _validate_multiclass_probabilistic_prediction(\n \n     # Check if dimensions are consistent.\n     transformed_labels = check_array(transformed_labels)\n-    if len(lb.classes_) != y_prob.shape[1]:\n+    if len(lb_classes) != y_prob.shape[1]:\n         if labels is None:\n             raise ValueError(\n                 \"y_true and y_prob contain different number of \"\n@@ -292,14 +327,14 @@ def _validate_multiclass_probabilistic_prediction(\n                 \"labels explicitly through the labels argument. \"\n                 \"Classes found in \"\n                 \"y_true: {2}\".format(\n-                    transformed_labels.shape[1], y_prob.shape[1], lb.classes_\n+                    transformed_labels.shape[1], y_prob.shape[1], lb_classes\n                 )\n             )\n         else:\n             raise ValueError(\n                 \"The number of classes in labels is different \"\n                 \"from that in y_prob. Classes found in \"\n-                \"labels: {0}\".format(lb.classes_)\n+                \"labels: {0}\".format(lb_classes)\n             )\n \n     return transformed_labels, y_prob\n@@ -3320,6 +3355,9 @@ def log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None)\n     ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n     0.21616\n     \"\"\"\n+    if sample_weight is not None:\n+        sample_weight = ensure_common_namespace_device(y_pred, sample_weight)[0]\n+\n     transformed_labels, y_pred = _validate_multiclass_probabilistic_prediction(\n         y_true, y_pred, sample_weight, labels\n     )\n@@ -3333,9 +3371,12 @@ def log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None)\n \n def _log_loss(transformed_labels, y_pred, *, normalize=True, sample_weight=None):\n     \"\"\"Log loss for transformed labels and validated probabilistic predictions.\"\"\"\n-    eps = np.finfo(y_pred.dtype).eps\n-    y_pred = np.clip(y_pred, eps, 1 - eps)\n-    loss = -xlogy(transformed_labels, y_pred).sum(axis=1)\n+    xp, _ = get_namespace(y_pred, transformed_labels)\n+    if sample_weight is not None:\n+        sample_weight = ensure_common_namespace_device(y_pred, sample_weight)[0]\n+    eps = xp.finfo(y_pred.dtype).eps\n+    y_pred = xp.clip(y_pred, eps, 1 - eps)\n+    loss = -xp.sum(xlogy(transformed_labels, y_pred), axis=1)\n     return float(_average(loss, weights=sample_weight, normalize=normalize))\n \n \n@@ -3491,6 +3532,16 @@ def hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None):\n     return float(np.average(losses, weights=sample_weight))\n \n \n+def _one_hot_encoding_binary_target(y_true, pos_label, target_xp, target_device):\n+    \"\"\"Convert binary `y_true` into a one-hot encoded array and also ensure that\n+    the encoded array is placed on the target API namespace and device.\n+    \"\"\"\n+    xp_y_true, _ = get_namespace(y_true)\n+    y_true_pos = xp_y_true.asarray(y_true == pos_label, dtype=xp_y_true.int64)\n+    y_true_pos = target_xp.asarray(y_true_pos, device=target_device)\n+    return target_xp.stack((1 - y_true_pos, y_true_pos), axis=1)\n+\n+\n def _validate_binary_probabilistic_prediction(y_true, y_prob, sample_weight, pos_label):\n     r\"\"\"Convert y_true and y_prob in binary classification to shape (n_samples, 2)\n \n@@ -3530,7 +3581,7 @@ def _validate_binary_probabilistic_prediction(y_true, y_prob, sample_weight, pos\n \n     check_consistent_length(y_prob, y_true, sample_weight)\n     if sample_weight is not None:\n-        _check_sample_weight(sample_weight, y_true, force_float_dtype=False)\n+        _check_sample_weight(sample_weight, y_prob, force_float_dtype=False)\n \n     y_type = type_of_target(y_true, input_name=\"y_true\")\n     if y_type != \"binary\":\n@@ -3539,10 +3590,11 @@ def _validate_binary_probabilistic_prediction(y_true, y_prob, sample_weight, pos\n             \"binary according to the shape of y_prob.\"\n         )\n \n-    if y_prob.max() > 1:\n-        raise ValueError(f\"y_prob contains values greater than 1: {y_prob.max()}\")\n-    if y_prob.min() < 0:\n-        raise ValueError(f\"y_prob contains values less than 0: {y_prob.min()}\")\n+    xp, _, device_ = get_namespace_and_device(y_prob)\n+    if xp.max(y_prob) > 1:\n+        raise ValueError(f\"y_prob contains values greater than 1: {xp.max(y_prob)}\")\n+    if xp.min(y_prob) < 0:\n+        raise ValueError(f\"y_prob contains values less than 0: {xp.min(y_prob)}\")\n \n     # check that pos_label is consistent with y_true\n     try:\n@@ -3557,9 +3609,10 @@ def _validate_binary_probabilistic_prediction(y_true, y_prob, sample_weight, pos\n             raise\n \n     # convert (n_samples,) to (n_samples, 2) shape\n-    y_true = np.array(y_true == pos_label, int)\n-    transformed_labels = np.column_stack((1 - y_true, y_true))\n-    y_prob = np.column_stack((1 - y_prob, y_prob))\n+    transformed_labels = _one_hot_encoding_binary_target(\n+        y_true=y_true, pos_label=pos_label, target_xp=xp, target_device=device_\n+    )\n+    y_prob = xp.stack((1 - y_prob, y_prob), axis=1)\n \n     return transformed_labels, y_prob\n \n@@ -3692,9 +3745,12 @@ def brier_score_loss(\n     ... )\n     0.146\n     \"\"\"\n+    xp, _, device_ = get_namespace_and_device(y_proba)\n     y_proba = check_array(\n-        y_proba, ensure_2d=False, dtype=[np.float64, np.float32, np.float16]\n+        y_proba, ensure_2d=False, dtype=supported_float_dtypes(xp, device=device_)\n     )\n+    if sample_weight is not None:\n+        sample_weight = ensure_common_namespace_device(y_proba, sample_weight)[0]\n \n     if y_proba.ndim == 1 or y_proba.shape[1] == 1:\n         transformed_labels, y_proba = _validate_binary_probabilistic_prediction(\n@@ -3705,8 +3761,9 @@ def brier_score_loss(\n             y_true, y_proba, sample_weight, labels\n         )\n \n-    brier_score = np.average(\n-        np.sum((transformed_labels - y_proba) ** 2, axis=1), weights=sample_weight\n+    transformed_labels = xp.astype(transformed_labels, y_proba.dtype, copy=False)\n+    brier_score = _average(\n+        xp.sum((transformed_labels - y_proba) ** 2, axis=1), weights=sample_weight\n     )\n \n     if scale_by_half == \"auto\":\n@@ -3781,11 +3838,15 @@ def d2_log_loss_score(y_true, y_pred, *, sample_weight=None, labels=None):\n         return float(\"nan\")\n \n     y_pred = check_array(y_pred, ensure_2d=False, dtype=\"numeric\")\n+    if sample_weight is not None:\n+        sample_weight = ensure_common_namespace_device(y_pred, sample_weight)[0]\n+\n     transformed_labels, y_pred = _validate_multiclass_probabilistic_prediction(\n         y_true, y_pred, sample_weight, labels\n     )\n-    y_pred_null = np.average(transformed_labels, axis=0, weights=sample_weight)\n-    y_pred_null = np.tile(y_pred_null, (len(y_true), 1))\n+    xp, _ = get_namespace(y_pred, transformed_labels)\n+    y_pred_null = _average(transformed_labels, axis=0, weights=sample_weight)\n+    y_pred_null = xp.tile(y_pred_null, (y_pred.shape[0], 1))\n \n     numerator = _log_loss(\n         transformed_labels,\n@@ -3876,9 +3937,13 @@ def d2_brier_score(\n         warnings.warn(msg, UndefinedMetricWarning)\n         return float(\"nan\")\n \n+    xp, _, device_ = get_namespace_and_device(y_proba)\n     y_proba = check_array(\n-        y_proba, ensure_2d=False, dtype=[np.float64, np.float32, np.float16]\n+        y_proba, ensure_2d=False, dtype=supported_float_dtypes(xp, device=device_)\n     )\n+    if sample_weight is not None:\n+        sample_weight = ensure_common_namespace_device(y_proba, sample_weight)[0]\n+\n     if y_proba.ndim == 1 or y_proba.shape[1] == 1:\n         transformed_labels, y_proba = _validate_binary_probabilistic_prediction(\n             y_true, y_proba, sample_weight, pos_label\n@@ -3887,16 +3952,17 @@ def d2_brier_score(\n         transformed_labels, y_proba = _validate_multiclass_probabilistic_prediction(\n             y_true, y_proba, sample_weight, labels\n         )\n-    y_proba_null = np.average(transformed_labels, axis=0, weights=sample_weight)\n-    y_proba_null = np.tile(y_proba_null, (len(y_true), 1))\n+    transformed_labels = xp.astype(transformed_labels, y_proba.dtype, copy=False)\n+    y_proba_null = _average(transformed_labels, axis=0, weights=sample_weight)\n+    y_proba_null = xp.tile(y_proba_null, (y_proba.shape[0], 1))\n \n     # Scaling does not matter in D^2 score as it cancels out by taking the ratio.\n-    brier_score = np.average(\n-        np.sum((transformed_labels - y_proba) ** 2, axis=1),\n+    brier_score = _average(\n+        xp.sum((transformed_labels - y_proba) ** 2, axis=1),\n         weights=sample_weight,\n     )\n-    brier_score_null = np.average(\n-        np.sum((transformed_labels - y_proba_null) ** 2, axis=1),\n+    brier_score_null = _average(\n+        xp.sum((transformed_labels - y_proba_null) ** 2, axis=1),\n         weights=sample_weight,\n     )\n     return float(1 - brier_score / brier_score_null)\ndiff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\nindex 66b0f0bc9b895..4bf51b8c6b832 100644\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -3670,3 +3670,114 @@ def test_confusion_matrix_array_api(array_namespace, device, _):\n         result = confusion_matrix(y_true, y_pred, labels=labels)\n         assert get_namespace(result)[0] == get_namespace(y_pred)[0]\n         assert array_api_device(result) == array_api_device(y_pred)\n+\n+\n+@pytest.mark.parametrize(\n+    \"prob_metric\", [brier_score_loss, log_loss, d2_brier_score, d2_log_loss_score]\n+)\n+@pytest.mark.parametrize(\"str_y_true\", [False, True])\n+@pytest.mark.parametrize(\"use_sample_weight\", [False, True])\n+@pytest.mark.parametrize(\n+    \"array_namespace, device_, dtype_name\", yield_namespace_device_dtype_combinations()\n+)\n+def test_probabilistic_metrics_array_api(\n+    prob_metric, str_y_true, use_sample_weight, array_namespace, device_, dtype_name\n+):\n+    \"\"\"Test that :func:`brier_score_loss`, :func:`log_loss`, func:`d2_brier_score`\n+    and :func:`d2_log_loss_score` work correctly with the array API for binary\n+    and mutli-class inputs.\n+    \"\"\"\n+    xp = _array_api_for_tests(array_namespace, device_)\n+    sample_weight = np.array([1, 2, 3, 1]) if use_sample_weight else None\n+\n+    # binary case\n+    extra_kwargs = {}\n+    if str_y_true:\n+        y_true_np = np.array([\"yes\", \"no\", \"yes\", \"no\"])\n+        y_true_xp_or_np = np.asarray(y_true_np)\n+        if \"brier\" in prob_metric.__name__:\n+            # `brier_score_loss` and `d2_brier_score` require specifying the\n+            # `pos_label`\n+            extra_kwargs[\"pos_label\"] = \"yes\"\n+    else:\n+        y_true_np = np.array([1, 0, 1, 0])\n+        y_true_xp_or_np = xp.asarray(y_true_np, device=device_)\n+\n+    y_prob_np = np.array([0.5, 0.2, 0.7, 0.6], dtype=dtype_name)\n+    y_prob_xp = xp.asarray(y_prob_np, device=device_)\n+    metric_score_np = prob_metric(\n+        y_true_np, y_prob_np, sample_weight=sample_weight, **extra_kwargs\n+    )\n+    with config_context(array_api_dispatch=True):\n+        metric_score_xp = prob_metric(\n+            y_true_xp_or_np, y_prob_xp, sample_weight=sample_weight, **extra_kwargs\n+        )\n+\n+    assert metric_score_xp == pytest.approx(metric_score_np)\n+\n+    # multi-class case\n+    if str_y_true:\n+        y_true_np = np.array([\"a\", \"b\", \"c\", \"d\"])\n+        y_true_xp_or_np = np.asarray(y_true_np)\n+    else:\n+        y_true_np = np.array([0, 1, 2, 3])\n+        y_true_xp_or_np = xp.asarray(y_true_np, device=device_)\n+\n+    y_prob_np = np.array(\n+        [\n+            [0.5, 0.2, 0.2, 0.1],\n+            [0.4, 0.4, 0.1, 0.1],\n+            [0.1, 0.1, 0.7, 0.1],\n+            [0.1, 0.2, 0.6, 0.1],\n+        ],\n+        dtype=dtype_name,\n+    )\n+    y_prob_xp = xp.asarray(y_prob_np, device=device_)\n+    metric_score_np = prob_metric(y_true_np, y_prob_np)\n+    with config_context(array_api_dispatch=True):\n+        metric_score_xp = prob_metric(y_true_xp_or_np, y_prob_xp)\n+\n+    assert metric_score_xp == pytest.approx(metric_score_np)\n+\n+\n+@pytest.mark.parametrize(\n+    \"prob_metric\", [brier_score_loss, log_loss, d2_brier_score, d2_log_loss_score]\n+)\n+@pytest.mark.parametrize(\"use_sample_weight\", [False, True])\n+@pytest.mark.parametrize(\n+    \"array_namespace, device_, dtype_name\", yield_namespace_device_dtype_combinations()\n+)\n+def test_probabilistic_metrics_multilabel_array_api(\n+    prob_metric, use_sample_weight, array_namespace, device_, dtype_name\n+):\n+    \"\"\"Test that :func:`brier_score_loss`, :func:`log_loss`, func:`d2_brier_score`\n+    and :func:`d2_log_loss_score` work correctly with the array API for\n+    multi-label inputs.\n+    \"\"\"\n+    xp = _array_api_for_tests(array_namespace, device_)\n+    sample_weight = np.array([1, 2, 3, 1]) if use_sample_weight else None\n+    y_true_np = np.array(\n+        [\n+            [0, 0, 1, 1],\n+            [1, 0, 1, 0],\n+            [0, 1, 0, 0],\n+            [1, 1, 0, 1],\n+        ],\n+        dtype=dtype_name,\n+    )\n+    y_true_xp = xp.asarray(y_true_np, device=device_)\n+    y_prob_np = np.array(\n+        [\n+            [0.15, 0.27, 0.46, 0.12],\n+            [0.33, 0.38, 0.06, 0.23],\n+            [0.06, 0.28, 0.03, 0.63],\n+            [0.14, 0.31, 0.26, 0.29],\n+        ],\n+        dtype=dtype_name,\n+    )\n+    y_prob_xp = xp.asarray(y_prob_np, device=device_)\n+    metric_score_np = prob_metric(y_true_np, y_prob_np, sample_weight=sample_weight)\n+    with config_context(array_api_dispatch=True):\n+        metric_score_xp = prob_metric(y_true_xp, y_prob_xp, sample_weight=sample_weight)\n+\n+    assert metric_score_xp == pytest.approx(metric_score_np)\n",
  "fail_to_pass": [
    "test_probabilistic_metrics_array_api",
    "test_probabilistic_metrics_multilabel_array_api"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_classification.py",
    "sklearn/metrics/tests/test_classification.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-10-07T16:21:24Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32422",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}