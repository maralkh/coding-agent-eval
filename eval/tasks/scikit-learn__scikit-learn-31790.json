{
  "id": "scikit-learn__scikit-learn-31790",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "d80b0c7216ed64aa62f1dcd3a18bf28d83198c6b",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 31790,
  "pr_title": "ENH Add clip parameter to MaxAbsScaler",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.preprocessing/31790.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.preprocessing/31790.enhancement.rst\nnew file mode 100644\nindex 0000000000000..caabc96b626fd\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.preprocessing/31790.enhancement.rst\n@@ -0,0 +1,3 @@\n+- :class:`preprocessing.MaxAbsScaler` can now clip out-of-range values in held-out data\n+  with the parameter `clip`.\n+  By :user:`Hleb Levitski <glevv>`.\ndiff --git a/sklearn/preprocessing/_data.py b/sklearn/preprocessing/_data.py\nindex a2be5578298e9..d1ff4ee42101f 100644\n--- a/sklearn/preprocessing/_data.py\n+++ b/sklearn/preprocessing/_data.py\n@@ -329,7 +329,16 @@ class MinMaxScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n \n     clip : bool, default=False\n         Set to True to clip transformed values of held-out data to\n-        provided `feature range`.\n+        provided `feature_range`.\n+        Since this parameter will clip values, `inverse_transform` may not\n+        be able to restore the original data.\n+\n+        .. note::\n+            Setting `clip=True` does not prevent feature drift (a distribution\n+            shift between training and test data). The transformed values are clipped\n+            to the `feature_range`, which helps avoid unintended behavior in models\n+            sensitive to out-of-range inputs (e.g. linear models). Use with care,\n+            as clipping can distort the distribution of test data.\n \n         .. versionadded:: 0.24\n \n@@ -1172,6 +1181,18 @@ class MaxAbsScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n         Set to False to perform inplace scaling and avoid a copy (if the input\n         is already a numpy array).\n \n+    clip : bool, default=False\n+        Set to True to clip transformed values of held-out data to [-1, 1].\n+        Since this parameter will clip values, `inverse_transform` may not\n+        be able to restore the original data.\n+\n+        .. note::\n+            Setting `clip=True` does not prevent feature drift (a distribution\n+            shift between training and test data). The transformed values are clipped\n+            to the [-1, 1] range, which helps avoid unintended behavior in models\n+            sensitive to out-of-range inputs (e.g. linear models). Use with care,\n+            as clipping can distort the distribution of test data.\n+\n     Attributes\n     ----------\n     scale_ : ndarray of shape (n_features,)\n@@ -1222,10 +1243,14 @@ class MaxAbsScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n            [ 0. ,  1. , -0.5]])\n     \"\"\"\n \n-    _parameter_constraints: dict = {\"copy\": [\"boolean\"]}\n+    _parameter_constraints: dict = {\n+        \"copy\": [\"boolean\"],\n+        \"clip\": [\"boolean\"],\n+    }\n \n-    def __init__(self, *, copy=True):\n+    def __init__(self, *, copy=True, clip=False):\n         self.copy = copy\n+        self.clip = clip\n \n     def _reset(self):\n         \"\"\"Reset internal data-dependent state of the scaler, if necessary.\n@@ -1340,8 +1365,20 @@ def transform(self, X):\n \n         if sparse.issparse(X):\n             inplace_column_scale(X, 1.0 / self.scale_)\n+            if self.clip:\n+                np.clip(X.data, -1.0, 1.0, out=X.data)\n         else:\n             X /= self.scale_\n+            if self.clip:\n+                device_ = device(X)\n+                X = _modify_in_place_if_numpy(\n+                    xp,\n+                    xp.clip,\n+                    X,\n+                    xp.asarray(-1.0, dtype=X.dtype, device=device_),\n+                    xp.asarray(1.0, dtype=X.dtype, device=device_),\n+                    out=X,\n+                )\n         return X\n \n     def inverse_transform(self, X):\ndiff --git a/sklearn/preprocessing/tests/test_common.py b/sklearn/preprocessing/tests/test_common.py\nindex 09f702f64ce23..3e779a0227066 100644\n--- a/sklearn/preprocessing/tests/test_common.py\n+++ b/sklearn/preprocessing/tests/test_common.py\n@@ -42,7 +42,7 @@ def _get_valid_samples_by_column(X, col):\n @pytest.mark.parametrize(\n     \"est, func, support_sparse, strictly_positive, omit_kwargs\",\n     [\n-        (MaxAbsScaler(), maxabs_scale, True, False, []),\n+        (MaxAbsScaler(), maxabs_scale, True, False, [\"clip\"]),\n         (MinMaxScaler(), minmax_scale, False, False, [\"clip\"]),\n         (StandardScaler(), scale, False, False, []),\n         (StandardScaler(with_mean=False), scale, True, False, []),\ndiff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py\nindex 32199c9dbaa13..20712fbbebd0e 100644\n--- a/sklearn/preprocessing/tests/test_data.py\n+++ b/sklearn/preprocessing/tests/test_data.py\n@@ -707,6 +707,7 @@ def test_standard_check_array_of_inverse_transform():\n     \"estimator\",\n     [\n         MaxAbsScaler(),\n+        MaxAbsScaler(clip=True),\n         MinMaxScaler(),\n         MinMaxScaler(clip=True),\n         KernelCenterer(),\n@@ -2517,6 +2518,8 @@ def test_minmax_scaler_clip(feature_range):\n     # test behaviour of the parameter 'clip' in MinMaxScaler\n     X = iris.data\n     scaler = MinMaxScaler(feature_range=feature_range, clip=True).fit(X)\n+    # create a test sample with features outside the training feature range:\n+    # first 2 features < min(X) and last 2 features > max(X)\n     X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n     X_test = [np.r_[X_min[:2] - 10, X_max[2:] + 10]]\n     X_transformed = scaler.transform(X_test)\n@@ -2526,6 +2529,25 @@ def test_minmax_scaler_clip(feature_range):\n     )\n \n \n+@pytest.mark.parametrize(\n+    \"data_constructor\", [np.array] + CSC_CONTAINERS + CSR_CONTAINERS\n+)\n+def test_maxabs_scaler_clip(data_constructor):\n+    # test behaviour of the parameter 'clip' in MaxAbsScaler\n+    X = data_constructor(iris.data)\n+    is_sparse = sparse.issparse(X)\n+    scaler = MaxAbsScaler(clip=True).fit(X)\n+    # create a test sample with features outside the training max abs range:\n+    # first 2 features > max(abs(X)) and last 2 features < -max(abs(X))\n+    max_abs = np.max(np.abs(X), axis=0)\n+    max_abs = max_abs.data if is_sparse else max_abs\n+    X_test = data_constructor(\n+        np.hstack((max_abs[:2] + 10, -max_abs[2:] - 10)).reshape(1, -1)\n+    )\n+    X_transformed = scaler.transform(X_test)\n+    assert_allclose_dense_sparse(X_transformed, data_constructor([[1, 1, -1, -1]]))\n+\n+\n def test_standard_scaler_raise_error_for_1d_input():\n     \"\"\"Check that `inverse_transform` from `StandardScaler` raises an error\n     with 1D array.\n",
  "fail_to_pass": [
    "test_maxabs_scaler_clip"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/preprocessing/_data.py",
    "sklearn/preprocessing/tests/test_common.py",
    "sklearn/preprocessing/tests/test_data.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-07-19T08:50:58Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/31790",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}