{
  "id": "scikit-learn__scikit-learn-30100",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "c08b4332a3358f0090c8e3873aedde815908e248",
  "issue_number": 28840,
  "issue_title": "ENH multiclass/multinomial newton cholesky for LogisticRegression",
  "issue_body": "#### Reference Issues/PRs\r\nIn a way a follow-up of #24767.\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThis extends the `\"newton-cholesky\"` solver of `LogisticRegression` and `LogisticRegressionCV` to full multinomial loss. In particular, the full hessian is calculated. This way, this solver does not need to resort to OvR for multiclass targets.\r\n\r\n#### Any other comments?\r\nThere are 2 tricky parts:\r\n1. Some index battle as one usually divides the index of coefficients hierarchically into `n_features` and `n_classes`. But in the end, the hessian is a 2-dim matrix - and it is!\r\n2. The multinomial is over-parameterized for any unpenalized coefficient, so at least for the intercept. We therefore choose the last class intercept as reference and set its intercept value to zero.",
  "pr_number": 30100,
  "pr_title": "FIX properly report `n_iter_` in case of fallback from Newton-Cholesky to LBFGS ",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/30100.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/30100.fix.rst\nnew file mode 100644\nindex 0000000000000..4ec508ad984a2\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/30100.fix.rst\n@@ -0,0 +1,5 @@\n+- :class:`linear_model.LogisticRegression` and and other linear models that\n+  accept `solver=\"newton-cholesky\"` now report the correct number of iterations\n+  when they fall back to the `\"lbfgs\"` solver because of a rank deficient\n+  Hessian matrix.\n+  By :user:`Olivier Grisel <ogrisel>`\ndiff --git a/sklearn/linear_model/_glm/_newton_solver.py b/sklearn/linear_model/_glm/_newton_solver.py\nindex faed4b3697b1a..2967b91225fdb 100644\n--- a/sklearn/linear_model/_glm/_newton_solver.py\n+++ b/sklearn/linear_model/_glm/_newton_solver.py\n@@ -184,7 +184,7 @@ def fallback_lbfgs_solve(self, X, y, sample_weight):\n             method=\"L-BFGS-B\",\n             jac=True,\n             options={\n-                \"maxiter\": self.max_iter,\n+                \"maxiter\": self.max_iter - self.iteration,\n                 \"maxls\": 50,  # default is 20\n                 \"iprint\": self.verbose - 1,\n                 \"gtol\": self.tol,\n@@ -192,7 +192,7 @@ def fallback_lbfgs_solve(self, X, y, sample_weight):\n             },\n             args=(X, y, sample_weight, self.l2_reg_strength, self.n_threads),\n         )\n-        self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n+        self.iteration += _check_optimize_result(\"lbfgs\", opt_res)\n         self.coef = opt_res.x\n         self.converged = opt_res.status == 0\n \ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\nindex 4f97eacaebf80..38325e4fe4cfd 100644\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -12,7 +12,7 @@\n     assert_array_equal,\n )\n from scipy import sparse\n-from scipy.linalg import svd\n+from scipy.linalg import LinAlgWarning, svd\n \n from sklearn import config_context\n from sklearn._loss import HalfMultinomialLoss\n@@ -2374,3 +2374,45 @@ def test_multi_class_deprecated():\n     lrCV = LogisticRegressionCV(multi_class=\"multinomial\")\n     with pytest.warns(FutureWarning, match=msg):\n         lrCV.fit(X, y)\n+\n+\n+def test_newton_cholesky_fallback_to_lbfgs(global_random_seed):\n+    # Wide data matrix should lead to a rank-deficient Hessian matrix\n+    # hence make the Newton-Cholesky solver raise a warning and fallback to\n+    # lbfgs.\n+    X, y = make_classification(\n+        n_samples=10, n_features=20, random_state=global_random_seed\n+    )\n+    C = 1e30  # very high C to nearly disable regularization\n+\n+    # Check that LBFGS can converge without any warning on this problem.\n+    lr_lbfgs = LogisticRegression(solver=\"lbfgs\", C=C)\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"error\")\n+        lr_lbfgs.fit(X, y)\n+        n_iter_lbfgs = lr_lbfgs.n_iter_[0]\n+\n+    assert n_iter_lbfgs >= 1\n+\n+    # Check that the Newton-Cholesky solver raises a warning and falls back to\n+    # LBFGS. This should converge with the same number of iterations as the\n+    # above call of lbfgs since the Newton-Cholesky triggers the fallback\n+    # before completing the first iteration, for the problem setting at hand.\n+    lr_nc = LogisticRegression(solver=\"newton-cholesky\", C=C)\n+    with ignore_warnings(category=LinAlgWarning):\n+        lr_nc.fit(X, y)\n+        n_iter_nc = lr_nc.n_iter_[0]\n+\n+    assert n_iter_nc == n_iter_lbfgs\n+\n+    # Trying to fit the same model again with a small iteration budget should\n+    # therefore raise a ConvergenceWarning:\n+    lr_nc_limited = LogisticRegression(\n+        solver=\"newton-cholesky\", C=C, max_iter=n_iter_lbfgs - 1\n+    )\n+    with ignore_warnings(category=LinAlgWarning):\n+        with pytest.warns(ConvergenceWarning, match=\"lbfgs failed to converge\"):\n+            lr_nc_limited.fit(X, y)\n+            n_iter_nc_limited = lr_nc_limited.n_iter_[0]\n+\n+    assert n_iter_nc_limited == lr_nc_limited.max_iter - 1\n",
  "fail_to_pass": [
    "test_newton_cholesky_fallback_to_lbfgs"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_glm/_newton_solver.py",
    "sklearn/linear_model/tests/test_logistic.py"
  ],
  "difficulty": "medium",
  "created_at": "2024-10-18T12:10:28Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30100",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/28840"
}