{
  "id": "scikit-learn__scikit-learn-32014",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "91d5640a82dd0bd35c8c438ef22ff3c2cd56bce3",
  "issue_number": 31882,
  "issue_title": "ENH add gap safe screening rules to enet_coordinate_descent",
  "issue_body": "#### Reference Issues/PRs\r\nSolves #229. The 2nd oldest open issue at the time of opening this PR!!!\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThis PR adds Gap Safe Screening Rules for the private function `enet_coordinate_descent`.\r\n\r\nIn order to make it testable for others, I added a private attribute `_do_screening` (bool) to several classes like `Lasso`.\r\n\r\n\r\n#### Any other comments?\r\nFollowing https://arxiv.org/abs/1802.07481 but without acceleration or working sets. To keep it simple.\r\n\r\n**For reviewers:** Please first merge #31906 and #31905.",
  "pr_number": 32014,
  "pr_title": "ENH add gap safe screening rules to enet_coordinate_descent_multi_task",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/31986.efficiency.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/32014.efficiency.rst\nsimilarity index 75%\nrename from doc/whats_new/upcoming_changes/sklearn.linear_model/31986.efficiency.rst\nrename to doc/whats_new/upcoming_changes/sklearn.linear_model/32014.efficiency.rst\nindex 66d341e58f8ec..5b553ebd111ee 100644\n--- a/doc/whats_new/upcoming_changes/sklearn.linear_model/31986.efficiency.rst\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/32014.efficiency.rst\n@@ -1,5 +1,7 @@\n - :class:`linear_model.ElasticNet`, :class:`linear_model.ElasticNetCV`,\n-  :class:`linear_model.Lasso`, :class:`linear_model.LassoCV` as well as\n+  :class:`linear_model.Lasso`, :class:`linear_model.LassoCV`,\n+  :class:`linear_model.MultiTaskElasticNetCV`, :class:`linear_model.MultiTaskLassoCV`\n+  as well as\n   :func:`linear_model.lasso_path` and :func:`linear_model.enet_path` now implement\n   gap safe screening rules in the coordinate descent solver for dense `X` (with\n   `precompute=False` or `\"auto\"` with `n_samples < n_features`) and sparse `X`\n@@ -9,4 +11,4 @@\n   There is now an additional check of the stopping criterion before entering the main\n   loop of descent steps. As the stopping criterion requires the computation of the dual\n   gap, the screening happens whenever the dual gap is computed.\n-  By :user:`Christian Lorentzen <lorentzenchr>` :pr:`31882` and\n+  By :user:`Christian Lorentzen <lorentzenchr>` :pr:`31882`, :pr:`31986` and\ndiff --git a/sklearn/linear_model/_cd_fast.pyx b/sklearn/linear_model/_cd_fast.pyx\nindex e21c395bffb70..fc086e10c983f 100644\n--- a/sklearn/linear_model/_cd_fast.pyx\n+++ b/sklearn/linear_model/_cd_fast.pyx\n@@ -258,7 +258,7 @@ def enet_coordinate_descent(\n     cdef floating dual_norm_XtA\n     cdef unsigned int n_active = n_features\n     cdef uint32_t[::1] active_set\n-    # TODO: use binset insteaf of array of bools\n+    # TODO: use binset instead of array of bools\n     cdef uint8_t[::1] excluded_set\n     cdef unsigned int j\n     cdef unsigned int n_iter = 0\n@@ -359,7 +359,6 @@ def enet_coordinate_descent(\n                 gap, dual_norm_XtA = gap_enet(\n                     n_samples, n_features, w, alpha, beta, X, y, R, XtA, positive\n                 )\n-\n                 if gap <= tol:\n                     # return if we reached desired tolerance\n                     break\n@@ -1020,8 +1019,85 @@ def enet_coordinate_descent_gram(\n     return np.asarray(w), gap, tol, n_iter + 1\n \n \n+cdef (floating, floating) gap_enet_multi_task(\n+    int n_samples,\n+    int n_features,\n+    int n_tasks,\n+    const floating[::1, :] W,  # in\n+    floating l1_reg,\n+    floating l2_reg,\n+    const floating[::1, :] X,  # in\n+    const floating[::1, :] Y,  # in\n+    const floating[::1, :] R,  # in\n+    floating[:, ::1] XtA,  # out\n+    floating[::1] XtA_row_norms,  # out\n+) noexcept nogil:\n+    \"\"\"Compute dual gap for use in enet_coordinate_descent_multi_task.\n+\n+    Parameters\n+    ----------\n+    W : memoryview of shape (n_tasks, n_features)\n+    X : memoryview of shape (n_samples, n_features)\n+    Y : memoryview of shape (n_samples, n_tasks)\n+    R : memoryview of shape (n_samples, n_tasks)\n+        Current residuals = Y - X @ W.T\n+    XtA : memoryview of shape (n_features, n_tasks)\n+        Inplace calculated as XtA = X.T @ R - l2_reg * W.T\n+    XtA_row_norms : memoryview of shape n_features\n+        Inplace calculated as np.sqrt(np.sum(XtA ** 2, axis=1))\n+    \"\"\"\n+    cdef floating gap = 0.0\n+    cdef floating dual_norm_XtA\n+    cdef floating R_norm2\n+    cdef floating w_norm2 = 0.0\n+    cdef floating l21_norm\n+    cdef floating A_norm2\n+    cdef floating const_\n+    cdef unsigned int t, j\n+\n+    # XtA = X.T @ R - l2_reg * W.T\n+    for j in range(n_features):\n+        for t in range(n_tasks):\n+            XtA[j, t] = _dot(n_samples, &X[0, j], 1, &R[0, t], 1) - l2_reg * W[t, j]\n+\n+    # dual_norm_XtA = np.max(np.sqrt(np.sum(XtA ** 2, axis=1)))\n+    dual_norm_XtA = 0.0\n+    for j in range(n_features):\n+        # np.sqrt(np.sum(XtA ** 2, axis=1))\n+        XtA_row_norms[j] = _nrm2(n_tasks, &XtA[j, 0], 1)\n+        if XtA_row_norms[j] > dual_norm_XtA:\n+            dual_norm_XtA = XtA_row_norms[j]\n+\n+    # R_norm2 = linalg.norm(R, ord=\"fro\") ** 2\n+    R_norm2 = _dot(n_samples * n_tasks, &R[0, 0], 1, &R[0, 0], 1)\n+\n+    # w_norm2 = linalg.norm(W, ord=\"fro\") ** 2\n+    if l2_reg > 0:\n+        w_norm2 = _dot(n_features * n_tasks, &W[0, 0], 1, &W[0, 0], 1)\n+\n+    if (dual_norm_XtA > l1_reg):\n+        const_ = l1_reg / dual_norm_XtA\n+        A_norm2 = R_norm2 * (const_ ** 2)\n+        gap = 0.5 * (R_norm2 + A_norm2)\n+    else:\n+        const_ = 1.0\n+        gap = R_norm2\n+\n+    # l21_norm = np.sqrt(np.sum(W ** 2, axis=0)).sum()\n+    l21_norm = 0.0\n+    for ii in range(n_features):\n+        l21_norm += _nrm2(n_tasks, &W[0, ii], 1)\n+\n+    gap += (\n+        l1_reg * l21_norm\n+        - const_ * _dot(n_samples * n_tasks, &R[0, 0], 1, &Y[0, 0], 1)  # np.sum(R * Y)\n+        + 0.5 * l2_reg * (1 + const_ ** 2) * w_norm2\n+    )\n+    return gap, dual_norm_XtA\n+\n+\n def enet_coordinate_descent_multi_task(\n-    const floating[::1, :] W,\n+    floating[::1, :] W,\n     floating l1_reg,\n     floating l2_reg,\n     const floating[::1, :] X,\n@@ -1029,7 +1105,8 @@ def enet_coordinate_descent_multi_task(\n     unsigned int max_iter,\n     floating tol,\n     object rng,\n-    bint random=0\n+    bint random=0,\n+    bint do_screening=1,\n ):\n     \"\"\"Cython version of the coordinate descent algorithm\n         for Elastic-Net multi-task regression\n@@ -1072,29 +1149,29 @@ def enet_coordinate_descent_multi_task(\n         \"ij,ij->j\", X, X, dtype=dtype, order=\"C\"\n     )\n \n-    # to store XtA\n-    cdef floating[:, ::1] XtA = np.zeros((n_features, n_tasks), dtype=dtype)\n-    cdef floating XtA_axis1norm\n-    cdef floating dual_norm_XtA\n-\n     # initial value of the residuals\n-    cdef floating[::1, :] R = np.zeros((n_samples, n_tasks), dtype=dtype, order='F')\n+    cdef floating[::1, :] R = np.empty((n_samples, n_tasks), dtype=dtype, order='F')\n+    cdef floating[:, ::1] XtA = np.empty((n_features, n_tasks), dtype=dtype)\n+    cdef floating[::1] XtA_row_norms = np.empty(n_features, dtype=dtype)\n \n-    cdef floating[::1] tmp = np.zeros(n_tasks, dtype=dtype)\n-    cdef floating[::1] w_ii = np.zeros(n_tasks, dtype=dtype)\n+    cdef floating d_j\n+    cdef floating Xj_theta\n+    cdef floating[::1] tmp = np.empty(n_tasks, dtype=dtype)\n+    cdef floating[::1] w_j = np.empty(n_tasks, dtype=dtype)\n     cdef floating d_w_max\n     cdef floating w_max\n-    cdef floating d_w_ii\n+    cdef floating d_w_j\n     cdef floating nn\n-    cdef floating W_ii_abs_max\n+    cdef floating W_j_abs_max\n     cdef floating gap = tol + 1.0\n     cdef floating d_w_tol = tol\n-    cdef floating R_norm2\n-    cdef floating w_norm2\n-    cdef floating ry_sum\n-    cdef floating l21_norm\n-    cdef unsigned int ii\n-    cdef unsigned int jj\n+    cdef floating dual_norm_XtA\n+    cdef unsigned int n_active = n_features\n+    cdef uint32_t[::1] active_set\n+    # TODO: use binset instead of array of bools\n+    cdef uint8_t[::1] excluded_set\n+    cdef unsigned int j\n+    cdef unsigned int t\n     cdef unsigned int n_iter = 0\n     cdef unsigned int f_iter\n     cdef uint32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)\n@@ -1106,129 +1183,151 @@ def enet_coordinate_descent_multi_task(\n             \" results and is discouraged.\"\n         )\n \n+    if do_screening:\n+        active_set = np.empty(n_features, dtype=np.uint32)  # map [:n_active] -> j\n+        excluded_set = np.empty(n_features, dtype=np.uint8)\n+\n     with nogil:\n-        # R = Y - np.dot(X, W.T)\n+        # R = Y - X @ W.T\n         _copy(n_samples * n_tasks, &Y[0, 0], 1, &R[0, 0], 1)\n-        for ii in range(n_features):\n-            for jj in range(n_tasks):\n-                if W[jj, ii] != 0:\n-                    _axpy(n_samples, -W[jj, ii], &X[0, ii], 1,\n-                          &R[0, jj], 1)\n+        for j in range(n_features):\n+            for t in range(n_tasks):\n+                if W[t, j] != 0:\n+                    _axpy(n_samples, -W[t, j], &X[0, j], 1, &R[0, t], 1)\n \n         # tol = tol * linalg.norm(Y, ord='fro') ** 2\n         tol = tol * _nrm2(n_samples * n_tasks, &Y[0, 0], 1) ** 2\n \n+        # Check convergence before entering the main loop.\n+        gap, dual_norm_XtA = gap_enet_multi_task(\n+            n_samples, n_features, n_tasks, W, l1_reg, l2_reg, X, Y, R, XtA, XtA_row_norms\n+        )\n+        if gap <= tol:\n+            with gil:\n+                return np.asarray(W), gap, tol, 0\n+\n+        # Gap Safe Screening Rules for multi-task Lasso, see\n+        # https://arxiv.org/abs/1703.07285 Eq 2.2. (also arxiv:1506.03736)\n+        if do_screening:\n+            n_active = 0\n+            for j in range(n_features):\n+                if norm2_cols_X[j] == 0:\n+                    for t in range(n_tasks):\n+                        W[t, j] = 0\n+                    excluded_set[j] = 1\n+                    continue\n+                # Xj_theta = ||X[:,j] @ dual_theta||_2\n+                Xj_theta = XtA_row_norms[j] / fmax(l1_reg, dual_norm_XtA)\n+                d_j = (1 - Xj_theta) / sqrt(norm2_cols_X[j] + l2_reg)\n+                if d_j <= sqrt(2 * gap) / l1_reg:\n+                    # include feature j\n+                    active_set[n_active] = j\n+                    excluded_set[j] = 0\n+                    n_active += 1\n+                else:\n+                    # R += W[:, 1] * X[:, 1][:, None]\n+                    for t in range(n_tasks):\n+                        _axpy(n_samples, W[t, j], &X[0, j], 1, &R[0, t], 1)\n+                        W[t, j] = 0\n+                    excluded_set[j] = 1\n+\n         for n_iter in range(max_iter):\n             w_max = 0.0\n             d_w_max = 0.0\n-            for f_iter in range(n_features):  # Loop over coordinates\n+            for f_iter in range(n_active):  # Loop over coordinates\n                 if random:\n-                    ii = rand_int(n_features, rand_r_state)\n+                    j = rand_int(n_active, rand_r_state)\n                 else:\n-                    ii = f_iter\n+                    j = f_iter\n \n-                if norm2_cols_X[ii] == 0.0:\n+                if do_screening:\n+                    j = active_set[j]\n+\n+                if norm2_cols_X[j] == 0.0:\n                     continue\n \n-                # w_ii = W[:, ii] # Store previous value\n-                _copy(n_tasks, &W[0, ii], 1, &w_ii[0], 1)\n+                # w_j = W[:, j] # Store previous value\n+                _copy(n_tasks, &W[0, j], 1, &w_j[0], 1)\n \n-                # tmp = X[:, ii] @ (R + w_ii * X[:,ii][:, None])\n-                # first part: X[:, ii] @ R\n+                # tmp = X[:, j] @ (R + w_j * X[:,j][:, None])\n+                # first part: X[:, j] @ R\n                 #   Using BLAS Level 2:\n                 #   _gemv(RowMajor, Trans, n_samples, n_tasks, 1.0, &R[0, 0],\n-                #         n_tasks, &X[0, ii], 1, 0.0, &tmp[0], 1)\n-                # second part: (X[:, ii] @ X[:,ii]) * w_ii = norm2_cols * w_ii\n+                #         n_tasks, &X[0, j], 1, 0.0, &tmp[0], 1)\n+                # second part: (X[:, j] @ X[:,j]) * w_j = norm2_cols * w_j\n                 #   Using BLAS Level 1:\n-                #   _axpy(n_tasks, norm2_cols[ii], &w_ii[0], 1, &tmp[0], 1)\n+                #   _axpy(n_tasks, norm2_cols[j], &w_j[0], 1, &tmp[0], 1)\n                 # Using BLAS Level 1 (faster for small vectors like here):\n-                for jj in range(n_tasks):\n-                    tmp[jj] = _dot(n_samples, &X[0, ii], 1, &R[0, jj], 1)\n+                for t in range(n_tasks):\n+                    tmp[t] = _dot(n_samples, &X[0, j], 1, &R[0, t], 1)\n                     # As we have the loop already, we use it to replace the second BLAS\n                     # Level 1, i.e., _axpy, too.\n-                    tmp[jj] += w_ii[jj] * norm2_cols_X[ii]\n+                    tmp[t] += w_j[t] * norm2_cols_X[j]\n \n                 # nn = sqrt(np.sum(tmp ** 2))\n                 nn = _nrm2(n_tasks, &tmp[0], 1)\n \n-                # W[:, ii] = tmp * fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[ii] + l2_reg)\n-                _copy(n_tasks, &tmp[0], 1, &W[0, ii], 1)\n-                _scal(n_tasks, fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[ii] + l2_reg),\n-                      &W[0, ii], 1)\n+                # W[:, j] = tmp * fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[j] + l2_reg)\n+                _copy(n_tasks, &tmp[0], 1, &W[0, j], 1)\n+                _scal(n_tasks, fmax(1. - l1_reg / nn, 0) / (norm2_cols_X[j] + l2_reg),\n+                      &W[0, j], 1)\n \n                 # Update residual\n                 # Using numpy:\n-                #   R -= (W[:, ii] - w_ii) * X[:, ii][:, None]\n+                #   R -= (W[:, j] - w_j) * X[:, j][:, None]\n                 # Using BLAS Level 1 and 2:\n-                #   _axpy(n_tasks, -1.0, &W[0, ii], 1, &w_ii[0], 1)\n+                #   _axpy(n_tasks, -1.0, &W[0, j], 1, &w_j[0], 1)\n                 #   _ger(RowMajor, n_samples, n_tasks, 1.0,\n-                #        &X[0, ii], 1, &w_ii, 1,\n+                #        &X[0, j], 1, &w_j, 1,\n                 #        &R[0, 0], n_tasks)\n                 # Using BLAS Level 1 (faster for small vectors like here):\n-                for jj in range(n_tasks):\n-                    if W[jj, ii] != w_ii[jj]:\n-                        _axpy(n_samples, w_ii[jj] - W[jj, ii], &X[0, ii], 1,\n-                              &R[0, jj], 1)\n+                for t in range(n_tasks):\n+                    if W[t, j] != w_j[t]:\n+                        _axpy(n_samples, w_j[t] - W[t, j], &X[0, j], 1, &R[0, t], 1)\n \n                 # update the maximum absolute coefficient update\n-                d_w_ii = diff_abs_max(n_tasks, &W[0, ii], &w_ii[0])\n+                d_w_j = diff_abs_max(n_tasks, &W[0, j], &w_j[0])\n \n-                if d_w_ii > d_w_max:\n-                    d_w_max = d_w_ii\n+                if d_w_j > d_w_max:\n+                    d_w_max = d_w_j\n \n-                W_ii_abs_max = abs_max(n_tasks, &W[0, ii])\n-                if W_ii_abs_max > w_max:\n-                    w_max = W_ii_abs_max\n+                W_j_abs_max = abs_max(n_tasks, &W[0, j])\n+                if W_j_abs_max > w_max:\n+                    w_max = W_j_abs_max\n \n             if w_max == 0.0 or d_w_max / w_max <= d_w_tol or n_iter == max_iter - 1:\n                 # the biggest coordinate update of this iteration was smaller than\n                 # the tolerance: check the duality gap as ultimate stopping\n                 # criterion\n-\n-                # XtA = np.dot(X.T, R) - l2_reg * W.T\n-                for ii in range(n_features):\n-                    for jj in range(n_tasks):\n-                        XtA[ii, jj] = _dot(\n-                            n_samples, &X[0, ii], 1, &R[0, jj], 1\n-                            ) - l2_reg * W[jj, ii]\n-\n-                # dual_norm_XtA = np.max(np.sqrt(np.sum(XtA ** 2, axis=1)))\n-                dual_norm_XtA = 0.0\n-                for ii in range(n_features):\n-                    # np.sqrt(np.sum(XtA ** 2, axis=1))\n-                    XtA_axis1norm = _nrm2(n_tasks, &XtA[ii, 0], 1)\n-                    if XtA_axis1norm > dual_norm_XtA:\n-                        dual_norm_XtA = XtA_axis1norm\n-\n-                # R_norm2 = linalg.norm(R, ord='fro') ** 2\n-                # w_norm2 = linalg.norm(W, ord='fro') ** 2\n-                R_norm2 = _dot(n_samples * n_tasks, &R[0, 0], 1, &R[0, 0], 1)\n-                w_norm2 = _dot(n_features * n_tasks, &W[0, 0], 1, &W[0, 0], 1)\n-                if (dual_norm_XtA > l1_reg):\n-                    const_ = l1_reg / dual_norm_XtA\n-                    A_norm2 = R_norm2 * (const_ ** 2)\n-                    gap = 0.5 * (R_norm2 + A_norm2)\n-                else:\n-                    const_ = 1.0\n-                    gap = R_norm2\n-\n-                # ry_sum = np.sum(R * y)\n-                ry_sum = _dot(n_samples * n_tasks, &R[0, 0], 1, &Y[0, 0], 1)\n-\n-                # l21_norm = np.sqrt(np.sum(W ** 2, axis=0)).sum()\n-                l21_norm = 0.0\n-                for ii in range(n_features):\n-                    l21_norm += _nrm2(n_tasks, &W[0, ii], 1)\n-\n-                gap += (\n-                    l1_reg * l21_norm\n-                    - const_ * ry_sum\n-                    + 0.5 * l2_reg * (1 + const_ ** 2) * w_norm2\n+                gap, dual_norm_XtA = gap_enet_multi_task(\n+                    n_samples, n_features, n_tasks, W, l1_reg, l2_reg, X, Y, R, XtA, XtA_row_norms\n                 )\n-\n                 if gap <= tol:\n                     # return if we reached desired tolerance\n                     break\n+\n+                # Gap Safe Screening Rules for multi-task Lasso, see\n+                # https://arxiv.org/abs/1703.07285 Eq 2.2. (also arxiv:1506.03736)\n+                if do_screening:\n+                    n_active = 0\n+                    for j in range(n_features):\n+                        if norm2_cols_X[j] == 0:\n+                            continue\n+                        # Xj_theta = ||X[:,j] @ dual_theta||_2\n+                        Xj_theta = XtA_row_norms[j] / fmax(l1_reg, dual_norm_XtA)\n+                        d_j = (1 - Xj_theta) / sqrt(norm2_cols_X[j] + l2_reg)\n+                        if d_j <= sqrt(2 * gap) / l1_reg:\n+                            # include feature j\n+                            active_set[n_active] = j\n+                            excluded_set[j] = 0\n+                            n_active += 1\n+                        else:\n+                            # R += W[:, 1] * X[:, 1][:, None]\n+                            for t in range(n_tasks):\n+                                _axpy(n_samples, W[t, j], &X[0, j], 1, &R[0, t], 1)\n+                                W[t, j] = 0\n+                            excluded_set[j] = 1\n+\n         else:\n             # for/else, runs if for doesn't end with a `break`\n             with gil:\ndiff --git a/sklearn/linear_model/_coordinate_descent.py b/sklearn/linear_model/_coordinate_descent.py\nindex b29df23e142f7..9977249154dd2 100644\n--- a/sklearn/linear_model/_coordinate_descent.py\n+++ b/sklearn/linear_model/_coordinate_descent.py\n@@ -691,7 +691,7 @@ def enet_path(\n             )\n         elif multi_output:\n             model = cd_fast.enet_coordinate_descent_multi_task(\n-                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random\n+                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, do_screening\n             )\n         elif isinstance(precompute, np.ndarray):\n             # We expect precompute to be already Fortran ordered when bypassing\n@@ -3102,10 +3102,10 @@ class MultiTaskElasticNetCV(RegressorMixin, LinearModelCV):\n     ...         [[0, 0], [1, 1], [2, 2]])\n     MultiTaskElasticNetCV(cv=3)\n     >>> print(clf.coef_)\n-    [[0.52875032 0.46958558]\n-     [0.52875032 0.46958558]]\n+    [[0.51841231 0.479658]\n+     [0.51841231 0.479658]]\n     >>> print(clf.intercept_)\n-    [0.00166409 0.00166409]\n+    [0.001929... 0.001929...]\n     \"\"\"\n \n     _parameter_constraints: dict = {\n@@ -3356,7 +3356,7 @@ class MultiTaskLassoCV(RegressorMixin, LinearModelCV):\n     >>> r2_score(y, reg.predict(X))\n     0.9994\n     >>> reg.alpha_\n-    np.float64(0.5713)\n+    np.float64(0.4321...)\n     >>> reg.predict(X[:1,])\n     array([[153.7971,  94.9015]])\n     \"\"\"\ndiff --git a/sklearn/linear_model/tests/test_coordinate_descent.py b/sklearn/linear_model/tests/test_coordinate_descent.py\nindex 0b1ac1faa0a9c..ec43587bcc0ce 100644\n--- a/sklearn/linear_model/tests/test_coordinate_descent.py\n+++ b/sklearn/linear_model/tests/test_coordinate_descent.py\n@@ -702,7 +702,7 @@ def test_multitask_enet_and_lasso_cv():\n     X, y, _, _ = build_dataset(n_features=50, n_targets=3)\n     clf = MultiTaskElasticNetCV(cv=3).fit(X, y)\n     assert_almost_equal(clf.alpha_, 0.00556, 3)\n-    clf = MultiTaskLassoCV(cv=3).fit(X, y)\n+    clf = MultiTaskLassoCV(cv=3, tol=1e-6).fit(X, y)\n     assert_almost_equal(clf.alpha_, 0.00278, 3)\n \n     X, y, _, _ = build_dataset(n_targets=3)\n@@ -1233,7 +1233,7 @@ def test_multi_task_lasso_cv_dtype():\n     X = rng.binomial(1, 0.5, size=(n_samples, n_features))\n     X = X.astype(int)  # make it explicit that X is int\n     y = X[:, [0, 0]].copy()\n-    est = MultiTaskLassoCV(alphas=5, fit_intercept=True).fit(X, y)\n+    est = MultiTaskLassoCV(alphas=5, fit_intercept=True, tol=1e-6).fit(X, y)\n     assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n \n \n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_coordinate_descent.py",
    "sklearn/linear_model/tests/test_coordinate_descent.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-08-25T19:00:36Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32014",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/31882"
}