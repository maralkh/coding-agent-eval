{
  "id": "scikit-learn__scikit-learn-32077",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "ed0a98a22b9039ed4db6c943fabc0e4c4f80083f",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 32077,
  "pr_title": "FIX Run common tests on SparseCoder",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.decomposition/32077.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.decomposition/32077.enhancement.rst\nnew file mode 100644\nindex 0000000000000..aacff8ae1b76c\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.decomposition/32077.enhancement.rst\n@@ -0,0 +1,3 @@\n+- :class:`decomposition.SparseCoder` now follows the transformer API of scikit-learn.\n+  In addition, the :meth:`fit` method now validates the input and parameters.\n+  By :user:`Fran\u00e7ois Paugam <FrancoisPgm>`.\ndiff --git a/sklearn/decomposition/_dict_learning.py b/sklearn/decomposition/_dict_learning.py\nindex a1834dd29a8ce..2c3852abadbbf 100644\n--- a/sklearn/decomposition/_dict_learning.py\n+++ b/sklearn/decomposition/_dict_learning.py\n@@ -356,14 +356,11 @@ def sparse_encode(\n            [ 0.,  1.,  1.,  0.,  0.]])\n     \"\"\"\n     if check_input:\n-        if algorithm == \"lasso_cd\":\n-            dictionary = check_array(\n-                dictionary, order=\"C\", dtype=[np.float64, np.float32]\n-            )\n-            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\n-        else:\n-            dictionary = check_array(dictionary)\n-            X = check_array(X)\n+        order = \"C\" if algorithm == \"lasso_cd\" else None\n+        dictionary = check_array(\n+            dictionary, order=order, dtype=[np.float64, np.float32]\n+        )\n+        X = check_array(X, order=order, dtype=[np.float64, np.float32])\n \n     if dictionary.shape[1] != X.shape[1]:\n         raise ValueError(\n@@ -421,7 +418,7 @@ def _sparse_encode(\n             regularization = 1.0\n \n     if gram is None and algorithm != \"threshold\":\n-        gram = np.dot(dictionary, dictionary.T)\n+        gram = np.dot(dictionary, dictionary.T).astype(X.dtype, copy=False)\n \n     if cov is None and algorithm != \"lasso_cd\":\n         copy_cov = False\n@@ -1301,6 +1298,19 @@ class SparseCoder(_BaseSparseCoding, BaseEstimator):\n            [ 0.,  1.,  1.,  0.,  0.]])\n     \"\"\"\n \n+    _parameter_constraints: dict = {\n+        \"dictionary\": [\"array-like\"],\n+        \"transform_algorithm\": [\n+            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\n+        ],\n+        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\n+        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\n+        \"split_sign\": [\"boolean\"],\n+        \"n_jobs\": [Integral, None],\n+        \"positive_code\": [\"boolean\"],\n+        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\n+    }\n+\n     def __init__(\n         self,\n         dictionary,\n@@ -1324,16 +1334,17 @@ def __init__(\n         )\n         self.dictionary = dictionary\n \n+    @_fit_context(prefer_skip_nested_validation=True)\n     def fit(self, X, y=None):\n-        \"\"\"Do nothing and return the estimator unchanged.\n+        \"\"\"Only validate the parameters of the estimator.\n \n-        This method is just there to implement the usual API and hence\n-        work in pipelines.\n+        This method allows to: (i) validate the parameters of the estimator and\n+        (ii) be consistent with the scikit-learn transformer API.\n \n         Parameters\n         ----------\n-        X : Ignored\n-            Not used, present for API consistency by convention.\n+        X : array-like of shape (n_samples, n_features)\n+            Training data. Only used for input validation.\n \n         y : Ignored\n             Not used, present for API consistency by convention.\n@@ -1343,6 +1354,13 @@ def fit(self, X, y=None):\n         self : object\n             Returns the instance itself.\n         \"\"\"\n+        X = validate_data(self, X)\n+        self.n_components_ = self.dictionary.shape[0]\n+        if X.shape[1] != self.dictionary.shape[1]:\n+            raise ValueError(\n+                \"Dictionary and X have different numbers of features:\"\n+                f\"dictionary.shape: {self.dictionary.shape} X.shape{X.shape}\"\n+            )\n         return self\n \n     def transform(self, X, y=None):\n@@ -1353,7 +1371,7 @@ def transform(self, X, y=None):\n \n         Parameters\n         ----------\n-        X : ndarray of shape (n_samples, n_features)\n+        X : array-like of shape (n_samples, n_features)\n             Training vector, where `n_samples` is the number of samples\n             and `n_features` is the number of features.\n \n@@ -1389,16 +1407,6 @@ def __sklearn_tags__(self):\n         tags.transformer_tags.preserves_dtype = [\"float64\", \"float32\"]\n         return tags\n \n-    @property\n-    def n_components_(self):\n-        \"\"\"Number of atoms.\"\"\"\n-        return self.dictionary.shape[0]\n-\n-    @property\n-    def n_features_in_(self):\n-        \"\"\"Number of features seen during `fit`.\"\"\"\n-        return self.dictionary.shape[1]\n-\n     @property\n     def _n_features_out(self):\n         \"\"\"Number of transformed output features.\"\"\"\ndiff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex 8d747ea5e8c00..626496a230439 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -622,7 +622,7 @@ def test_sparse_coder_estimator():\n def test_sparse_coder_estimator_clone():\n     n_components = 12\n     rng = np.random.RandomState(0)\n-    V = rng.randn(n_components, n_features)  # random init\n+    V = rng.normal(size=(n_components, n_features))  # random init\n     V /= np.sum(V**2, axis=1)[:, np.newaxis]\n     coder = SparseCoder(\n         dictionary=V, transform_algorithm=\"lasso_lars\", transform_alpha=0.001\n@@ -631,8 +631,6 @@ def test_sparse_coder_estimator_clone():\n     assert id(cloned) != id(coder)\n     np.testing.assert_allclose(cloned.dictionary, coder.dictionary)\n     assert id(cloned.dictionary) != id(coder.dictionary)\n-    assert cloned.n_components_ == coder.n_components_\n-    assert cloned.n_features_in_ == coder.n_features_in_\n     data = np.random.rand(n_samples, n_features).astype(np.float32)\n     np.testing.assert_allclose(cloned.transform(data), coder.transform(data))\n \n@@ -677,10 +675,24 @@ def test_sparse_coder_common_transformer():\n \n def test_sparse_coder_n_features_in():\n     d = np.array([[1, 2, 3], [1, 2, 3]])\n+    X = np.array([[1, 2, 3]])\n     sc = SparseCoder(d)\n+    sc.fit(X)\n     assert sc.n_features_in_ == d.shape[1]\n \n \n+def test_sparse_encoder_feature_number_error():\n+    n_components = 10\n+    rng = np.random.RandomState(0)\n+    D = rng.uniform(size=(n_components, n_features))\n+    X = rng.uniform(size=(n_samples, n_features + 1))\n+    coder = SparseCoder(D)\n+    with pytest.raises(\n+        ValueError, match=\"Dictionary and X have different numbers of features\"\n+    ):\n+        coder.fit(X)\n+\n+\n def test_update_dict():\n     # Check the dict update in batch mode vs online mode\n     # Non-regression test for #4866\n@@ -958,7 +970,7 @@ def test_dict_learning_online_numerical_consistency(method):\n @pytest.mark.parametrize(\n     \"estimator\",\n     [\n-        SparseCoder(X.T),\n+        SparseCoder(rng_global.uniform(size=(n_features, n_features))),\n         DictionaryLearning(),\n         MiniBatchDictionaryLearning(batch_size=4, max_iter=10),\n     ],\ndiff --git a/sklearn/tests/test_common.py b/sklearn/tests/test_common.py\nindex a48ea5231560b..ec83b79d8c321 100644\n--- a/sklearn/tests/test_common.py\n+++ b/sklearn/tests/test_common.py\n@@ -39,6 +39,7 @@\n     _get_check_estimator_ids,\n     _get_expected_failed_checks,\n     _tested_estimators,\n+    _yield_instances_for_check,\n )\n from sklearn.utils._testing import (\n     SkipTest,\n@@ -256,24 +257,27 @@ def _estimators_that_predict_in_fit():\n \n \n @pytest.mark.parametrize(\n-    \"estimator\", column_name_estimators, ids=_get_check_estimator_ids\n+    \"estimator_orig\", column_name_estimators, ids=_get_check_estimator_ids\n )\n-def test_pandas_column_name_consistency(estimator):\n-    if isinstance(estimator, ColumnTransformer):\n+def test_pandas_column_name_consistency(estimator_orig):\n+    if isinstance(estimator_orig, ColumnTransformer):\n         pytest.skip(\"ColumnTransformer is not tested here\")\n     if \"check_dataframe_column_names_consistency\" in _get_expected_failed_checks(\n-        estimator\n+        estimator_orig\n     ):\n         pytest.skip(\n             \"Estimator does not support check_dataframe_column_names_consistency\"\n         )\n-    with ignore_warnings(category=(FutureWarning)):\n-        with warnings.catch_warnings(record=True) as record:\n-            check_dataframe_column_names_consistency(\n-                estimator.__class__.__name__, estimator\n-            )\n-        for warning in record:\n-            assert \"was fitted without feature names\" not in str(warning.message)\n+    for estimator in _yield_instances_for_check(\n+        check_dataframe_column_names_consistency, estimator_orig\n+    ):\n+        with ignore_warnings(category=(FutureWarning)):\n+            with warnings.catch_warnings(record=True) as record:\n+                check_dataframe_column_names_consistency(\n+                    estimator.__class__.__name__, estimator\n+                )\n+            for warning in record:\n+                assert \"was fitted without feature names\" not in str(warning.message)\n \n \n # TODO: As more modules support get_feature_names_out they should be removed\n@@ -347,21 +351,24 @@ def test_check_param_validation(estimator):\n \n \n @pytest.mark.parametrize(\n-    \"estimator\", SET_OUTPUT_ESTIMATORS, ids=_get_check_estimator_ids\n+    \"estimator_orig\", SET_OUTPUT_ESTIMATORS, ids=_get_check_estimator_ids\n )\n-def test_set_output_transform(estimator):\n-    name = estimator.__class__.__name__\n-    if not hasattr(estimator, \"set_output\"):\n+def test_set_output_transform(estimator_orig):\n+    name = estimator_orig.__class__.__name__\n+    if not hasattr(estimator_orig, \"set_output\"):\n         pytest.skip(\n             f\"Skipping check_set_output_transform for {name}: Does not support\"\n             \" set_output API\"\n         )\n-    with ignore_warnings(category=(FutureWarning)):\n-        check_set_output_transform(estimator.__class__.__name__, estimator)\n+    for estimator in _yield_instances_for_check(\n+        check_set_output_transform, estimator_orig\n+    ):\n+        with ignore_warnings(category=(FutureWarning)):\n+            check_set_output_transform(estimator.__class__.__name__, estimator)\n \n \n @pytest.mark.parametrize(\n-    \"estimator\", SET_OUTPUT_ESTIMATORS, ids=_get_check_estimator_ids\n+    \"estimator_orig\", SET_OUTPUT_ESTIMATORS, ids=_get_check_estimator_ids\n )\n @pytest.mark.parametrize(\n     \"check_func\",\n@@ -372,15 +379,16 @@ def test_set_output_transform(estimator):\n         check_global_set_output_transform_polars,\n     ],\n )\n-def test_set_output_transform_configured(estimator, check_func):\n-    name = estimator.__class__.__name__\n-    if not hasattr(estimator, \"set_output\"):\n+def test_set_output_transform_configured(estimator_orig, check_func):\n+    name = estimator_orig.__class__.__name__\n+    if not hasattr(estimator_orig, \"set_output\"):\n         pytest.skip(\n             f\"Skipping {check_func.__name__} for {name}: Does not support\"\n             \" set_output API yet\"\n         )\n-    with ignore_warnings(category=(FutureWarning)):\n-        check_func(estimator.__class__.__name__, estimator)\n+    for estimator in _yield_instances_for_check(check_func, estimator_orig):\n+        with ignore_warnings(category=(FutureWarning)):\n+            check_func(estimator.__class__.__name__, estimator)\n \n \n @pytest.mark.parametrize(\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex 1ceee3c9b847a..838c12ec40e3e 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -9,6 +9,8 @@\n from functools import partial\n from inspect import isfunction\n \n+import numpy as np\n+\n from sklearn import clone, config_context\n from sklearn.calibration import CalibratedClassifierCV\n from sklearn.cluster import (\n@@ -177,6 +179,8 @@\n \n CROSS_DECOMPOSITION = [\"PLSCanonical\", \"PLSRegression\", \"CCA\", \"PLSSVD\"]\n \n+rng = np.random.RandomState(0)\n+\n # The following dictionary is to indicate constructor arguments suitable for the test\n # suite, which uses very small datasets, and is intended to run rather quickly.\n INIT_PARAMS = {\n@@ -441,6 +445,7 @@\n     SGDClassifier: dict(max_iter=5),\n     SGDOneClassSVM: dict(max_iter=5),\n     SGDRegressor: dict(max_iter=5),\n+    SparseCoder: dict(dictionary=rng.normal(size=(5, 3))),\n     SparsePCA: dict(max_iter=5),\n     # Due to the jl lemma and often very few samples, the number\n     # of components of the random matrix projection will be probably\n@@ -711,6 +716,38 @@\n         ],\n     },\n     SkewedChi2Sampler: {\"check_dict_unchanged\": dict(n_components=1)},\n+    SparseCoder: {\n+        \"check_estimators_dtypes\": dict(dictionary=rng.normal(size=(5, 5))),\n+        \"check_dtype_object\": dict(dictionary=rng.normal(size=(5, 10))),\n+        \"check_transformers_unfitted_stateless\": dict(\n+            dictionary=rng.normal(size=(5, 5))\n+        ),\n+        \"check_fit_idempotent\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_transformer_preserve_dtypes\": dict(\n+            dictionary=rng.normal(size=(5, 3)).astype(np.float32)\n+        ),\n+        \"check_set_output_transform\": dict(dictionary=rng.normal(size=(5, 5))),\n+        \"check_global_output_transform_pandas\": dict(\n+            dictionary=rng.normal(size=(5, 5))\n+        ),\n+        \"check_set_output_transform_pandas\": dict(dictionary=rng.normal(size=(5, 5))),\n+        \"check_set_output_transform_polars\": dict(dictionary=rng.normal(size=(5, 5))),\n+        \"check_global_set_output_transform_polars\": dict(\n+            dictionary=rng.normal(size=(5, 5))\n+        ),\n+        \"check_dataframe_column_names_consistency\": dict(\n+            dictionary=rng.normal(size=(5, 8))\n+        ),\n+        \"check_estimators_overwrite_params\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_estimators_fit_returns_self\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_readonly_memmap_input\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_n_features_in_after_fitting\": dict(dictionary=rng.normal(size=(5, 4))),\n+        \"check_fit_check_is_fitted\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_n_features_in\": dict(dictionary=rng.normal(size=(5, 2))),\n+        \"check_positive_only_tag_during_fit\": dict(dictionary=rng.normal(size=(5, 4))),\n+        \"check_fit2d_1sample\": dict(dictionary=rng.normal(size=(5, 10))),\n+        \"check_fit2d_1feature\": dict(dictionary=rng.normal(size=(5, 1))),\n+    },\n     SparsePCA: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     SparseRandomProjection: {\"check_dict_unchanged\": dict(n_components=1)},\n     SpectralBiclustering: {\n@@ -748,7 +785,7 @@ def _tested_estimators(type_filter=None):\n                 yield estimator\n \n \n-SKIPPED_ESTIMATORS = [SparseCoder, FrozenEstimator]\n+SKIPPED_ESTIMATORS = [FrozenEstimator]\n \n \n def _construct_instances(Estimator):\n",
  "fail_to_pass": [
    "test_sparse_encoder_feature_number_error",
    "test_pandas_column_name_consistency",
    "test_set_output_transform",
    "test_set_output_transform_configured"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/decomposition/_dict_learning.py",
    "sklearn/decomposition/tests/test_dict_learning.py",
    "sklearn/tests/test_common.py",
    "sklearn/utils/_test_common/instance_generator.py"
  ],
  "difficulty": "hard",
  "created_at": "2025-09-02T10:24:08Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/32077",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}