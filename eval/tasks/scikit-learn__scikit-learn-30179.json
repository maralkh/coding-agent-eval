{
  "id": "scikit-learn__scikit-learn-30179",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "af7df5ced0eb1124df12dd389cc4ef7a9042837e",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30179,
  "pr_title": "FEAT rfecv: add support and ranking for each cv and step",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.feature_selection/30179.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.feature_selection/30179.enhancement.rst\nnew file mode 100644\nindex 0000000000000..97e147d81db10\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.feature_selection/30179.enhancement.rst\n@@ -0,0 +1,3 @@\n+- :class:`feature_selection.RFECV` now gives access to the ranking and support in each\n+ iteration and cv step of feature selection.\n+  By :user:`Marie S. <MarieSacksick>`\ndiff --git a/examples/feature_selection/plot_rfe_with_cross_validation.py b/examples/feature_selection/plot_rfe_with_cross_validation.py\nindex 4e3e45384e026..16e4a0e9454c5 100644\n--- a/examples/feature_selection/plot_rfe_with_cross_validation.py\n+++ b/examples/feature_selection/plot_rfe_with_cross_validation.py\n@@ -22,9 +22,12 @@\n \n from sklearn.datasets import make_classification\n \n+n_features = 15\n+feat_names = [f\"feature_{i}\" for i in range(15)]\n+\n X, y = make_classification(\n     n_samples=500,\n-    n_features=15,\n+    n_features=n_features,\n     n_informative=3,\n     n_redundant=2,\n     n_repeated=0,\n@@ -71,7 +74,12 @@\n import matplotlib.pyplot as plt\n import pandas as pd\n \n-cv_results = pd.DataFrame(rfecv.cv_results_)\n+data = {\n+    key: value\n+    for key, value in rfecv.cv_results_.items()\n+    if key in [\"n_features\", \"mean_test_score\", \"std_test_score\"]\n+}\n+cv_results = pd.DataFrame(data)\n plt.figure()\n plt.xlabel(\"Number of features selected\")\n plt.ylabel(\"Mean test accuracy\")\n@@ -91,3 +99,17 @@\n # cross-validation technique. The test accuracy decreases above 5 selected\n # features, this is, keeping non-informative features leads to over-fitting and\n # is therefore detrimental for the statistical performance of the models.\n+\n+# %%\n+import numpy as np\n+\n+for i in range(cv.n_splits):\n+    mask = rfecv.cv_results_[f\"split{i}_support\"][\n+        rfecv.n_features_\n+    ]  # mask of features selected by the RFE\n+    features_selected = np.ma.compressed(np.ma.masked_array(feat_names, mask=1 - mask))\n+    print(f\"Features selected in fold {i}: {features_selected}\")\n+# %%\n+# In the five folds, the selected features are consistant. This is good news,\n+# it means that the selection is stable accross folds, and it confirms that\n+# these features are the most informative ones.\ndiff --git a/sklearn/feature_selection/_rfe.py b/sklearn/feature_selection/_rfe.py\nindex 1c1a560c42dcf..d2bd78e225a54 100644\n--- a/sklearn/feature_selection/_rfe.py\n+++ b/sklearn/feature_selection/_rfe.py\n@@ -62,7 +62,7 @@ def _rfe_single_fit(rfe, estimator, X, y, train, test, scorer, routed_params):\n         **fit_params,\n     )\n \n-    return rfe.step_scores_, rfe.step_n_features_\n+    return rfe.step_scores_, rfe.step_support_, rfe.step_ranking_, rfe.step_n_features_\n \n \n class RFE(SelectorMixin, MetaEstimatorMixin, BaseEstimator):\n@@ -318,6 +318,8 @@ def _fit(self, X, y, step_score=None, **fit_params):\n         if step_score:\n             self.step_n_features_ = []\n             self.step_scores_ = []\n+            self.step_support_ = []\n+            self.step_ranking_ = []\n \n         # Elimination\n         while np.sum(support_) > n_features_to_select:\n@@ -331,6 +333,14 @@ def _fit(self, X, y, step_score=None, **fit_params):\n \n             estimator.fit(X[:, features], y, **fit_params)\n \n+            # Compute step values on the previous selection iteration because\n+            # 'estimator' must use features that have not been eliminated yet\n+            if step_score:\n+                self.step_n_features_.append(len(features))\n+                self.step_scores_.append(step_score(estimator, features))\n+                self.step_support_.append(list(support_))\n+                self.step_ranking_.append(list(ranking_))\n+\n             # Get importance and rank them\n             importances = _get_feature_importances(\n                 estimator,\n@@ -345,12 +355,6 @@ def _fit(self, X, y, step_score=None, **fit_params):\n             # Eliminate the worse features\n             threshold = min(step, np.sum(support_) - n_features_to_select)\n \n-            # Compute step score on the previous selection iteration\n-            # because 'estimator' must use features\n-            # that have not been eliminated yet\n-            if step_score:\n-                self.step_n_features_.append(len(features))\n-                self.step_scores_.append(step_score(estimator, features))\n             support_[features[ranks][:threshold]] = False\n             ranking_[np.logical_not(support_)] += 1\n \n@@ -359,10 +363,12 @@ def _fit(self, X, y, step_score=None, **fit_params):\n         self.estimator_ = clone(self.estimator)\n         self.estimator_.fit(X[:, features], y, **fit_params)\n \n-        # Compute step score when only n_features_to_select features left\n+        # Compute step values when only n_features_to_select features left\n         if step_score:\n             self.step_n_features_.append(len(features))\n             self.step_scores_.append(step_score(self.estimator_, features))\n+            self.step_support_.append(support_)\n+            self.step_ranking_.append(ranking_)\n         self.n_features_ = support_.sum()\n         self.support_ = support_\n         self.ranking_ = ranking_\n@@ -674,6 +680,20 @@ class RFECV(RFE):\n \n             .. versionadded:: 1.5\n \n+        split(k)_ranking : ndarray of shape (n_subsets_of_features,)\n+            The cross-validation rankings across (k)th fold.\n+            Selected (i.e., estimated best) features are assigned rank 1.\n+            Illustration in\n+            :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`\n+\n+            .. versionadded:: 1.7\n+\n+        split(k)_support : ndarray of shape (n_subsets_of_features,)\n+            The cross-validation supports across (k)th fold. The support\n+            is the mask of selected features.\n+\n+            .. versionadded:: 1.7\n+\n     n_features_ : int\n         The number of selected features with cross-validation.\n \n@@ -874,14 +894,16 @@ def fit(self, X, y, *, groups=None, **params):\n             parallel = Parallel(n_jobs=self.n_jobs)\n             func = delayed(_rfe_single_fit)\n \n-        scores_features = parallel(\n+        step_results = parallel(\n             func(clone(rfe), self.estimator, X, y, train, test, scorer, routed_params)\n             for train, test in cv.split(X, y, **routed_params.splitter.split)\n         )\n-        scores, step_n_features = zip(*scores_features)\n+        scores, supports, rankings, step_n_features = zip(*step_results)\n \n         step_n_features_rev = np.array(step_n_features[0])[::-1]\n         scores = np.array(scores)\n+        rankings = np.array(rankings)\n+        supports = np.array(supports)\n \n         # Reverse order such that lowest number of features is selected in case of tie.\n         scores_sum_rev = np.sum(scores, axis=0)[::-1]\n@@ -907,10 +929,14 @@ def fit(self, X, y, *, groups=None, **params):\n \n         # reverse to stay consistent with before\n         scores_rev = scores[:, ::-1]\n+        supports_rev = supports[:, ::-1]\n+        rankings_rev = rankings[:, ::-1]\n         self.cv_results_ = {\n             \"mean_test_score\": np.mean(scores_rev, axis=0),\n             \"std_test_score\": np.std(scores_rev, axis=0),\n             **{f\"split{i}_test_score\": scores_rev[i] for i in range(scores.shape[0])},\n+            **{f\"split{i}_ranking\": rankings_rev[i] for i in range(rankings.shape[0])},\n+            **{f\"split{i}_support\": supports_rev[i] for i in range(supports.shape[0])},\n             \"n_features\": step_n_features_rev,\n         }\n         return self\ndiff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py\nindex ae11de2fadf59..1f5672545874c 100644\n--- a/sklearn/feature_selection/tests/test_rfe.py\n+++ b/sklearn/feature_selection/tests/test_rfe.py\n@@ -2,6 +2,7 @@\n Testing Recursive feature elimination\n \"\"\"\n \n+import re\n from operator import attrgetter\n \n import numpy as np\n@@ -541,7 +542,11 @@ def test_rfecv_std_and_mean(global_random_seed):\n \n     rfecv = RFECV(estimator=SVC(kernel=\"linear\"))\n     rfecv.fit(X, y)\n-    split_keys = [key for key in rfecv.cv_results_.keys() if \"split\" in key]\n+    split_keys = [\n+        key\n+        for key in rfecv.cv_results_.keys()\n+        if re.search(r\"split\\d+_test_score\", key)\n+    ]\n     cv_scores = np.asarray([rfecv.cv_results_[key] for key in split_keys])\n     expected_mean = np.mean(cv_scores, axis=0)\n     expected_std = np.std(cv_scores, axis=0)\n@@ -721,3 +726,30 @@ def test_rfe_with_joblib_threading_backend(global_random_seed):\n         rfe.fit(X, y)\n \n     assert_array_equal(ranking_ref, rfe.ranking_)\n+\n+\n+def test_results_per_cv_in_rfecv(global_random_seed):\n+    \"\"\"\n+    Test that the results of RFECV are consistent across the different folds\n+    in terms of length of the arrays.\n+    \"\"\"\n+    X, y = make_classification(random_state=global_random_seed)\n+\n+    clf = LogisticRegression()\n+    rfecv = RFECV(\n+        estimator=clf,\n+        n_jobs=2,\n+        cv=5,\n+    )\n+\n+    rfecv.fit(X, y)\n+\n+    assert len(rfecv.cv_results_[\"split1_test_score\"]) == len(\n+        rfecv.cv_results_[\"split2_test_score\"]\n+    )\n+    assert len(rfecv.cv_results_[\"split1_support\"]) == len(\n+        rfecv.cv_results_[\"split2_support\"]\n+    )\n+    assert len(rfecv.cv_results_[\"split1_ranking\"]) == len(\n+        rfecv.cv_results_[\"split2_ranking\"]\n+    )\n",
  "fail_to_pass": [
    "test_results_per_cv_in_rfecv"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "examples/feature_selection/plot_rfe_with_cross_validation.py",
    "sklearn/feature_selection/_rfe.py",
    "sklearn/feature_selection/tests/test_rfe.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-10-30T18:23:02Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30179",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}