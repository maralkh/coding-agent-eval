{
  "id": "scikit-learn__scikit-learn-30196",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "1f593bfa435ced7cb141b96cb67cfd8c8ffced5c",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30196,
  "pr_title": "ENH Add parameter return_X_y to `make_classification`",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.datasets/30196.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.datasets/30196.enhancement.rst\nnew file mode 100644\nindex 0000000000000..d044d039badd2\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.datasets/30196.enhancement.rst\n@@ -0,0 +1,3 @@\n+- New parameter ``return_X_y`` added to :func:`datasets.make_classification`. The\n+  default value of the parameter does not change how the function behaves.\n+  By :user:`Success Moses <SuccessMoses>` and :user:`Adam Cooper <arc12>`\ndiff --git a/sklearn/datasets/_samples_generator.py b/sklearn/datasets/_samples_generator.py\nindex 291d545f26177..04810675f66a4 100644\n--- a/sklearn/datasets/_samples_generator.py\n+++ b/sklearn/datasets/_samples_generator.py\n@@ -14,6 +14,8 @@\n import scipy.sparse as sp\n from scipy import linalg\n \n+from sklearn.utils import Bunch\n+\n from ..preprocessing import MultiLabelBinarizer\n from ..utils import check_array, check_random_state\n from ..utils import shuffle as util_shuffle\n@@ -54,6 +56,7 @@ def _generate_hypercube(samples, dimensions, rng):\n         \"scale\": [Interval(Real, 0, None, closed=\"neither\"), \"array-like\", None],\n         \"shuffle\": [\"boolean\"],\n         \"random_state\": [\"random_state\"],\n+        \"return_X_y\": [\"boolean\"],\n     },\n     prefer_skip_nested_validation=True,\n )\n@@ -74,6 +77,7 @@ def make_classification(\n     scale=1.0,\n     shuffle=True,\n     random_state=None,\n+    return_X_y=True,\n ):\n     \"\"\"Generate a random n-class classification problem.\n \n@@ -168,13 +172,32 @@ def make_classification(\n         for reproducible output across multiple function calls.\n         See :term:`Glossary <random_state>`.\n \n+    return_X_y : bool, default=True\n+        If True, a tuple ``(X, y)`` instead of a Bunch object is returned.\n+\n+        .. versionadded:: 1.7\n+\n     Returns\n     -------\n-    X : ndarray of shape (n_samples, n_features)\n-        The generated samples.\n-\n-    y : ndarray of shape (n_samples,)\n-        The integer labels for class membership of each sample.\n+    data : :class:`~sklearn.utils.Bunch` if `return_X_y` is `False`.\n+        Dictionary-like object, with the following attributes.\n+\n+        DESCR : str\n+            A description of the function that generated the dataset.\n+        parameter : dict\n+            A dictionary that stores the values of the arguments passed to the\n+            generator function.\n+        feature_info : list of len(n_features)\n+            A description for each generated feature.\n+        X : ndarray of shape (n_samples, n_features)\n+            The generated samples.\n+        y : ndarray of shape (n_samples,)\n+            An integer label for class membership of each sample.\n+\n+        .. versionadded:: 1.7\n+\n+    (X, y) : tuple if ``return_X_y`` is True\n+        A tuple of generated samples and labels.\n \n     See Also\n     --------\n@@ -220,25 +243,28 @@ def make_classification(\n         )\n \n     if weights is not None:\n+        # we define new variable, weight_, instead of modifying user defined parameter.\n         if len(weights) not in [n_classes, n_classes - 1]:\n             raise ValueError(\n                 \"Weights specified but incompatible with number of classes.\"\n             )\n         if len(weights) == n_classes - 1:\n             if isinstance(weights, list):\n-                weights = weights + [1.0 - sum(weights)]\n+                weights_ = weights + [1.0 - sum(weights)]\n             else:\n-                weights = np.resize(weights, n_classes)\n-                weights[-1] = 1.0 - sum(weights[:-1])\n+                weights_ = np.resize(weights, n_classes)\n+                weights_[-1] = 1.0 - sum(weights_[:-1])\n+        else:\n+            weights_ = weights.copy()\n     else:\n-        weights = [1.0 / n_classes] * n_classes\n+        weights_ = [1.0 / n_classes] * n_classes\n \n-    n_useless = n_features - n_informative - n_redundant - n_repeated\n+    n_random = n_features - n_informative - n_redundant - n_repeated\n     n_clusters = n_classes * n_clusters_per_class\n \n     # Distribute samples among clusters by weight\n     n_samples_per_cluster = [\n-        int(n_samples * weights[k % n_classes] / n_clusters_per_class)\n+        int(n_samples * weights_[k % n_classes] / n_clusters_per_class)\n         for k in range(n_clusters)\n     ]\n \n@@ -282,14 +308,14 @@ def make_classification(\n         )\n \n     # Repeat some features\n+    n = n_informative + n_redundant\n     if n_repeated > 0:\n-        n = n_informative + n_redundant\n         indices = ((n - 1) * generator.uniform(size=n_repeated) + 0.5).astype(np.intp)\n         X[:, n : n + n_repeated] = X[:, indices]\n \n     # Fill useless features\n-    if n_useless > 0:\n-        X[:, -n_useless:] = generator.standard_normal(size=(n_samples, n_useless))\n+    if n_random > 0:\n+        X[:, -n_random:] = generator.standard_normal(size=(n_samples, n_random))\n \n     # Randomly replace labels\n     if flip_y >= 0.0:\n@@ -305,16 +331,56 @@ def make_classification(\n         scale = 1 + 100 * generator.uniform(size=n_features)\n     X *= scale\n \n+    indices = np.arange(n_features)\n     if shuffle:\n         # Randomly permute samples\n         X, y = util_shuffle(X, y, random_state=generator)\n \n         # Randomly permute features\n-        indices = np.arange(n_features)\n         generator.shuffle(indices)\n         X[:, :] = X[:, indices]\n \n-    return X, y\n+    if return_X_y:\n+        return X, y\n+\n+    # feat_desc describes features in X\n+    feat_desc = [\"random\"] * n_features\n+    for i, index in enumerate(indices):\n+        if index < n_informative:\n+            feat_desc[i] = \"informative\"\n+        elif n_informative <= index < n_informative + n_redundant:\n+            feat_desc[i] = \"redundant\"\n+        elif n <= index < n + n_repeated:\n+            feat_desc[i] = \"repeated\"\n+\n+    parameters = {\n+        \"n_samples\": n_samples,\n+        \"n_features\": n_features,\n+        \"n_informative\": n_informative,\n+        \"n_redundant\": n_redundant,\n+        \"n_repeated\": n_repeated,\n+        \"n_classes\": n_classes,\n+        \"n_clusters_per_class\": n_clusters_per_class,\n+        \"weights\": weights,\n+        \"flip_y\": flip_y,\n+        \"class_sep\": class_sep,\n+        \"hypercube\": hypercube,\n+        \"shift\": shift,\n+        \"scale\": scale,\n+        \"shuffle\": shuffle,\n+        \"random_state\": random_state,\n+        \"return_X_y\": return_X_y,\n+    }\n+\n+    bunch = Bunch(\n+        DESCR=make_classification.__doc__,\n+        parameters=parameters,\n+        feature_info=feat_desc,\n+        X=X,\n+        y=y,\n+    )\n+\n+    return bunch\n \n \n @validate_params(\ndiff --git a/sklearn/datasets/tests/test_samples_generator.py b/sklearn/datasets/tests/test_samples_generator.py\nindex f4bc6384f763f..5611f8d2d02ac 100644\n--- a/sklearn/datasets/tests/test_samples_generator.py\n+++ b/sklearn/datasets/tests/test_samples_generator.py\n@@ -184,6 +184,57 @@ def test_make_classification_informative_features():\n         make(n_features=2, n_informative=2, n_classes=3, n_clusters_per_class=2)\n \n \n+def test_make_classification_return_x_y():\n+    \"\"\"\n+    Test that make_classification returns a Bunch when return_X_y is False.\n+\n+    Also that bunch.X is the same as X\n+    \"\"\"\n+\n+    kwargs = {\n+        \"n_samples\": 100,\n+        \"n_features\": 20,\n+        \"n_informative\": 5,\n+        \"n_redundant\": 1,\n+        \"n_repeated\": 1,\n+        \"n_classes\": 3,\n+        \"n_clusters_per_class\": 2,\n+        \"weights\": None,\n+        \"flip_y\": 0.01,\n+        \"class_sep\": 1.0,\n+        \"hypercube\": True,\n+        \"shift\": 0.0,\n+        \"scale\": 1.0,\n+        \"shuffle\": True,\n+        \"random_state\": 42,\n+        \"return_X_y\": True,\n+    }\n+\n+    X, y = make_classification(**kwargs)\n+\n+    kwargs[\"return_X_y\"] = False\n+    bunch = make_classification(**kwargs)\n+\n+    assert (\n+        hasattr(bunch, \"DESCR\")\n+        and hasattr(bunch, \"parameters\")\n+        and hasattr(bunch, \"feature_info\")\n+        and hasattr(bunch, \"X\")\n+        and hasattr(bunch, \"y\")\n+    )\n+\n+    def count(str_):\n+        return bunch.feature_info.count(str_)\n+\n+    assert np.array_equal(X, bunch.X)\n+    assert np.array_equal(y, bunch.y)\n+    assert bunch.DESCR == make_classification.__doc__\n+    assert bunch.parameters == kwargs\n+    assert count(\"informative\") == kwargs[\"n_informative\"]\n+    assert count(\"redundant\") == kwargs[\"n_redundant\"]\n+    assert count(\"repeated\") == kwargs[\"n_repeated\"]\n+\n+\n @pytest.mark.parametrize(\n     \"weights, err_type, err_msg\",\n     [\n",
  "fail_to_pass": [
    "test_make_classification_return_x_y"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/datasets/_samples_generator.py",
    "sklearn/datasets/tests/test_samples_generator.py"
  ],
  "difficulty": "medium",
  "created_at": "2024-11-02T13:14:14Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30196",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}