{
  "id": "scikit-learn__scikit-learn-30022",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "1d6cfde48194c36dd144d29849638ba9e04cd12f",
  "issue_number": 30016,
  "issue_title": "TfidfVectorizer does not preserve dtype for large size inputs",
  "issue_body": "### Describe the bug\r\n\r\nAfter fitting `TfidfVectorizer`, its `idf_` has `dtype` `np.float64` regardless of the provided `dtype` when the input data are large. The conversion from `np.float32` to `np.float64` happens [here](https://github.com/scikit-learn/scikit-learn/blob/d5082d32de2797f9594c9477f2810c743560a1f1/sklearn/feature_extraction/text.py#L1666).\r\n\r\nNot propagating `dtype` to `TfidfTransformer` has been [discussed](https://github.com/scikit-learn/scikit-learn/pull/10443) in the past. Back then passing `dtype` to `np.log` or adding `.astype(dtype)` to `np.log` were not approved.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nimport pandas as pd\r\nimport numpy as np\r\nimport uuid\r\n\r\n#check for 100 strings with output as np.float32, works fine\r\nsmall_data=[str(uuid.uuid4()) for i in range(100)]\r\nX = pd.Series(small_data)\r\nvectorizer = TfidfVectorizer(dtype=np.float32)\r\nvectorizer.fit(X)\r\nprint(vectorizer.idf_.dtype)\r\n\r\n#check for 1000000 strings with output as np.float32\r\n#the output of the following has dtype np.float64\r\nlarge_data=[str(uuid.uuid4()) for i in range(1000000)]\r\nX = pd.Series(large_data)\r\nvectorizer = TfidfVectorizer(dtype=np.float32)\r\nvectorizer.fit(X)\r\nprint(vectorizer.idf_.dtype)\r\n```\r\n\r\n### Expected Results\r\n\r\n```python\r\nfloat32\r\nfloat32\r\n```\r\n\r\n### Actual Results\r\n\r\n```python\r\nfloat32\r\nfloat64\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.13 (main, Feb  7 2024, 08:26:19) [Clang 15.0.0 (clang-1500.1.0.2.5)]\r\nexecutable: .../bin/python\r\n   machine: macOS-14.6.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.2\r\n          pip: 24.2\r\n   setuptools: 65.5.0\r\n        numpy: 1.23.5\r\n        scipy: 1.14.1\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 12\r\n         prefix: libopenblas\r\n       filepath: .../lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: libomp\r\n       filepath: .../lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib\r\n        version: None\r\n```\r\n",
  "pr_number": 30022,
  "pr_title": "FIX make sure that TFIDFVectorizer set idf_ dtype based on X.dtype",
  "gold_patch": "diff --git a/doc/whats_new/v1.6.rst b/doc/whats_new/v1.6.rst\nindex 629944b7e52be..9a9d655d48e37 100644\n--- a/doc/whats_new/v1.6.rst\n+++ b/doc/whats_new/v1.6.rst\n@@ -255,6 +255,13 @@ Changelog\n   and will be removed in 1.8.\n   :pr:`29997` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n \n+:mod:`sklearn.feature_extraction.text`\n+......................................\n+\n+- |Fix| :class:`feature_extraction.text.TfidfVectorizer` now correctly preserves the\n+  `dtype` of `idf_` based on the input data.\n+  :pr:`30022` by :user:`Guillaume Lemaitre <glemaitre>`.\n+\n :mod:`sklearn.impute`\n .....................\n \ndiff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py\nindex b064606542236..ab3f84668fd2d 100644\n--- a/sklearn/feature_extraction/tests/test_text.py\n+++ b/sklearn/feature_extraction/tests/test_text.py\n@@ -1,5 +1,6 @@\n import pickle\n import re\n+import uuid\n import warnings\n from collections import defaultdict\n from collections.abc import Mapping\n@@ -1613,3 +1614,15 @@ def test_tfidf_transformer_copy(csr_container):\n     assert X_transform is X_csr\n     with pytest.raises(AssertionError):\n         assert_allclose_dense_sparse(X_csr, X_csr_original)\n+\n+\n+@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n+def test_tfidf_vectorizer_perserve_dtype_idf(dtype):\n+    \"\"\"Check that `idf_` has the same dtype as the input data.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/30016\n+    \"\"\"\n+    X = [str(uuid.uuid4()) for i in range(100_000)]\n+    vectorizer = TfidfVectorizer(dtype=dtype).fit(X)\n+    assert vectorizer.idf_.dtype == dtype\ndiff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py\nindex 8105ab3a48f4b..2f21b3ccbe254 100644\n--- a/sklearn/feature_extraction/text.py\n+++ b/sklearn/feature_extraction/text.py\n@@ -1662,8 +1662,13 @@ def fit(self, X, y=None):\n \n             # log+1 instead of log makes sure terms with zero idf don't get\n             # suppressed entirely.\n+            # Force the dtype of `idf_` to be the same as `df`. In NumPy < 2, the dtype\n+            # was depending on the value of `n_samples`.\n+            self.idf_ = np.full_like(df, fill_value=n_samples, dtype=dtype)\n+            self.idf_ /= df\n             # `np.log` preserves the dtype of `df` and thus `dtype`.\n-            self.idf_ = np.log(n_samples / df) + 1.0\n+            np.log(self.idf_, out=self.idf_)\n+            self.idf_ += 1.0\n \n         return self\n \n",
  "fail_to_pass": [
    "test_tfidf_vectorizer_perserve_dtype_idf"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/feature_extraction/tests/test_text.py",
    "sklearn/feature_extraction/text.py"
  ],
  "difficulty": "medium",
  "created_at": "2024-10-07T12:26:20Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30022",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30016"
}