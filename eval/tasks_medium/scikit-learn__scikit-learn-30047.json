{
  "id": "scikit-learn__scikit-learn-30047",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "c08b4332a3358f0090c8e3873aedde815908e248",
  "issue_number": 29722,
  "issue_title": "Make `KNeighborsClassifier.predict` and `KNeighborsRegressor.predict` react the same way to `X=None`",
  "issue_body": "### Describe the workflow you want to enable\r\n\r\nCurrently `KNeighborsRegressor.predict()` accepts `None` as input, in which case it returns prediction for all samples in the training set based on the nearest neighbors not including the sample itself (consistent with `NearestNeighbors` behavior). However, `KNeighborsClassifier.predict()` does not accept `None` as input. This is inconsistent and should arguably be harmonized:\r\n\r\n```Python\r\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors\r\nimport numpy as np\r\n\r\nX = np.random.normal(size=(10, 5))\r\ny = np.random.normal(size=(10, 1))\r\n\r\nknn = NearestNeighbors(n_neighbors=3)\r\nknn.fit(X)\r\nknn.kneighbors() # works\r\n\r\nknn = KNeighborsRegressor(n_neighbors=3)\r\nknn.fit(X, y)\r\nknn.predict(None) # works (NB: does not work without \"None\")\r\n\r\nknn = KNeighborsClassifier(n_neighbors=3)\r\nknn.fit(X, np.ravel(y) > 0)\r\nknn.predict(None) # fails with an error\r\n```\r\n\r\n### Describe your proposed solution\r\n\r\nMy proposed solution is to make `KNeighborsClassifier.predict(None)` behave the same as `KNeighborsRegressor.predict(None)`. As explained in https://github.com/scikit-learn/scikit-learn/issues/27747, the necessary fix requires changing only two lines of code.\r\n\r\n\r\n### Additional context\r\n\r\nAs explained in https://github.com/scikit-learn/scikit-learn/issues/27747, this would be a great feature, super useful and convenient for computing LOOCV accuracy simply via `score(None, y)`. Using `score(X, y)` where `X` is the training set used in `fit(X)` gives a biased result because each (training set) sample gets included into its own neighbors.",
  "pr_number": 30047,
  "pr_title": "ENH Make `KNeighborsClassifier.predict` handle `X=None`",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.neighbors/30047.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.neighbors/30047.enhancement.rst\nnew file mode 100644\nindex 0000000000000..ed91b39ed2e0d\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.neighbors/30047.enhancement.rst\n@@ -0,0 +1,6 @@\n+- Make `predict`, `predict_proba`, and `score` of\n+  :class:`neighbors.KNeighborsClassifier` and\n+  :class:`neighbors.RadiusNeighborsClassifier` accept `X=None` as input. In this case\n+  predictions for all training set points are returned, and points are not included\n+  into their own neighbors.\n+  :pr:`30047` by :user:`Dmitry Kobak <dkobak>`.\ndiff --git a/sklearn/neighbors/_classification.py b/sklearn/neighbors/_classification.py\nindex b63381af84602..5f44a0ecca603 100644\n--- a/sklearn/neighbors/_classification.py\n+++ b/sklearn/neighbors/_classification.py\n@@ -244,8 +244,10 @@ def predict(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\n@@ -281,7 +283,7 @@ def predict(self, X):\n             classes_ = [self.classes_]\n \n         n_outputs = len(classes_)\n-        n_queries = _num_samples(X)\n+        n_queries = _num_samples(self._fit_X if X is None else X)\n         weights = _get_weights(neigh_dist, self.weights)\n         if weights is not None and _all_with_any_reduction_axis_1(weights, value=0):\n             raise ValueError(\n@@ -311,8 +313,10 @@ def predict_proba(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\n@@ -375,7 +379,7 @@ def predict_proba(self, X):\n             _y = self._y.reshape((-1, 1))\n             classes_ = [self.classes_]\n \n-        n_queries = _num_samples(X)\n+        n_queries = _num_samples(self._fit_X if X is None else X)\n \n         weights = _get_weights(neigh_dist, self.weights)\n         if weights is None:\n@@ -408,6 +412,39 @@ def predict_proba(self, X):\n \n         return probabilities\n \n+    # This function is defined here only to modify the parent docstring\n+    # and add information about X=None\n+    def score(self, X, y, sample_weight=None):\n+        \"\"\"\n+        Return the mean accuracy on the given test data and labels.\n+\n+        In multi-label classification, this is the subset accuracy\n+        which is a harsh metric since you require for each sample that\n+        each label set be correctly predicted.\n+\n+        Parameters\n+        ----------\n+        X : array-like of shape (n_samples, n_features), or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            used; in this case, points are not considered their own\n+            neighbors. This means that `knn.fit(X, y).score(None, y)`\n+            implicitly performs a leave-one-out cross-validation procedure\n+            and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n+            but typically much faster.\n+\n+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n+            True labels for `X`.\n+\n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n+        Returns\n+        -------\n+        score : float\n+            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n+        \"\"\"\n+        return super().score(X, y, sample_weight)\n+\n     def __sklearn_tags__(self):\n         tags = super().__sklearn_tags__()\n         tags.classifier_tags.multi_label = True\n@@ -692,8 +729,10 @@ def predict(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\n@@ -734,8 +773,10 @@ def predict_proba(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\n@@ -745,7 +786,7 @@ def predict_proba(self, X):\n             by lexicographic order.\n         \"\"\"\n         check_is_fitted(self, \"_fit_method\")\n-        n_queries = _num_samples(X)\n+        n_queries = _num_samples(self._fit_X if X is None else X)\n \n         metric, metric_kwargs = _adjusted_metric(\n             metric=self.metric, metric_kwargs=self.metric_params, p=self.p\n@@ -846,6 +887,39 @@ def predict_proba(self, X):\n \n         return probabilities\n \n+    # This function is defined here only to modify the parent docstring\n+    # and add information about X=None\n+    def score(self, X, y, sample_weight=None):\n+        \"\"\"\n+        Return the mean accuracy on the given test data and labels.\n+\n+        In multi-label classification, this is the subset accuracy\n+        which is a harsh metric since you require for each sample that\n+        each label set be correctly predicted.\n+\n+        Parameters\n+        ----------\n+        X : array-like of shape (n_samples, n_features), or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            used; in this case, points are not considered their own\n+            neighbors. This means that `knn.fit(X, y).score(None, y)`\n+            implicitly performs a leave-one-out cross-validation procedure\n+            and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n+            but typically much faster.\n+\n+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n+            True labels for `X`.\n+\n+        sample_weight : array-like of shape (n_samples,), default=None\n+            Sample weights.\n+\n+        Returns\n+        -------\n+        score : float\n+            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n+        \"\"\"\n+        return super().score(X, y, sample_weight)\n+\n     def __sklearn_tags__(self):\n         tags = super().__sklearn_tags__()\n         tags.classifier_tags.multi_label = True\ndiff --git a/sklearn/neighbors/_regression.py b/sklearn/neighbors/_regression.py\nindex 8410a140b9eb1..f324d3fb7e2f2 100644\n--- a/sklearn/neighbors/_regression.py\n+++ b/sklearn/neighbors/_regression.py\n@@ -234,8 +234,10 @@ def predict(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\n@@ -464,8 +466,10 @@ def predict(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n-                or (n_queries, n_indexed) if metric == 'precomputed'\n-            Test samples.\n+                or (n_queries, n_indexed) if metric == 'precomputed', or None\n+            Test samples. If `None`, predictions for all indexed points are\n+            returned; in this case, points are not considered their own\n+            neighbors.\n \n         Returns\n         -------\ndiff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py\nindex cb6acb65cb1cc..b480847ed1f45 100644\n--- a/sklearn/neighbors/tests/test_neighbors.py\n+++ b/sklearn/neighbors/tests/test_neighbors.py\n@@ -24,7 +24,12 @@\n     assert_compatible_argkmin_results,\n     assert_compatible_radius_results,\n )\n-from sklearn.model_selection import cross_val_score, train_test_split\n+from sklearn.model_selection import (\n+    LeaveOneOut,\n+    cross_val_predict,\n+    cross_val_score,\n+    train_test_split,\n+)\n from sklearn.neighbors import (\n     VALID_METRICS_SPARSE,\n     KNeighborsRegressor,\n@@ -2390,3 +2395,41 @@ def _weights(dist):\n \n     with pytest.raises(ValueError, match=msg):\n         est.predict_proba([[1.1, 1.1]])\n+\n+\n+@pytest.mark.parametrize(\n+    \"nn_model\",\n+    [\n+        neighbors.KNeighborsClassifier(n_neighbors=10),\n+        neighbors.RadiusNeighborsClassifier(radius=5.0),\n+    ],\n+)\n+def test_neighbor_classifiers_loocv(nn_model):\n+    \"\"\"Check that `predict` and related functions work fine with X=None\"\"\"\n+    X, y = datasets.make_blobs(n_samples=500, centers=5, n_features=2, random_state=0)\n+\n+    loocv = cross_val_score(nn_model, X, y, cv=LeaveOneOut())\n+    nn_model.fit(X, y)\n+\n+    assert np.all(loocv == (nn_model.predict(None) == y))\n+    assert np.mean(loocv) == nn_model.score(None, y)\n+    assert nn_model.score(None, y) < nn_model.score(X, y)\n+\n+\n+@pytest.mark.parametrize(\n+    \"nn_model\",\n+    [\n+        neighbors.KNeighborsRegressor(n_neighbors=10),\n+        neighbors.RadiusNeighborsRegressor(radius=0.5),\n+    ],\n+)\n+def test_neighbor_regressors_loocv(nn_model):\n+    \"\"\"Check that `predict` and related functions work fine with X=None\"\"\"\n+    X, y = datasets.load_diabetes(return_X_y=True)\n+\n+    # Only checking cross_val_predict and not cross_val_score because\n+    # cross_val_score does not work with LeaveOneOut() for a regressor\n+    loocv = cross_val_predict(nn_model, X, y, cv=LeaveOneOut())\n+    nn_model.fit(X, y)\n+\n+    assert np.all(loocv == nn_model.predict(None))\n",
  "fail_to_pass": [
    "test_neighbor_classifiers_loocv",
    "test_neighbor_regressors_loocv"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/neighbors/_classification.py",
    "sklearn/neighbors/_regression.py",
    "sklearn/neighbors/tests/test_neighbors.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-10-11T14:37:45Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30047",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/29722"
}