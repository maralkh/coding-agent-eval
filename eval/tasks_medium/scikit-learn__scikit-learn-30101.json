{
  "id": "scikit-learn__scikit-learn-30101",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "595c5ca1a2f879fbd4bae99a31249e3992104917",
  "issue_number": 30047,
  "issue_title": "ENH Make `KNeighborsClassifier.predict` handle `X=None`",
  "issue_body": "#### Reference Issues/PRs\r\n\r\nFixes #29722 and #27747.\r\n\r\nThis makes `predict()`, `predict_proba()`, and `score()` of `KNeighborsClassifier` and `RadiusNeighborsClassifier` take `X=None`. In this case predictions for all training set points are returned, and points are NOT included into their own neighbors. This makes it consistent with the current behaviour of `KNeighborsRegressor` and `RadiusNeighborsRegressor`.\r\n\r\nNew functionality is described in docstrings (and added to the docstrings of `KNeighborsRegressor` and `RadiusNeighborsRegressor`).\r\n\r\nTests are added confirming that `knn.fit(X, y).predict(None)` is equivalent to `cross_val_predict(knn, X, y, cv=LeaveOneOut())`.\r\n\r\n#### Sanity checking\r\n\r\n@ogrisel asked me to add a notebook showing that this is a statistically sane thing to do (https://github.com/scikit-learn/scikit-learn/issues/29722#issuecomment-2328991557):\r\n\r\n> It would also be great to check in a side simulation study (in a notebook) to show that the variance of the LOO CV scheme is not significantly larger than the one obtained fors k-fold CV with k in {5, 10, 50} and for n_neighbors in {1, 10, 100} to confirm the outcome of the literature review in https://github.com/scikit-learn/scikit-learn/issues/27747#issuecomment-1806854282. This side notebook should not part of the scikit-learn repo but linked in the PR to help assert that the new feature exposed in the PR is a methodologically sane thing to do.\r\n\r\nI did this here: https://github.com/scikit-learn/scikit-learn/issues/29722#issuecomment-2380580710. Is it sufficient to simply leave it there?",
  "pr_number": 30101,
  "pr_title": "TST Improve tests for neighbor models with X=None",
  "gold_patch": "diff --git a/sklearn/neighbors/tests/test_neighbors.py b/sklearn/neighbors/tests/test_neighbors.py\nindex b480847ed1f45..9622e85c39718 100644\n--- a/sklearn/neighbors/tests/test_neighbors.py\n+++ b/sklearn/neighbors/tests/test_neighbors.py\n@@ -2401,18 +2401,40 @@ def _weights(dist):\n     \"nn_model\",\n     [\n         neighbors.KNeighborsClassifier(n_neighbors=10),\n-        neighbors.RadiusNeighborsClassifier(radius=5.0),\n+        neighbors.RadiusNeighborsClassifier(),\n     ],\n )\n-def test_neighbor_classifiers_loocv(nn_model):\n-    \"\"\"Check that `predict` and related functions work fine with X=None\"\"\"\n-    X, y = datasets.make_blobs(n_samples=500, centers=5, n_features=2, random_state=0)\n+@pytest.mark.parametrize(\"algorithm\", ALGORITHMS)\n+def test_neighbor_classifiers_loocv(nn_model, algorithm):\n+    \"\"\"Check that `predict` and related functions work fine with X=None\n+\n+    Calling predict with X=None computes a prediction for each training point\n+    from the labels of its neighbors (without the label of the data point being\n+    predicted upon). This is therefore mathematically equivalent to\n+    leave-one-out cross-validation without having do any retraining (rebuilding\n+    a KD-tree or Ball-tree index) or any data reshuffling.\n+    \"\"\"\n+    X, y = datasets.make_blobs(n_samples=15, centers=5, n_features=2, random_state=0)\n+\n+    nn_model = clone(nn_model).set_params(algorithm=algorithm)\n+\n+    # Set the radius for RadiusNeighborsRegressor to some percentile of the\n+    # empirical pairwise distances to avoid trivial test cases and warnings for\n+    # predictions with no neighbors within the radius.\n+    if \"radius\" in nn_model.get_params():\n+        dists = pairwise_distances(X).ravel()\n+        dists = dists[dists > 0]\n+        nn_model.set_params(radius=np.percentile(dists, 80))\n \n     loocv = cross_val_score(nn_model, X, y, cv=LeaveOneOut())\n     nn_model.fit(X, y)\n \n-    assert np.all(loocv == (nn_model.predict(None) == y))\n-    assert np.mean(loocv) == nn_model.score(None, y)\n+    assert_allclose(loocv, nn_model.predict(None) == y)\n+    assert np.mean(loocv) == pytest.approx(nn_model.score(None, y))\n+\n+    # Evaluating `nn_model` on its \"training\" set should lead to a higher\n+    # accuracy value than leaving out each data point in turn because the\n+    # former can overfit while the latter cannot by construction.\n     assert nn_model.score(None, y) < nn_model.score(X, y)\n \n \n@@ -2420,16 +2442,32 @@ def test_neighbor_classifiers_loocv(nn_model):\n     \"nn_model\",\n     [\n         neighbors.KNeighborsRegressor(n_neighbors=10),\n-        neighbors.RadiusNeighborsRegressor(radius=0.5),\n+        neighbors.RadiusNeighborsRegressor(),\n     ],\n )\n-def test_neighbor_regressors_loocv(nn_model):\n+@pytest.mark.parametrize(\"algorithm\", ALGORITHMS)\n+def test_neighbor_regressors_loocv(nn_model, algorithm):\n     \"\"\"Check that `predict` and related functions work fine with X=None\"\"\"\n-    X, y = datasets.load_diabetes(return_X_y=True)\n+    X, y = datasets.make_regression(n_samples=15, n_features=2, random_state=0)\n \n     # Only checking cross_val_predict and not cross_val_score because\n-    # cross_val_score does not work with LeaveOneOut() for a regressor\n+    # cross_val_score does not work with LeaveOneOut() for a regressor: the\n+    # default score method implements R2 score which is not well defined for a\n+    # single data point.\n+    #\n+    # TODO: if score is refactored to evaluate models for other scoring\n+    # functions, then this test can be extended to check cross_val_score as\n+    # well.\n+    nn_model = clone(nn_model).set_params(algorithm=algorithm)\n+\n+    # Set the radius for RadiusNeighborsRegressor to some percentile of the\n+    # empirical pairwise distances to avoid trivial test cases and warnings for\n+    # predictions with no neighbors within the radius.\n+    if \"radius\" in nn_model.get_params():\n+        dists = pairwise_distances(X).ravel()\n+        dists = dists[dists > 0]\n+        nn_model.set_params(radius=np.percentile(dists, 80))\n+\n     loocv = cross_val_predict(nn_model, X, y, cv=LeaveOneOut())\n     nn_model.fit(X, y)\n-\n-    assert np.all(loocv == nn_model.predict(None))\n+    assert_allclose(loocv, nn_model.predict(None))\n",
  "fail_to_pass": [
    "test_neighbor_classifiers_loocv",
    "test_neighbor_regressors_loocv"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/neighbors/tests/test_neighbors.py"
  ],
  "difficulty": "medium",
  "created_at": "2024-10-18T14:29:04Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30101",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/30047"
}