{
  "id": "scikit-learn__scikit-learn-30730",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "4af26a797d22f70f7507d6c5011d9bd086dfef0c",
  "issue_number": 1234,
  "issue_title": "MRG fix bincount mess I made in kmeans.",
  "issue_body": "This should clean up the stuff I pushed earlier.\ncc @ogrisel @gaelvaroquaux Could you have a brief look? What I pushed earlier is buggy but I didn't dare push again after so many failed fixes.\n",
  "pr_number": 30730,
  "pr_title": "ENH: improve validation for SGD models to accept l1_ratio=None when penalty is not `elasticnet`",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/30730.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/30730.enhancement.rst\nnew file mode 100644\nindex 0000000000000..91638cbcd9c7a\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/30730.enhancement.rst\n@@ -0,0 +1,3 @@\n+- :class:`linear_model.SGDClassifier` and :class:`linear_model.SGDRegressor` now accept\n+  `l1_ratio=None` when `penalty` is not `\"elasticnet\"`.\n+  By :user:`Marc Bresson <MarcBresson>`.\ndiff --git a/sklearn/linear_model/_stochastic_gradient.py b/sklearn/linear_model/_stochastic_gradient.py\nindex 89463f65ede98..8f7c814000614 100644\n--- a/sklearn/linear_model/_stochastic_gradient.py\n+++ b/sklearn/linear_model/_stochastic_gradient.py\n@@ -154,11 +154,20 @@ def _more_validate_params(self, for_partial_fit=False):\n                 \"learning_rate is 'optimal'. alpha is used \"\n                 \"to compute the optimal learning rate.\"\n             )\n+        if self.penalty == \"elasticnet\" and self.l1_ratio is None:\n+            raise ValueError(\"l1_ratio must be set when penalty is 'elasticnet'\")\n \n         # raises ValueError if not registered\n         self._get_penalty_type(self.penalty)\n         self._get_learning_rate_type(self.learning_rate)\n \n+    def _get_l1_ratio(self):\n+        if self.l1_ratio is None:\n+            # plain_sgd expects a float. Any value is fine since at this point\n+            # penalty can't be \"elsaticnet\" so l1_ratio is not used.\n+            return 0.0\n+        return self.l1_ratio\n+\n     def _get_loss_function(self, loss):\n         \"\"\"Get concrete ``LossFunction`` object for str ``loss``.\"\"\"\n         loss_ = self.loss_functions[loss]\n@@ -462,7 +471,7 @@ def fit_binary(\n         penalty_type,\n         alpha,\n         C,\n-        est.l1_ratio,\n+        est._get_l1_ratio(),\n         dataset,\n         validation_mask,\n         est.early_stopping,\n@@ -993,7 +1002,11 @@ class SGDClassifier(BaseSGDClassifier):\n         The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n         l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n         Only used if `penalty` is 'elasticnet'.\n-        Values must be in the range `[0.0, 1.0]`.\n+        Values must be in the range `[0.0, 1.0]` or can be `None` if\n+        `penalty` is not `elasticnet`.\n+\n+        .. versionchanged:: 1.7\n+            `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\n \n     fit_intercept : bool, default=True\n         Whether the intercept should be estimated or not. If False, the\n@@ -1194,7 +1207,7 @@ class SGDClassifier(BaseSGDClassifier):\n         **BaseSGDClassifier._parameter_constraints,\n         \"penalty\": [StrOptions({\"l2\", \"l1\", \"elasticnet\"}), None],\n         \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\n-        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\")],\n+        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\"), None],\n         \"power_t\": [Interval(Real, None, None, closed=\"neither\")],\n         \"epsilon\": [Interval(Real, 0, None, closed=\"left\")],\n         \"learning_rate\": [\n@@ -1695,7 +1708,7 @@ def _fit_regressor(\n             penalty_type,\n             alpha,\n             C,\n-            self.l1_ratio,\n+            self._get_l1_ratio(),\n             dataset,\n             validation_mask,\n             self.early_stopping,\n@@ -1796,7 +1809,11 @@ class SGDRegressor(BaseSGDRegressor):\n         The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n         l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n         Only used if `penalty` is 'elasticnet'.\n-        Values must be in the range `[0.0, 1.0]`.\n+        Values must be in the range `[0.0, 1.0]` or can be `None` if\n+        `penalty` is not `elasticnet`.\n+\n+        .. versionchanged:: 1.7\n+            `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\n \n     fit_intercept : bool, default=True\n         Whether the intercept should be estimated or not. If False, the\n@@ -1976,7 +1993,7 @@ class SGDRegressor(BaseSGDRegressor):\n         **BaseSGDRegressor._parameter_constraints,\n         \"penalty\": [StrOptions({\"l2\", \"l1\", \"elasticnet\"}), None],\n         \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\n-        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\")],\n+        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\"), None],\n         \"power_t\": [Interval(Real, None, None, closed=\"neither\")],\n         \"learning_rate\": [\n             StrOptions({\"constant\", \"optimal\", \"invscaling\", \"adaptive\"}),\ndiff --git a/sklearn/linear_model/tests/test_sgd.py b/sklearn/linear_model/tests/test_sgd.py\nindex 6252237ebf514..26d138ae3649b 100644\n--- a/sklearn/linear_model/tests/test_sgd.py\n+++ b/sklearn/linear_model/tests/test_sgd.py\n@@ -486,6 +486,27 @@ def test_not_enough_sample_for_early_stopping(klass):\n         clf.fit(X3, Y3)\n \n \n+@pytest.mark.parametrize(\"Estimator\", [SGDClassifier, SGDRegressor])\n+@pytest.mark.parametrize(\"l1_ratio\", [0, 0.7, 1])\n+def test_sgd_l1_ratio_not_used(Estimator, l1_ratio):\n+    \"\"\"Check that l1_ratio is not used when penalty is not 'elasticnet'\"\"\"\n+    clf1 = Estimator(penalty=\"l1\", l1_ratio=None, random_state=0).fit(X, Y)\n+    clf2 = Estimator(penalty=\"l1\", l1_ratio=l1_ratio, random_state=0).fit(X, Y)\n+\n+    assert_allclose(clf1.coef_, clf2.coef_)\n+\n+\n+@pytest.mark.parametrize(\n+    \"Estimator\", [SGDClassifier, SparseSGDClassifier, SGDRegressor, SparseSGDRegressor]\n+)\n+def test_sgd_failing_penalty_validation(Estimator):\n+    clf = Estimator(penalty=\"elasticnet\", l1_ratio=None)\n+    with pytest.raises(\n+        ValueError, match=\"l1_ratio must be set when penalty is 'elasticnet'\"\n+    ):\n+        clf.fit(X, Y)\n+\n+\n ###############################################################################\n # Classification Test Case\n \n",
  "fail_to_pass": [
    "test_sgd_l1_ratio_not_used",
    "test_sgd_failing_penalty_validation"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/linear_model/_stochastic_gradient.py",
    "sklearn/linear_model/tests/test_sgd.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-01-28T15:41:06Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30730",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/1234"
}