{
  "id": "scikit-learn__scikit-learn-30415",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "d2e123d29237c9428e85e79ca6ac6331422039a3",
  "issue_number": 30382,
  "issue_title": "Gaussian Mixture: Diagonal covariance vectors might contain unreasonably negative values when the input datatype is np.float32",
  "issue_body": "### Describe the bug\r\n\r\nThe Gaussian Mixture implementation shows numerical instabilities on single-precision floating point input numbers, that even large values of the regularization parameter reg_covar (like 0.1) cannot mitigate.\r\n\r\nMore specifically, diagonal covariance elements must not be negative. However, due to the numerical instabilities intrinsic to floating point arithmetic, they might end up being tiny negative numbers that reg_covar must compensate.\r\nIt turns out that, for some input float32 , the covariance can reach the unreasonable value of -0.99999979.\r\nThis is because squaring float32 numbers significantly magnifies their precision errors.\r\n\r\nThe proposed solution consists in converting float32 values to float64 before squaring them.\r\nCare must be taken to not increase memory consumption in the overall process.\r\nHence, as avgX_means is equal to avg_means2, the return value can be simplified.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nmodel = GaussianMixture(n_components=2, covariance_type=\"spherical\", reg_covar=0.1)\r\nmodel.fit(np.array([[9999.0], [0.0]], dtype=np.float32))\r\nmodel.covariances_\r\n```\r\n\r\n### Expected Results\r\n\r\n```python\r\narray([0.1, 0.1])\r\n```\r\n\r\n### Actual Results\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [132], in <cell line: 49>()\r\n     45 skgm._estimate_gaussian_covariances_diag = _optimized_estimate_gaussian_covariances_diag\r\n     48 model = GaussianMixture(n_components=2,covariance_type=\"spherical\", reg_covar=0.1)\r\n---> 49 model.fit(np.array([[9999.0], [0.0]], dtype=np.float32))\r\n     50 model.covariances_\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:200](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=199), in BaseMixture.fit(self, X, y)\r\n    174 def fit(self, X, y=None):\r\n    175     \"\"\"Estimate model parameters with the EM algorithm.\r\n    176 \r\n    177     The method fits the model ``n_init`` times and sets the parameters with\r\n   (...)\r\n    198         The fitted mixture.\r\n    199     \"\"\"\r\n--> 200     self.fit_predict(X, y)\r\n    201     return self\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:253](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=252), in BaseMixture.fit_predict(self, X, y)\r\n    250 self._print_verbose_msg_init_beg(init)\r\n    252 if do_init:\r\n--> 253     self._initialize_parameters(X, random_state)\r\n    255 lower_bound = -np.inf if do_init else self.lower_bound_\r\n    257 if self.max_iter == 0:\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:160](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=159), in BaseMixture._initialize_parameters(self, X, random_state)\r\n    155 else:\r\n    156     raise ValueError(\r\n    157         \"Unimplemented initialization method '%s'\" % self.init_params\r\n    158     )\r\n--> 160 self._initialize(X, resp)\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:723](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_gaussian_mixture.py#line=722), in GaussianMixture._initialize(self, X, resp)\r\n    721 if self.precisions_init is None:\r\n    722     self.covariances_ = covariances\r\n--> 723     self.precisions_cholesky_ = _compute_precision_cholesky(\r\n    724         covariances, self.covariance_type\r\n    725     )\r\n    726 elif self.covariance_type == \"full\":\r\n    727     self.precisions_cholesky_ = np.array(\r\n    728         [\r\n    729             linalg.cholesky(prec_init, lower=True)\r\n    730             for prec_init in self.precisions_init\r\n    731         ]\r\n    732     )\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:347](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_gaussian_mixture.py#line=346), in _compute_precision_cholesky(covariances, covariance_type)\r\n    345 else:\r\n    346     if np.any(np.less_equal(covariances, 0.0)):\r\n--> 347         raise ValueError(estimate_precision_error_message)\r\n    348     precisions_chol = 1.0 / np.sqrt(covariances)\r\n    349 return precisions_chol\r\n\r\nValueError: Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\r\n   machine: Windows-10-10.0.19043-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.0.4\r\n   setuptools: 58.1.0\r\n        numpy: 1.22.4\r\n        scipy: 1.8.1\r\n       Cython: 0.29.30\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\scipy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n",
  "pr_number": 30415,
  "pr_title": "PERF float32 propagation in GaussianMixture",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.mixture/30415.efficiency.rst b/doc/whats_new/upcoming_changes/sklearn.mixture/30415.efficiency.rst\nnew file mode 100644\nindex 0000000000000..095ef66ce28c0\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.mixture/30415.efficiency.rst\n@@ -0,0 +1,5 @@\n+- :class:`~mixture.GaussianMixture` now consistently operates at `float32`\n+  precision when fitted with `float32` data to improve training speed and\n+  memory efficiency. Previously, part of the computation would be implicitly\n+  cast to `float64`. By :user:`Olivier Grisel <ogrisel>` and :user:`Omar Salman\n+  <OmarManzoor>`.\ndiff --git a/sklearn/mixture/_base.py b/sklearn/mixture/_base.py\nindex ebb069a1ff563..dd50d39b4fdb0 100644\n--- a/sklearn/mixture/_base.py\n+++ b/sklearn/mixture/_base.py\n@@ -109,7 +109,7 @@ def _initialize_parameters(self, X, random_state):\n         n_samples, _ = X.shape\n \n         if self.init_params == \"kmeans\":\n-            resp = np.zeros((n_samples, self.n_components))\n+            resp = np.zeros((n_samples, self.n_components), dtype=X.dtype)\n             label = (\n                 cluster.KMeans(\n                     n_clusters=self.n_components, n_init=1, random_state=random_state\n@@ -119,16 +119,18 @@ def _initialize_parameters(self, X, random_state):\n             )\n             resp[np.arange(n_samples), label] = 1\n         elif self.init_params == \"random\":\n-            resp = random_state.uniform(size=(n_samples, self.n_components))\n+            resp = np.asarray(\n+                random_state.uniform(size=(n_samples, self.n_components)), dtype=X.dtype\n+            )\n             resp /= resp.sum(axis=1)[:, np.newaxis]\n         elif self.init_params == \"random_from_data\":\n-            resp = np.zeros((n_samples, self.n_components))\n+            resp = np.zeros((n_samples, self.n_components), dtype=X.dtype)\n             indices = random_state.choice(\n                 n_samples, size=self.n_components, replace=False\n             )\n             resp[indices, np.arange(self.n_components)] = 1\n         elif self.init_params == \"k-means++\":\n-            resp = np.zeros((n_samples, self.n_components))\n+            resp = np.zeros((n_samples, self.n_components), dtype=X.dtype)\n             _, indices = kmeans_plusplus(\n                 X,\n                 self.n_components,\ndiff --git a/sklearn/mixture/_gaussian_mixture.py b/sklearn/mixture/_gaussian_mixture.py\nindex 9acfd6bb045e1..a5b3a5ae5c172 100644\n--- a/sklearn/mixture/_gaussian_mixture.py\n+++ b/sklearn/mixture/_gaussian_mixture.py\n@@ -42,7 +42,8 @@ def _check_weights(weights, n_components):\n         )\n \n     # check normalization\n-    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0):\n+    atol = 1e-6 if weights.dtype == np.float32 else 1e-8\n+    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0, atol=atol):\n         raise ValueError(\n             \"The parameter 'weights' should be normalized, but got sum(weights) = %.5f\"\n             % np.sum(weights)\n@@ -170,7 +171,7 @@ def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n         The covariance matrix of the current components.\n     \"\"\"\n     n_components, n_features = means.shape\n-    covariances = np.empty((n_components, n_features, n_features))\n+    covariances = np.empty((n_components, n_features, n_features), dtype=X.dtype)\n     for k in range(n_components):\n         diff = X - means[k]\n         covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n@@ -316,19 +317,25 @@ def _compute_precision_cholesky(covariances, covariance_type):\n         \"Fitting the mixture model failed because some components have \"\n         \"ill-defined empirical covariance (for instance caused by singleton \"\n         \"or collapsed samples). Try to decrease the number of components, \"\n-        \"or increase reg_covar.\"\n+        \"increase reg_covar, or scale the input data.\"\n     )\n+    dtype = covariances.dtype\n+    if dtype == np.float32:\n+        estimate_precision_error_message += (\n+            \" The numerical accuracy can also be improved by passing float64\"\n+            \" data instead of float32.\"\n+        )\n \n     if covariance_type == \"full\":\n         n_components, n_features, _ = covariances.shape\n-        precisions_chol = np.empty((n_components, n_features, n_features))\n+        precisions_chol = np.empty((n_components, n_features, n_features), dtype=dtype)\n         for k, covariance in enumerate(covariances):\n             try:\n                 cov_chol = linalg.cholesky(covariance, lower=True)\n             except linalg.LinAlgError:\n                 raise ValueError(estimate_precision_error_message)\n             precisions_chol[k] = linalg.solve_triangular(\n-                cov_chol, np.eye(n_features), lower=True\n+                cov_chol, np.eye(n_features, dtype=dtype), lower=True\n             ).T\n     elif covariance_type == \"tied\":\n         _, n_features = covariances.shape\n@@ -337,7 +344,7 @@ def _compute_precision_cholesky(covariances, covariance_type):\n         except linalg.LinAlgError:\n             raise ValueError(estimate_precision_error_message)\n         precisions_chol = linalg.solve_triangular(\n-            cov_chol, np.eye(n_features), lower=True\n+            cov_chol, np.eye(n_features, dtype=dtype), lower=True\n         ).T\n     else:\n         if np.any(np.less_equal(covariances, 0.0)):\n@@ -428,7 +435,7 @@ def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n     if covariance_type == \"full\":\n         n_components, _, _ = matrix_chol.shape\n         log_det_chol = np.sum(\n-            np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n+            np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), axis=1\n         )\n \n     elif covariance_type == \"tied\":\n@@ -438,7 +445,7 @@ def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n         log_det_chol = np.sum(np.log(matrix_chol), axis=1)\n \n     else:\n-        log_det_chol = n_features * (np.log(matrix_chol))\n+        log_det_chol = n_features * np.log(matrix_chol)\n \n     return log_det_chol\n \n@@ -474,13 +481,13 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n     log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features)\n \n     if covariance_type == \"full\":\n-        log_prob = np.empty((n_samples, n_components))\n+        log_prob = np.empty((n_samples, n_components), dtype=X.dtype)\n         for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n             y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n             log_prob[:, k] = np.sum(np.square(y), axis=1)\n \n     elif covariance_type == \"tied\":\n-        log_prob = np.empty((n_samples, n_components))\n+        log_prob = np.empty((n_samples, n_components), dtype=X.dtype)\n         for k, mu in enumerate(means):\n             y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n             log_prob[:, k] = np.sum(np.square(y), axis=1)\n@@ -502,7 +509,7 @@ def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n         )\n     # Since we are using the precision of the Cholesky decomposition,\n     # `- 0.5 * log_det_precision` becomes `+ log_det_precision_chol`\n-    return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n+    return -0.5 * (n_features * np.log(2 * np.pi).astype(X.dtype) + log_prob) + log_det\n \n \n class GaussianMixture(BaseMixture):\n@@ -845,8 +852,9 @@ def _set_parameters(self, params):\n         # Attributes computation\n         _, n_features = self.means_.shape\n \n+        dtype = self.precisions_cholesky_.dtype\n         if self.covariance_type == \"full\":\n-            self.precisions_ = np.empty(self.precisions_cholesky_.shape)\n+            self.precisions_ = np.empty_like(self.precisions_cholesky_)\n             for k, prec_chol in enumerate(self.precisions_cholesky_):\n                 self.precisions_[k] = np.dot(prec_chol, prec_chol.T)\n \ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\nindex f6e3ffb4991f2..e8144ada64f67 100644\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -40,7 +40,9 @@\n COVARIANCE_TYPE = [\"full\", \"tied\", \"diag\", \"spherical\"]\n \n \n-def generate_data(n_samples, n_features, weights, means, precisions, covariance_type):\n+def generate_data(\n+    n_samples, n_features, weights, means, precisions, covariance_type, dtype=np.float64\n+):\n     rng = np.random.RandomState(0)\n \n     X = []\n@@ -49,44 +51,58 @@ def generate_data(n_samples, n_features, weights, means, precisions, covariance_\n             X.append(\n                 rng.multivariate_normal(\n                     m, c * np.eye(n_features), int(np.round(w * n_samples))\n-                )\n+                ).astype(dtype)\n             )\n     if covariance_type == \"diag\":\n         for _, (w, m, c) in enumerate(zip(weights, means, precisions[\"diag\"])):\n             X.append(\n-                rng.multivariate_normal(m, np.diag(c), int(np.round(w * n_samples)))\n+                rng.multivariate_normal(\n+                    m, np.diag(c), int(np.round(w * n_samples))\n+                ).astype(dtype)\n             )\n     if covariance_type == \"tied\":\n         for _, (w, m) in enumerate(zip(weights, means)):\n             X.append(\n                 rng.multivariate_normal(\n                     m, precisions[\"tied\"], int(np.round(w * n_samples))\n-                )\n+                ).astype(dtype)\n             )\n     if covariance_type == \"full\":\n         for _, (w, m, c) in enumerate(zip(weights, means, precisions[\"full\"])):\n-            X.append(rng.multivariate_normal(m, c, int(np.round(w * n_samples))))\n+            X.append(\n+                rng.multivariate_normal(m, c, int(np.round(w * n_samples))).astype(\n+                    dtype\n+                )\n+            )\n \n     X = np.vstack(X)\n     return X\n \n \n class RandomData:\n-    def __init__(self, rng, n_samples=200, n_components=2, n_features=2, scale=50):\n+    def __init__(\n+        self,\n+        rng,\n+        n_samples=200,\n+        n_components=2,\n+        n_features=2,\n+        scale=50,\n+        dtype=np.float64,\n+    ):\n         self.n_samples = n_samples\n         self.n_components = n_components\n         self.n_features = n_features\n \n-        self.weights = rng.rand(n_components)\n-        self.weights = self.weights / self.weights.sum()\n-        self.means = rng.rand(n_components, n_features) * scale\n+        self.weights = rng.rand(n_components).astype(dtype)\n+        self.weights = self.weights.astype(dtype) / self.weights.sum()\n+        self.means = rng.rand(n_components, n_features).astype(dtype) * scale\n         self.covariances = {\n-            \"spherical\": 0.5 + rng.rand(n_components),\n-            \"diag\": (0.5 + rng.rand(n_components, n_features)) ** 2,\n-            \"tied\": make_spd_matrix(n_features, random_state=rng),\n+            \"spherical\": 0.5 + rng.rand(n_components).astype(dtype),\n+            \"diag\": (0.5 + rng.rand(n_components, n_features).astype(dtype)) ** 2,\n+            \"tied\": make_spd_matrix(n_features, random_state=rng).astype(dtype),\n             \"full\": np.array(\n                 [\n-                    make_spd_matrix(n_features, random_state=rng) * 0.5\n+                    make_spd_matrix(n_features, random_state=rng).astype(dtype) * 0.5\n                     for _ in range(n_components)\n                 ]\n             ),\n@@ -111,6 +127,7 @@ def __init__(self, rng, n_samples=200, n_components=2, n_features=2, scale=50):\n                         self.means,\n                         self.covariances,\n                         covar_type,\n+                        dtype=dtype,\n                     )\n                     for covar_type in COVARIANCE_TYPE\n                 ],\n@@ -376,31 +393,33 @@ def test_suffstat_sk_diag():\n     assert_almost_equal(covars_pred_diag, 1.0 / precs_chol_pred**2)\n \n \n-def test_gaussian_suffstat_sk_spherical():\n+def test_gaussian_suffstat_sk_spherical(global_dtype):\n     # computing spherical covariance equals to the variance of one-dimension\n     # data after flattening, n_components=1\n     rng = np.random.RandomState(0)\n     n_samples, n_features = 500, 2\n \n-    X = rng.rand(n_samples, n_features)\n+    X = rng.rand(n_samples, n_features).astype(global_dtype)\n     X = X - X.mean()\n-    resp = np.ones((n_samples, 1))\n-    nk = np.array([n_samples])\n+    resp = np.ones((n_samples, 1), dtype=global_dtype)\n+    nk = np.array([n_samples], dtype=global_dtype)\n     xk = X.mean()\n     covars_pred_spherical = _estimate_gaussian_covariances_spherical(resp, X, nk, xk, 0)\n     covars_pred_spherical2 = np.dot(X.flatten().T, X.flatten()) / (\n         n_features * n_samples\n     )\n     assert_almost_equal(covars_pred_spherical, covars_pred_spherical2)\n+    assert covars_pred_spherical.dtype == global_dtype\n \n     # check the precision computation\n     precs_chol_pred = _compute_precision_cholesky(covars_pred_spherical, \"spherical\")\n     assert_almost_equal(covars_pred_spherical, 1.0 / precs_chol_pred**2)\n+    assert precs_chol_pred.dtype == global_dtype\n \n \n-def test_compute_log_det_cholesky():\n+def test_compute_log_det_cholesky(global_dtype):\n     n_features = 2\n-    rand_data = RandomData(np.random.RandomState(0))\n+    rand_data = RandomData(np.random.RandomState(0), dtype=global_dtype)\n \n     for covar_type in COVARIANCE_TYPE:\n         covariance = rand_data.covariances[covar_type]\n@@ -415,12 +434,14 @@ def test_compute_log_det_cholesky():\n             predected_det = covariance**n_features\n \n         # We compute the cholesky decomposition of the covariance matrix\n+        assert covariance.dtype == global_dtype\n         expected_det = _compute_log_det_cholesky(\n             _compute_precision_cholesky(covariance, covar_type),\n             covar_type,\n             n_features=n_features,\n         )\n         assert_array_almost_equal(expected_det, -0.5 * np.log(predected_det))\n+        assert expected_det.dtype == global_dtype\n \n \n def _naive_lmvnpdf_diag(X, means, covars):\n@@ -548,9 +569,9 @@ def test_gaussian_mixture_predict_predict_proba():\n         (4, 300, 1e-1),  # loose convergence\n     ],\n )\n-def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n+def test_gaussian_mixture_fit_predict(seed, max_iter, tol, global_dtype):\n     rng = np.random.RandomState(seed)\n-    rand_data = RandomData(rng)\n+    rand_data = RandomData(rng, dtype=global_dtype)\n     for covar_type in COVARIANCE_TYPE:\n         X = rand_data.X[covar_type]\n         Y = rand_data.Y\n@@ -571,6 +592,9 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         Y_pred2 = g.fit_predict(X)\n         assert_array_equal(Y_pred1, Y_pred2)\n         assert adjusted_rand_score(Y, Y_pred2) > 0.95\n+        assert g.means_.dtype == global_dtype\n+        assert g.weights_.dtype == global_dtype\n+        assert g.precisions_.dtype == global_dtype\n \n \n def test_gaussian_mixture_fit_predict_n_init():\n@@ -582,10 +606,10 @@ def test_gaussian_mixture_fit_predict_n_init():\n     assert_array_equal(y_pred1, y_pred2)\n \n \n-def test_gaussian_mixture_fit():\n+def test_gaussian_mixture_fit(global_dtype):\n     # recover the ground truth\n     rng = np.random.RandomState(0)\n-    rand_data = RandomData(rng)\n+    rand_data = RandomData(rng, dtype=global_dtype)\n     n_features = rand_data.n_features\n     n_components = rand_data.n_components\n \n@@ -634,6 +658,10 @@ def test_gaussian_mixture_fit():\n             # the accuracy depends on the number of data and randomness, rng\n             assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.15)\n \n+        assert g.means_.dtype == global_dtype\n+        assert g.covariances_.dtype == global_dtype\n+        assert g.precisions_.dtype == global_dtype\n+\n \n def test_gaussian_mixture_fit_best_params():\n     rng = np.random.RandomState(0)\n@@ -901,12 +929,13 @@ def test_convergence_detected_with_warm_start():\n         assert max_iter >= gmm.n_iter_\n \n \n-def test_score():\n+def test_score(global_dtype):\n     covar_type = \"full\"\n     rng = np.random.RandomState(0)\n-    rand_data = RandomData(rng, scale=7)\n+    rand_data = RandomData(rng, scale=7, dtype=global_dtype)\n     n_components = rand_data.n_components\n     X = rand_data.X[covar_type]\n+    assert X.dtype == global_dtype\n \n     # Check the error message if we don't call fit\n     gmm1 = GaussianMixture(\n@@ -928,9 +957,14 @@ def test_score():\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"ignore\", ConvergenceWarning)\n         gmm1.fit(X)\n+\n+    assert gmm1.means_.dtype == global_dtype\n+    assert gmm1.covariances_.dtype == global_dtype\n+\n     gmm_score = gmm1.score(X)\n     gmm_score_proba = gmm1.score_samples(X).mean()\n     assert_almost_equal(gmm_score, gmm_score_proba)\n+    assert gmm_score_proba.dtype == global_dtype\n \n     # Check if the score increase\n     gmm2 = GaussianMixture(\n@@ -1027,7 +1061,7 @@ def test_regularisation():\n                 \"Fitting the mixture model failed because some components have\"\n                 \" ill-defined empirical covariance (for instance caused by \"\n                 \"singleton or collapsed samples). Try to decrease the number \"\n-                \"of components, or increase reg_covar.\"\n+                \"of components, increase reg_covar, or scale the input data.\"\n             )\n             with pytest.raises(ValueError, match=msg):\n                 gmm.fit(X)\n@@ -1035,27 +1069,29 @@ def test_regularisation():\n             gmm.set_params(reg_covar=1e-6).fit(X)\n \n \n-def test_property():\n+@pytest.mark.parametrize(\"covar_type\", COVARIANCE_TYPE)\n+def test_fitted_precision_covariance_concistency(covar_type, global_dtype):\n     rng = np.random.RandomState(0)\n-    rand_data = RandomData(rng, scale=7)\n+    rand_data = RandomData(rng, scale=7, dtype=global_dtype)\n     n_components = rand_data.n_components\n \n-    for covar_type in COVARIANCE_TYPE:\n-        X = rand_data.X[covar_type]\n-        gmm = GaussianMixture(\n-            n_components=n_components,\n-            covariance_type=covar_type,\n-            random_state=rng,\n-            n_init=5,\n-        )\n-        gmm.fit(X)\n-        if covar_type == \"full\":\n-            for prec, covar in zip(gmm.precisions_, gmm.covariances_):\n-                assert_array_almost_equal(linalg.inv(prec), covar)\n-        elif covar_type == \"tied\":\n-            assert_array_almost_equal(linalg.inv(gmm.precisions_), gmm.covariances_)\n-        else:\n-            assert_array_almost_equal(gmm.precisions_, 1.0 / gmm.covariances_)\n+    X = rand_data.X[covar_type]\n+    gmm = GaussianMixture(\n+        n_components=n_components,\n+        covariance_type=covar_type,\n+        random_state=rng,\n+        n_init=5,\n+    )\n+    gmm.fit(X)\n+    assert gmm.precisions_.dtype == global_dtype\n+    assert gmm.covariances_.dtype == global_dtype\n+    if covar_type == \"full\":\n+        for prec, covar in zip(gmm.precisions_, gmm.covariances_):\n+            assert_array_almost_equal(linalg.inv(prec), covar)\n+    elif covar_type == \"tied\":\n+        assert_array_almost_equal(linalg.inv(gmm.precisions_), gmm.covariances_)\n+    else:\n+        assert_array_almost_equal(gmm.precisions_, 1.0 / gmm.covariances_)\n \n \n def test_sample():\n@@ -1227,10 +1263,10 @@ def test_init_means_not_duplicated(init_params, global_random_seed):\n @pytest.mark.parametrize(\n     \"init_params\", [\"random\", \"random_from_data\", \"k-means++\", \"kmeans\"]\n )\n-def test_means_for_all_inits(init_params, global_random_seed):\n+def test_means_for_all_inits(init_params, global_random_seed, global_dtype):\n     # Check fitted means properties for all initializations\n     rng = np.random.RandomState(global_random_seed)\n-    rand_data = RandomData(rng, scale=5)\n+    rand_data = RandomData(rng, scale=5, dtype=global_dtype)\n     n_components = rand_data.n_components\n     X = rand_data.X[\"full\"]\n \n@@ -1243,6 +1279,9 @@ def test_means_for_all_inits(init_params, global_random_seed):\n     assert np.all(X.min(axis=0) <= gmm.means_)\n     assert np.all(gmm.means_ <= X.max(axis=0))\n     assert gmm.converged_\n+    assert gmm.means_.dtype == global_dtype\n+    assert gmm.covariances_.dtype == global_dtype\n+    assert gmm.weights_.dtype == global_dtype\n \n \n def test_max_iter_zero():\n@@ -1265,7 +1304,7 @@ def test_max_iter_zero():\n     assert_allclose(gmm.means_, means_init)\n \n \n-def test_gaussian_mixture_precisions_init_diag():\n+def test_gaussian_mixture_precisions_init_diag(global_dtype):\n     \"\"\"Check that we properly initialize `precision_cholesky_` when we manually\n     provide the precision matrix.\n \n@@ -1284,7 +1323,7 @@ def test_gaussian_mixture_precisions_init_diag():\n     shifted_gaussian = rng.randn(n_samples, 2) + np.array([20, 20])\n     C = np.array([[0.0, -0.7], [3.5, 0.7]])\n     stretched_gaussian = np.dot(rng.randn(n_samples, 2), C)\n-    X = np.vstack([shifted_gaussian, stretched_gaussian])\n+    X = np.vstack([shifted_gaussian, stretched_gaussian]).astype(global_dtype)\n \n     # common parameters to check the consistency of precision initialization\n     n_components, covariance_type, reg_covar, random_state = 2, \"diag\", 1e-6, 0\n@@ -1293,7 +1332,7 @@ def test_gaussian_mixture_precisions_init_diag():\n     # - run KMeans to have an initial guess\n     # - estimate the covariance\n     # - compute the precision matrix from the estimated covariance\n-    resp = np.zeros((X.shape[0], n_components))\n+    resp = np.zeros((X.shape[0], n_components)).astype(global_dtype)\n     label = (\n         KMeans(n_clusters=n_components, n_init=1, random_state=random_state)\n         .fit(X)\n@@ -1303,6 +1342,7 @@ def test_gaussian_mixture_precisions_init_diag():\n     _, _, covariance = _estimate_gaussian_parameters(\n         X, resp, reg_covar=reg_covar, covariance_type=covariance_type\n     )\n+    assert covariance.dtype == global_dtype\n     precisions_init = 1 / covariance\n \n     gm_with_init = GaussianMixture(\n@@ -1312,6 +1352,9 @@ def test_gaussian_mixture_precisions_init_diag():\n         precisions_init=precisions_init,\n         random_state=random_state,\n     ).fit(X)\n+    assert gm_with_init.means_.dtype == global_dtype\n+    assert gm_with_init.covariances_.dtype == global_dtype\n+    assert gm_with_init.precisions_cholesky_.dtype == global_dtype\n \n     gm_without_init = GaussianMixture(\n         n_components=n_components,\n@@ -1319,6 +1362,9 @@ def test_gaussian_mixture_precisions_init_diag():\n         reg_covar=reg_covar,\n         random_state=random_state,\n     ).fit(X)\n+    assert gm_without_init.means_.dtype == global_dtype\n+    assert gm_without_init.covariances_.dtype == global_dtype\n+    assert gm_without_init.precisions_cholesky_.dtype == global_dtype\n \n     assert gm_without_init.n_iter_ == gm_with_init.n_iter_\n     assert_allclose(\n@@ -1326,11 +1372,11 @@ def test_gaussian_mixture_precisions_init_diag():\n     )\n \n \n-def _generate_data(seed, n_samples, n_features, n_components):\n+def _generate_data(seed, n_samples, n_features, n_components, dtype=np.float64):\n     \"\"\"Randomly generate samples and responsibilities.\"\"\"\n     rs = np.random.RandomState(seed)\n-    X = rs.random_sample((n_samples, n_features))\n-    resp = rs.random_sample((n_samples, n_components))\n+    X = rs.random_sample((n_samples, n_features)).astype(dtype)\n+    resp = rs.random_sample((n_samples, n_components)).astype(dtype)\n     resp /= resp.sum(axis=1)[:, np.newaxis]\n     return X, resp\n \n@@ -1357,7 +1403,9 @@ def _calculate_precisions(X, resp, covariance_type):\n \n \n @pytest.mark.parametrize(\"covariance_type\", COVARIANCE_TYPE)\n-def test_gaussian_mixture_precisions_init(covariance_type, global_random_seed):\n+def test_gaussian_mixture_precisions_init(\n+    covariance_type, global_random_seed, global_dtype\n+):\n     \"\"\"Non-regression test for #26415.\"\"\"\n \n     X, resp = _generate_data(\n@@ -1365,11 +1413,15 @@ def test_gaussian_mixture_precisions_init(covariance_type, global_random_seed):\n         n_samples=100,\n         n_features=3,\n         n_components=4,\n+        dtype=global_dtype,\n     )\n \n     precisions_init, desired_precisions_cholesky = _calculate_precisions(\n         X, resp, covariance_type\n     )\n+    assert precisions_init.dtype == global_dtype\n+    assert desired_precisions_cholesky.dtype == global_dtype\n+\n     gmm = GaussianMixture(\n         covariance_type=covariance_type, precisions_init=precisions_init\n     )\n",
  "fail_to_pass": [
    "test_gaussian_suffstat_sk_spherical",
    "test_compute_log_det_cholesky",
    "test_gaussian_mixture_fit_predict",
    "test_gaussian_mixture_fit",
    "test_score",
    "test_fitted_precision_covariance_concistency",
    "test_means_for_all_inits",
    "test_gaussian_mixture_precisions_init_diag",
    "test_gaussian_mixture_precisions_init"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/mixture/_base.py",
    "sklearn/mixture/_gaussian_mixture.py",
    "sklearn/mixture/tests/test_gaussian_mixture.py"
  ],
  "difficulty": "hard",
  "created_at": "2024-12-05T17:47:23Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30415",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30382"
}