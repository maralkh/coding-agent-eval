{
  "id": "scikit-learn__scikit-learn-30787",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "7d0cbaf20f2a517139cad35540da72858fcb14ab",
  "issue_number": 9903,
  "issue_title": "[MRG+1] Improve the error message for some metrics when the shape of sample_weight is inappropriate",
  "issue_body": "<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests\r\n-->\r\n#### Reference Issue\r\n<!-- Example: Fixes #1234 -->\r\nproposed by @lesteve in https://github.com/scikit-learn/scikit-learn/pull/9786#issuecomment-332662498.\r\n\r\n> in this PR we spotted a place where check_consistent_lengths(X, y) was used where check_consistent_lengths(X, y, sample_weight) should have called it would be good to double-check that this error is not present in some other places in our codebase.\r\n\r\nFixes #9870\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nCurrently, many metrics do not explicitly check the shape of sample_weight. Instead, they rely on certain statement to block the code from running through, this may cause:\r\n(1)Users can't get meaningful error message (e.g., now you may get `Axis must be specified when shapes of a and weights differ` or even `operands could not be broadcast together with shapes (2,1) (3,1)`)\r\n(2)Sometimes all the statements fail to block the code and you even can't get an erorr (e.g., roc_auc_score previously)\r\nThe PR fixes the problem and improves the common test to ensure that meaningful error message is raised by all metrics with sample_weight.\r\n\r\n#### Any other comments?\r\ncc @jnothman @lesteve \r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n",
  "pr_number": 30787,
  "pr_title": "Remove `median_absolute_error` from `METRICS_WITHOUT_SAMPLE_WEIGHT`",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/30787.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/30787.fix.rst\nnew file mode 100644\nindex 0000000000000..13edbdfc7874d\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/30787.fix.rst\n@@ -0,0 +1,6 @@\n+- :func:`metrics.median_absolute_error` now uses `_averaged_weighted_percentile`\n+  instead of `_weighted_percentile` to calculate median when `sample_weight` is not\n+  `None`. This is equivalent to using the \"averaged_inverted_cdf\" instead of\n+  the \"inverted_cdf\" quantile method, which gives results equivalent to `numpy.median`\n+  if equal weights used.\n+  By :user:`Lucy Liu <lucyleeow>`\ndiff --git a/sklearn/metrics/_regression.py b/sklearn/metrics/_regression.py\nindex e7435756c52b2..3e0148345ffa1 100644\n--- a/sklearn/metrics/_regression.py\n+++ b/sklearn/metrics/_regression.py\n@@ -28,7 +28,7 @@\n     _xlogy as xlogy,\n )\n from ..utils._param_validation import Interval, StrOptions, validate_params\n-from ..utils.stats import _weighted_percentile\n+from ..utils.stats import _averaged_weighted_percentile, _weighted_percentile\n from ..utils.validation import (\n     _check_sample_weight,\n     _num_samples,\n@@ -923,7 +923,7 @@ def median_absolute_error(\n     if sample_weight is None:\n         output_errors = _median(xp.abs(y_pred - y_true), axis=0)\n     else:\n-        output_errors = _weighted_percentile(\n+        output_errors = _averaged_weighted_percentile(\n             xp.abs(y_pred - y_true), sample_weight=sample_weight\n         )\n     if isinstance(multioutput, str):\ndiff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 238ea821d8340..77e16c2da86c3 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -555,7 +555,6 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n \n # No Sample weight support\n METRICS_WITHOUT_SAMPLE_WEIGHT = {\n-    \"median_absolute_error\",\n     \"max_error\",\n     \"ovo_roc_auc\",\n     \"weighted_ovo_roc_auc\",\n@@ -1474,9 +1473,10 @@ def test_averaging_multilabel_all_ones(name):\n     check_averaging(name, y_true, y_true_binarize, y_pred, y_pred_binarize, y_score)\n \n \n-def check_sample_weight_invariance(name, metric, y1, y2):\n+def check_sample_weight_invariance(name, metric, y1, y2, sample_weight=None):\n     rng = np.random.RandomState(0)\n-    sample_weight = rng.randint(1, 10, size=len(y1))\n+    if sample_weight is None:\n+        sample_weight = rng.randint(1, 10, size=len(y1))\n \n     # top_k_accuracy_score always lead to a perfect score for k > 1 in the\n     # binary case\n@@ -1552,7 +1552,10 @@ def check_sample_weight_invariance(name, metric, y1, y2):\n     if not name.startswith(\"unnormalized\"):\n         # check that the score is invariant under scaling of the weights by a\n         # common factor\n-        for scaling in [2, 0.3]:\n+        # Due to numerical instability of floating points in `cumulative_sum` in\n+        # `median_absolute_error`, it is not always equivalent when scaling by a float.\n+        scaling_values = [2] if name == \"median_absolute_error\" else [2, 0.3]\n+        for scaling in scaling_values:\n             assert_allclose(\n                 weighted_score,\n                 metric(y1, y2, sample_weight=sample_weight * scaling),\n@@ -1584,8 +1587,10 @@ def test_regression_sample_weight_invariance(name):\n     # regression\n     y_true = random_state.random_sample(size=(n_samples,))\n     y_pred = random_state.random_sample(size=(n_samples,))\n+    sample_weight = np.arange(len(y_true))\n     metric = ALL_METRICS[name]\n-    check_sample_weight_invariance(name, metric, y_true, y_pred)\n+\n+    check_sample_weight_invariance(name, metric, y_true, y_pred, sample_weight)\n \n \n @pytest.mark.parametrize(\n",
  "fail_to_pass": [],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/metrics/_regression.py",
    "sklearn/metrics/tests/test_common.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-02-08T10:34:05Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30787",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/pull/9903"
}