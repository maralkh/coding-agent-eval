{
  "id": "scikit-learn__scikit-learn-30873",
  "repo": "scikit-learn/scikit-learn",
  "base_commit": "98af964055bb1e719de0468ff1ddf58ec1e9d3d0",
  "issue_number": 30868,
  "issue_title": "Calibration cannot handle different dtype for prediction and sample weight.",
  "issue_body": "### Describe the bug\n\nThis is from the comment: https://github.com/scikit-learn/scikit-learn/issues/28245#issuecomment-2106845979 . I did not find a corresponding issue. Please close if this is duplicated.\n\n\nAligning the types here https://github.com/scikit-learn/scikit-learn/blob/6a2472fa5e48a53907418a427c29562a889bd1a7/sklearn/calibration.py#L842 can help resolve the problem, but the casting is done for every grad calculation. Hopefully there's a better solution.\n\nUsers can workaround the issue by using `float32` for sample weights.\n\n### Steps/Code to Reproduce\n\n``` python\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\n\ndf = pd.DataFrame(\n    {\"x\": np.random.random(size=100), \"y\": np.random.choice([0, 1], size=100)}\n)\nsample_weight = np.ones((100))\n\nmodel = xgb.XGBClassifier()\n\ncalibrator = CalibratedClassifierCV(model)\n\ncalibrator.fit(df[[\"x\"]], df[\"y\"], sample_weight)\n```\n\n### Expected Results\n\nNo error.\n\n### Actual Results\n\n\n\nHere, xgboost outputs `float32`, but `sample_weight` is `float64`. These mismatched types lead to the following error:\n\n```python-traceback\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/sklearn/calibration.py\", line 673, in _fit_calibrator\n    calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/sklearn/calibration.py\", line 908, in fit\n    self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/sklearn/calibration.py\", line 855, in _sigmoid_calibration\n    opt_result = minimize(\n                 ^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_minimize.py\", line 738, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py\", line 386, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_optimize.py\", line 291, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py\", line 223, in __init__\n    self._update_fun()\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py\", line 295, in _update_fun\n    fx = self._wrapped_fun(self.x)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py\", line 21, in wrapped\n    fx = fun(np.copy(x), *args)\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_optimize.py\", line 80, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/scipy/optimize/_optimize.py\", line 74, in _compute_if_needed\n    fg = self.fun(x, *args)\n         ^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/sklearn/calibration.py\", line 840, in loss_grad\n    l, g = bin_loss.loss_gradient(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiamingy/.anaconda/envs/xgboost_dev/lib/python3.12/site-packages/sklearn/_loss/loss.py\", line 258, in loss_gradient\n    self.closs.loss_gradient(\n  File \"sklearn/_loss/_loss.pyx\", line 1791, in _loss.CyHalfBinomialLoss.loss_gradient\nValueError: Buffer dtype mismatch, expected 'const float' but got 'double'\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.0 | packaged by conda-forge | (main, Oct  3 2023, 08:43:22) [GCC 12.3.0]\nexecutable: /home/jiamingy/.anaconda/envs/xgboost_dev/bin/python3\n   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.8.0\n        numpy: 1.26.4\n        scipy: 1.15.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: mkl\n    num_threads: 32\n         prefix: libmkl_rt\n       filepath: /home/jiamingy/.anaconda/envs/xgboost_dev/lib/libmkl_rt.so.2\n        version: 2024.2.2-Product\nthreading_layer: intel\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 64\n         prefix: libomp\n       filepath: /home/jiamingy/.anaconda/envs/xgboost_dev/lib/libomp.so\n        version: None\n```",
  "pr_number": 30873,
  "pr_title": "FIX unintentional sample_weight upcast in CalibratedClassifierCV",
  "gold_patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.calibration/30873.fix.rst b/doc/whats_new/upcoming_changes/sklearn.calibration/30873.fix.rst\nnew file mode 100644\nindex 0000000000000..3e438622f4918\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.calibration/30873.fix.rst\n@@ -0,0 +1,7 @@\n+- :class:`~calibration.CalibratedClassifierCV` now raises `FutureWarning`\n+  instead of `UserWarning` when passing `cv=\"prefit`\". By\n+  :user:`Olivier Grisel <ogrisel>`\n+- :class:`~calibration.CalibratedClassifierCV` with `method=\"sigmoid\"` no\n+  longer crashes when passing `float64`-dtyped `sample_weight` along with a\n+  base estimator that outputs `float32`-dtyped predictions. By :user:`Olivier\n+  Grisel <ogrisel>`\ndiff --git a/sklearn/calibration.py b/sklearn/calibration.py\nindex 5034d2b0f4d89..80932629983f0 100644\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -318,9 +318,6 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n         \"\"\"\n         check_classification_targets(y)\n         X, y = indexable(X, y)\n-        if sample_weight is not None:\n-            sample_weight = _check_sample_weight(sample_weight, X)\n-\n         estimator = self._get_estimator()\n \n         _ensemble = self.ensemble\n@@ -333,7 +330,8 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n             warnings.warn(\n                 \"The `cv='prefit'` option is deprecated in 1.6 and will be removed in\"\n                 \" 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator))\"\n-                \" instead.\"\n+                \" instead.\",\n+                category=FutureWarning,\n             )\n             # `classes_` should be consistent with that of estimator\n             check_is_fitted(self.estimator, attributes=[\"classes_\"])\n@@ -348,6 +346,13 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n                 # Reshape binary output from `(n_samples,)` to `(n_samples, 1)`\n                 predictions = predictions.reshape(-1, 1)\n \n+            if sample_weight is not None:\n+                # Check that the sample_weight dtype is consistent with the predictions\n+                # to avoid unintentional upcasts.\n+                sample_weight = _check_sample_weight(\n+                    sample_weight, predictions, dtype=predictions.dtype\n+                )\n+\n             calibrated_classifier = _fit_calibrator(\n                 estimator,\n                 predictions,\n@@ -457,6 +462,13 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n                         )\n                     predictions = predictions.reshape(-1, 1)\n \n+                if sample_weight is not None:\n+                    # Check that the sample_weight dtype is consistent with the\n+                    # predictions to avoid unintentional upcasts.\n+                    sample_weight = _check_sample_weight(\n+                        sample_weight, predictions, dtype=predictions.dtype\n+                    )\n+\n                 this_estimator.fit(X, y, **routed_params.estimator.fit)\n                 # Note: Here we don't pass on fit_params because the supported\n                 # calibrators don't support fit_params anyway\n@@ -622,7 +634,13 @@ def _fit_classifier_calibrator_pair(\n         # Reshape binary output from `(n_samples,)` to `(n_samples, 1)`\n         predictions = predictions.reshape(-1, 1)\n \n-    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)\n+    if sample_weight is not None:\n+        # Check that the sample_weight dtype is consistent with the predictions\n+        # to avoid unintentional upcasts.\n+        sample_weight = _check_sample_weight(sample_weight, X, dtype=predictions.dtype)\n+        sw_test = _safe_indexing(sample_weight, test)\n+    else:\n+        sw_test = None\n     calibrated_classifier = _fit_calibrator(\n         estimator, predictions, y_test, classes, method, sample_weight=sw_test\n     )\ndiff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py\nindex 6e5900e4fa4a6..774a6f83ad1b6 100644\n--- a/sklearn/tests/test_calibration.py\n+++ b/sklearn/tests/test_calibration.py\n@@ -579,8 +579,12 @@ def test_calibration_attributes(clf, cv):\n     X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n     if cv == \"prefit\":\n         clf = clf.fit(X, y)\n-    calib_clf = CalibratedClassifierCV(clf, cv=cv)\n-    calib_clf.fit(X, y)\n+        calib_clf = CalibratedClassifierCV(clf, cv=cv)\n+        with pytest.warns(FutureWarning):\n+            calib_clf.fit(X, y)\n+    else:\n+        calib_clf = CalibratedClassifierCV(clf, cv=cv)\n+        calib_clf.fit(X, y)\n \n     if cv == \"prefit\":\n         assert_array_equal(calib_clf.classes_, clf.classes_)\n@@ -1077,20 +1081,48 @@ def test_sigmoid_calibration_max_abs_prediction_threshold(global_random_seed):\n     assert_allclose(b2, b3, atol=atol)\n \n \n-def test_float32_predict_proba(data):\n+@pytest.mark.parametrize(\"use_sample_weight\", [True, False])\n+@pytest.mark.parametrize(\"method\", [\"sigmoid\", \"isotonic\"])\n+def test_float32_predict_proba(data, use_sample_weight, method):\n     \"\"\"Check that CalibratedClassifierCV works with float32 predict proba.\n \n-    Non-regression test for gh-28245.\n+    Non-regression test for gh-28245 and gh-28247.\n     \"\"\"\n+    if use_sample_weight:\n+        # Use dtype=np.float64 to check that this does not trigger an\n+        # unintentional upcasting: the dtype of the base estimator should\n+        # control the dtype of the final model. In particular, the\n+        # sigmoid calibrator relies on inputs (predictions and sample weights)\n+        # with consistent dtypes because it is partially written in Cython.\n+        # As this test forces the predictions to be `float32`, we want to check\n+        # that `CalibratedClassifierCV` internally converts `sample_weight` to\n+        # the same dtype to avoid crashing the Cython call.\n+        sample_weight = np.ones_like(data[1], dtype=np.float64)\n+    else:\n+        sample_weight = None\n \n     class DummyClassifer32(DummyClassifier):\n         def predict_proba(self, X):\n             return super().predict_proba(X).astype(np.float32)\n \n     model = DummyClassifer32()\n-    calibrator = CalibratedClassifierCV(model)\n-    # Does not raise an error\n-    calibrator.fit(*data)\n+    calibrator = CalibratedClassifierCV(model, method=method)\n+    # Does not raise an error.\n+    calibrator.fit(*data, sample_weight=sample_weight)\n+\n+    # Check with frozen prefit model\n+    model = DummyClassifer32().fit(*data, sample_weight=sample_weight)\n+    calibrator = CalibratedClassifierCV(FrozenEstimator(model), method=method)\n+    # Does not raise an error.\n+    calibrator.fit(*data, sample_weight=sample_weight)\n+\n+    # TODO(1.8): remove me once the deprecation period is over.\n+    # Check with prefit model using the deprecated cv=\"prefit\" argument:\n+    model = DummyClassifer32().fit(*data, sample_weight=sample_weight)\n+    calibrator = CalibratedClassifierCV(model, method=method, cv=\"prefit\")\n+    # Does not raise an error.\n+    with pytest.warns(FutureWarning):\n+        calibrator.fit(*data, sample_weight=sample_weight)\n \n \n def test_error_less_class_samples_than_folds():\n",
  "fail_to_pass": [
    "test_float32_predict_proba"
  ],
  "pass_to_pass": [],
  "relevant_files": [
    "sklearn/calibration.py",
    "sklearn/tests/test_calibration.py"
  ],
  "difficulty": "medium",
  "created_at": "2025-02-21T15:36:55Z",
  "pr_url": "https://github.com/scikit-learn/scikit-learn/pull/30873",
  "issue_url": "https://github.com/scikit-learn/scikit-learn/issues/30868"
}